[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Complex Systems Modeling of Human-Environment Interactions",
    "section": "",
    "text": "Preface\nThese notes offer a comprehensive and opinionated, but foremost, practical introduction to the field of complex systems modeling applied to human-environment interactions and sustainability transitions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Complex Systems Modeling of Human-Environment Interactions",
    "section": "Learning goals",
    "text": "Learning goals\n\nStudents can model human-environment interactions to answer relevant questions in sustainability science.\nStudents can implement models of human-environment interactions  in the general-purpose computer language Python.\nStudents can critically evaluate models of human-environment interactions to judge their relevance to issues in sustainability science.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Complex Systems Modeling of Human-Environment Interactions",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI am grateful to everyone whose writings and teachings influenced my thinking. I am thankful to all contributors and creators of the Python language and its ecosystem, which made this work possible. Furthermore, I acknowledge many helpers who may use some form of generative AI, such as ChatGPT, Perplexity, Github Copilot, and Grammarly. All remaining errors remain my own.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01.01-Introduction.html",
    "href": "01.01-Introduction.html",
    "title": "1  Sustainability Systems Science",
    "section": "",
    "text": "Learning goals\nAfter this chapter, students will be able to explain:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intoduction</span>"
    ]
  },
  {
    "objectID": "01.01-Introduction.html#human-environment-interactions-for-sustainability-transitions",
    "href": "01.01-Introduction.html#human-environment-interactions-for-sustainability-transitions",
    "title": "1  Sustainability Systems Science",
    "section": "1.1 Human-environment interactions for sustainability transitions",
    "text": "1.1 Human-environment interactions for sustainability transitions\n\nThe state of the planet\nWatch this TED talk by Johan Rockström, who offers the 2024 scientific assessment of the state of the planet and explains what must be done to preserve Earth’s resilience to human pressure.\nWhile watching, ask yourself the following questions (and make notes around your answers):\n\nWhat are the main challenges facing humanity in the 21st century?\nIn which ways humans and the environment are interconnected?\nWhat is the most impressive fact you learned from the talk?\n\n\n\n\n        \n        \n\n\n\n\nWhy are we not acting?\nGiven the rather grim assessment of the planet’s state, the question arises: Why are we not acting more toward a safe and just future for all\n\ndespite all the scientific progress we have made so far,\ndespite all the knowledge about the risks and undesirable consequences ahead if we do not change our course of action,\ndespite all the knowledge we have obtained about possible solutions?\n\nReflect on this question on your own. Come up with a list of factors most relevant to you.\n\n\nA failure of systems thinking\nAccording to John Sterman, professor and director of the MIT System Dynamics Group, the underlying issue of inaction in the sustainability crises is a massive failure of systems thinking.\nWatch the part of his talk (from minutes 12:19 until 14:17) where he explains this failure and compare his assessment with your own from above. Did he miss some critical factors? How can we complement his assessment?\n\n\n\n        \n        \n\n\nThis book offers a comprehensive and opinionated, but foremost, practical introduction to the field of complex systems modeling applied to human-environment interactions and sustainability transitions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intoduction</span>"
    ]
  },
  {
    "objectID": "01.01-Introduction.html#modeling",
    "href": "01.01-Introduction.html#modeling",
    "title": "1  Sustainability Systems Science",
    "section": "1.2 Modeling",
    "text": "1.2 Modeling\n\nWe cannot not model\nIn information theory, a bit (i.e., a 0 or a 1) stores the answer to a yes-or-no question. We can measure the rate of information transmitted with the number of bits per second.\n\n\n\n\n\n\nFigure 1.1: We cannot not model - Human physiology requires simplification\n\n\n\nFor example, it is known that our senses gather some 11 million bits per second from the environment (britannica.com). When applied to the human brain, you expect it to show tremendous information processing capability.\nInterestingly, when researchers attempted to assess information processing capabilities during “intelligent” or “conscious” activities—like reading or playing the piano—they found a maximum capability of under 50 bits per second (Figure 1.1). For instance, a typical reading speed of 300 words per minute translates to about five words per second. Assuming an average of five characters per word and roughly two bits per character results in that 50 bits per second figure. The precise number can vary based on assumptions and may differ according to the individual and the specific task.\nThus, a tremendous amount of compression occurs if 11 million bits are reduced to less than 50. Our human physiology requires simplifaction. We model all the time. Note that the discrepancy between the amount of information being transmitted and the amount of information being processed is so large that any inaccuracy in the measurements is insignificant.\n\n\nAll models are wrong\n\nA model is a simplified representation of reality.\n\nA model’s simplification is necessary to make the phenomenon under question tractable and understandable. Simplifaction here is a feature, not a bug (Smaldino, 2017). The purpose of a model is to be wrong.\nThe models we use come in different forms or media.\n\n\n\n\n\n\nFigure 1.2: Different model forms or media\n\n\n\nSome are informal and qualitative, while others are more formal and quantitative (Figure 1.2). Mental models are intuitive and often subconscious. Verbal models describe concepts through language. Both model media can be vague and open to multiple interpretations, giving an illusion of understanding without precise clarity (Smaldino, 2017). Furthermore, the last 100,000 years of evolution have shaped Homo sapiens in ways that make it difficult for us to comprehend a dynamic, unstable, and unpredictable world. Our brains evolved to manage immediate, short-term situations and anticipate gradual, linear changes with a tendency to seek balance and stability (Raworth, 2017). Thus, we must make a conscious effort to overcome these cognitive priors. Formal models can help with that.\nPictorial models enhance understanding through visual representations. Take, for example, maps, and diagrams, but also artistic paintings. Mathematical models use equations to quantify relationships, providing greater precision. Computer models require the highest level of precision; all entities and causal mechanisms must be defined unambiguously to allow a computer simulation to operate. This high level of precision makes them essential for scientific research and understanding. However, our subconscious mental models often have the highest impact on how we perceive and act in the world.\nThus, we will develop primarily formal mathematical and computational models in this book. However, this will automatically refine our mental and verbal models.\nTo illustrate the power of mental models, consider the following riddle:\n\nA father and son are in a horrible car crash that kills the dad. The son is rushed to the hospital; just as he’s about to go under the knife, the surgeon says, “I can’t operate – that boy is my son!”   How can this be?\n\nRegardless of how obvious (or not) the answer appears to you, watch this video in which people in Vienna are asked this question (Autotranslation helps if you do not speak German). Observe their reactions when their mental models are updated (from minute 1:38 on).\n\n\n\n        \n        \n\n\nWe observe that modeling is an iterative process. When recognizing a mismatch between our models and reality, we get the opportunity to refine our models, and so gradually, we might become less wrong (Smaldino, 2017). Creating formal models of the systems we care about is the only method to achieve this in a structured, deliberate, and controlled manner.\nHowever, as it is the defining feature of a model to simplify or, in other words, to be wrong, making them more true cannot be the purpose of a model per se. But, what makes a model useful?\n\n\nSome models are useful\nThere is no universally agreed-upon classification of model use cases. I tend to distinguish between four clusters of model use cases (Figure 1.3): 1) Understanding & explaining, 2) Communication & learning, 3) Prediction & forecasting, and 4) Decision-making & action.\n\n\n\n\n\n\nFigure 1.3: Possible model use cases\n\n\n\n\nUnderstanding & Explaining\nUnderstanding and explaining phenomena may occur in various ways. For example, models help clarify assumptions (Smaldino, 2017), allowing for a more transparent assessment of their implications and conclusions. Models help us to reason, i.e., to identify conditions and deduce logical implications. They also can provide (testable) explanations for empirical phenomena. (Page, 2018) And, models are helpful to explore, i.e., to consider different “what if” scenarios to investigate possibilities and hypotheticals (Page, 2018; Smaldino, 2017).\n\n\nCommunication & Learning\nFormal models can serve as tools to overcome our cognitive limitations. They help in systematizing and synchronizing our understanding, ensuring that we discuss the same concepts and avoiding ambiguity that often accompanies verbal models.(Smaldino, 2017) Models can guide scientific questions. The precise specification of components and their relationships in a model helps clarify scientific questions and distinguishes them from unfalsifiable pseudo-theories (Smaldino, 2017).\n\n\nPrediction & Forcasting\nPrediction refers to making numerical and categorical predictions of future and unknown phenomena. Historically, explanation and prediction were often linked closely together. However, prediction differs from explanation. A model can predict without explaining. Deep learning algorithms can predict product sales, tomorrow’s weather, price trends, and specific health outcomes; however, they provide minimal explanation. Also, a model can explain without predicting. Ecology models can explain speciation patterns but cannot predict new species (Page, 2018). Related concepts to prediction are forecasting and projections, which can mean various things in different contexts.\n\n\nDecicion-making & Action\nFormal models help design institutions, policies, and rules by providing frameworks for contemplating the implications of choices. Combining this process with empirical data, formal models are helpful for action, guiding policy choices and strategic actions of governments, corporations, and nonprofits (Page, 2018). Likewise, good mental models are helpful for good actions in our everyday lives.\nIt is important to note that, in general, a single model does not fulfill all use cases. Some models might do, like, for example, Newtonian mechanics. It explains the motion of objects, predicts their future positions, guides the design of machines, and is learned in schools worldwide. However, in most instances, this is not the case. A model might explain a phenomenon but cannot predict it or vice versa. A helpful model for decision-making might neither make accurate predictions nor explain the underlying mechanisms. Take macroeconomic models as an example.\nBeyon being useful or not, are there some quality criteria a good model should fulfill?\n\n\n\nSome models are good\nI argue that there are some quality criteria that make a model a good model. A good model must be\n\ncoherent\ntransparent, and\nsparse.\n\n\nCoherence\nCoherence means that the model is consistent. It does not contain contradictions or logical errors. For example, many proverbs are contradictory (Page, 2018):\n\nProverb: Tie yourself to the mast  Opposite: Keep your options open\n\n\nProverb: The perfect is the enemy of the good  Opposite: Do it well or not at all\n\n\nProverb: Actions speak louder than words  Opposite: The pen is mightier than the sword\n\nA model can resolve these contradictions by specifying the conditions under which a particular statement holds. For example, under some conditions, it is best to tie yourself to the mast, while under others, it is best to keep your options open. A model can help to clarify these conditions.\nThis requirement of coherence or consistency imposes a set of helpful constraints within which the model development can take place.\n\n\nTransparency\nA good model makes its assumptions explicit and transparent. It also openly discusses its limitations. Bonus points if the model is transparent about uncertainty in empirical data, parameters, and processes. Transparency is a prerequisite for a model to be useful for communication and learning.\n\n\nSparsity\nYou should take the simpler model if you have two competing models of equal quality regarding their use case. This is also known under the term Occam’s razor or the principle of parsimony. This principle helps us avoid going overboard with introducing new assumptions, entities, and processes into our models, making our models overly complex and difficult to understand without further benefit. However, sometimes, during the modeling process, we are unsure whether a newly introduced assumption is helpful to explain the phenomenon under question. Thus, for model development, the principle of parsimony is a guideline, not a strict rule. As the famous saying goes\n\nYou should make things as simple as possible, but not simpler.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intoduction</span>"
    ]
  },
  {
    "objectID": "01.01-Introduction.html#systems-reductionism",
    "href": "01.01-Introduction.html#systems-reductionism",
    "title": "1  Sustainability Systems Science",
    "section": "1.3 Systems reductionism",
    "text": "1.3 Systems reductionism\nGiven that we cannot not model, how should we make sense of the world?\n\nClassical reductionism\nWith classical reductionism, I refer to the ideas of rationalism and empiricism that have dominated Western science since the last great transformation, the Enlightenment. Replacing religious dogmatism, this view argues, that\n\nthe whole can be understood from its parts.\n\nFor example, this approach has been highly successful in physics and chemistry, where the behavior of atoms and molecules can be understood by studying their interactions.\nAs a result, scientific disciplines tend to be hierarchically clustered around specific parts of the whole. For example, the German Research Association (DFG) clusters disciplines around the engineering, life, natural, and humanities and social sciences (Figure 1.4). Within each cluster, there are multiple disciplines with subdisciplines.\n\n\n\n\n\n\nFigure 1.4: DFG classification of scientific disciplines\n\n\n\nClassical reductionism produced a lot of experts. Together, they drove the massive increase in wealth, health, and knowledge in the last 200 years.\n\n\nThe problem with experts\nExperts carry a risk of overrating the importance of their area of expertise (Brockmann, 2021). At the same time, experts tend to overlook the interactions within and beyond the system under investigation (Figure 1.5).\n\n\n\n\n\n\nFigure 1.5: The problem with experts\n\n\n\nThese problems with classical experts become particularly problematic in complex systems, which are characterized by their interactions.\n\n\nComplex systems\nThe study of complex systems started around the 1950s and has been a diverse endeavor since then. See, for example, the map of complexity science.\nIn a complex system,\n\nthe whole is more than the sum of its parts.\n\nThe whole, the so-called macro-level, emerges from and feeds backs to the so-called micro-level, in which (often many, heterogeneous) entities or agents interact (in often non-linear ways) in a shared environment. Both levels are out-of-equilibrium, continuously evolving (Figure 1.6).\n\n\n\n\n\n\nFigure 1.6: Properties of a complex system\n\n\n\nTo illustrate the idea of emergence, where the whole is more than the sum of its parts, consider the following quote:\n\n“There’s no love in a carbon atom,  No hurricane in a water molecule,  No financial collapse in a dollar bill.”  – Peter Dodds\n\nTo observe a complex system in action, enjoy a video of bird flocking behavior. A century ago, the wonders of these highly coordinated yet leaderless flocks led people to believe that telepathy might be what guided these birds (phys.org).\n\n\n\n        \n        \n\n\nSee complexityexplained.github.io for more background information on complexity science.\nSo, what can we do to make sense of complex systems, given our limited information processing capacity and the consequences that we cannot not model?\n\n\nSystems reductionism\nTo quote one of the founding fathers of complexity science,\n\n“It may not be entirely vain, however, to search for common properties among diverse kinds of complex systems”  – Herbert Simon\n\nAs it turns out, flocking behavior, for example, can be explained by just three rules: separation, alignment, and cohesion (wikipedia.org/boids, Figure 1.7).\n\n\n\n\n\n\nFigure 1.7: The three rules to produce flocking behavior\n\n\n\nSystems and classic reductionism complement each other (Figure 1.8). While classical reductionism helps understand the parts of a system, systems reductionism helps understand the interactions between these parts. Collaboration between the two approaches is key.\n\n\n\n\n\n\nFigure 1.8: Systems and classic reductionism complementing each other\n\n\n\nConceptually, complex systems modeling combines the practice of (formal) modeling with the ideas of systems reductionism.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intoduction</span>"
    ]
  },
  {
    "objectID": "01.01-Introduction.html#sustainability-systems-modeling",
    "href": "01.01-Introduction.html#sustainability-systems-modeling",
    "title": "1  Sustainability Systems Science",
    "section": "1.4 Sustainability Systems Modeling",
    "text": "1.4 Sustainability Systems Modeling\nThis section conceptually synthesizes the practice of complex system modeling applied to the problem field of sustainability transitions and human-environment interactions.\n\nStructural challenges\nTo operationalize systems thinking for human-environment modeling, we require a collection of the structural elements and processes that either hinder or may foster action toward sustainability.\nSummarizing many fantastic review and perspective papers (Constantino et al., 2021; Elsawah et al., 2020; Farahbakhsh et al., 2022; Giupponi et al., 2022; Levin & Xepapadeas, 2021; Müller et al., 2020; Schill et al., 2019) we obtain the following list of structural challenges for sustainability transitions (Figure 1.9): Complex system models of human-environment interactions must account for the dynamics of the collective behavior emerging from cognitive agents within an environmental context (Barfuss et al., 2024). They must also adhere to the good modeling practices of coherence, transparency, and sparsity, as discussed above.\n\n\n\n\n\n\nFigure 1.9: Structural challanges for sustainability transitions\n\n\n\n\nCognitive agency 🧠\nImproving the representation of human behavior in models of social-ecological systems and human-environment interactions is a critical challenge (Constantino et al., 2021; Schill et al., 2019; Schlüter et al., 2017). Humans are neither hyper-rational nor overly simplistic, as many models assume. At the very least, they are cognitive agents who perceive their current environmental context, evaluate it and their options, make decisions, and act accordingly.\n\n\nEnvironmental context 🌳\nThe environmental context refers to the decision-making challenge the agents face. The environmental context is not static. It may change smoothly or abruptly, based on human activities or via inherent dynamics. For example, climate damages gradually worsen with increasing global mean temperature. And crossing climate tipping points may abruptly lead to catastrophic outcomes. Furthermore, these changes are not certain but stochastic in nature and may only be partially observable by humans. And often, the consequences of action are heavily delayed, impacting future generations. All these attributes make collective action for sustainability transitions tremendously challenging.\nThe environmental context includes the biophysical environment, such as the climate, biodiversity, and resources, as well as the social environment, out of which the collective behavior emerges.\n\n\nCollective behavior 👥\nFrom cognitive agents within an environmental context, collective behavior emerges. This collective behavior depends on the social context, the heterogeneity of the agents, the interaction structure between them, and the scale on which they operate. Collective behavior refers to the dynamics of the system as a whole, which are not easily reducible to the characteristics of individual agents.\n\n\nDynamics 🚀\nOur primary goal is to comprehend and advance sustainability transitions. Transitions are fundamentally dynamic in nature, so our modeling approach must reflect this dynamism, integrating non-linear feedback loops and critical transitions. Furthermore, the concepts of stability and resilience demand a dynamic viewpoint. Before a system reaches stability, its transient evolution offers crucial insights into the sustainability transitions itself.\nHow can we begin to make sense of this all?\nWhat precisely do all of these elements mean?\nAnd how do all of these elements relate to each other?\nThese questions will guide us through the following chapters.\nWe will tackle them with the help of a useful framework from transdisciplinary research: the three types of knowledge, applied to modeling.\n\n\n\nThree types of models\nWhen addressing societal challenges, the concept of the three types of knowledge helps to produce not only knowledge on problems but also knowledge that helps to overcome those problems (Buser & Schneider, 2021). In general, the concept applies to all research methodologies. We will specifically discuss it in the context of formal modeling, transforming it into three types of models (Figure 1.10).\n\n\n\n\n\n\nFigure 1.10: Three types of models based on three types of knowledge for transdisciplinary reserach\n\n\n\nThe three types of models are:\n\nDynamic-systems models\nDynamic-systems models operationalize systems knowledge, typically understood as knowledge concerning the existing system or issue. This understanding is primarily analytical and descriptive. For instance, in the context of sustainability transitions, systems knowledge assesses the risk triggering climate tipping points, biodiversity loss dynamics, or a specific region’s social-ecological dynamics. Systems knowledge is strongly associated with facts and asks what is?\nIn this regard, dynamic-systems models are often used to understand the system’s behavior under specific conditions.\nWe will discuss dynamic-systems modeling in the first part of the book, covering\n\nNonlinearity and feedback loops in Chapter 02.01\nTipping elements and regime shifts in Chapter 02.02\nResilience in Chapter 02.03, and\nStochastic state transitions in Chapter 02.04.\n\n\n\nTarget-equilibrium models\nTarget-equilibrium models operationalize target knowledge, which is knowledge about the desired future and the values that indicate which direction to take. It relies on deliberation by different societal actors and is based on values and norms. In sustainability transitions, ways of producing target knowledge include participatory vision, scenario development with a wide range of stakeholders, and the public discourse at large. Target knowledge is strongly associated with values and asks what ought to be?.\nTarget-equilibrium (or equilibrium-based models applied to sustainability transitions) are primarily used in economics. In theory, the equilibrium is the outcome of an optimization procedure where the specified normative target is reached.\nWe will discuss target-equilibrium modeling in the second part of the book, covering\n\nSequential decisions of a single agent in a dynamic environment in Chapter 03.01\nStrategic interactions of multiple agents in a static envrionment in Chapter 03.02, and\nDynamic interactions of multiple agents in a dynamic environment in Chapter 03.03.\n\n\n\nTransformation-agency models\nTransformation-agency models operationalize transformation knowledge, which is knowledge about how to move from the existing system to the desired future. This knowledge includes concrete strategies and steps to take. In sustainability transitions, producing transformation knowledge could involve developing policy instruments, designing new institutions, or implementing new technologies. Transformation knowledge is strongly associated with agency and asks how to?.\nTransformation-agency models (or agent-based models applied to sustainability transitions) are a flexible tool that combines the dynamics of how to get to a desired outcome with agency that defines what is desirable and possible to do.\nWe will discuss transformation-agency modeling in the third part of the book, covering\n\nRule-based behavioral agency in agent-based models in Chapter 04.01\nIndividual reinforcement learning in Chapter 04.02, and\nThe non-linear dynamics of reinforcement learning in Chapter 04.03.\n\nIt is important to note that the three knowledge types are interdependent. For example, knowledge about ‘how to’ would be of limited use or even dangerous if it was not oriented toward desirable target values and based on sound facts. In the same vein, we will integrate the different types of models toward the end of this book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intoduction</span>"
    ]
  },
  {
    "objectID": "01.01-Introduction.html#learning-goals-revisited",
    "href": "01.01-Introduction.html#learning-goals-revisited",
    "title": "1  Sustainability Systems Science",
    "section": "1.5 Learning goals revisited",
    "text": "1.5 Learning goals revisited\n\nUnderstanding and promoting sustainability transitions requires a coupled human-environment systems approach, as the challenges and possible solutions are tightly coupled between humans and the biosphere.\nLimited human information processing demands us to model the world around us. Formal models help overcome imprecise mental models and cognitive limitations. Models are helpful for understanding, communicating, predicting, and making decisions. Good models are coherent, transparent, and sparse.\nSystems reductionism complements classical reductionism to avoid unintended side effects in complex systems. Complex systems are characterized by interactions, emergent properties, and feedback loops.\nComplex systems models of human-environment interactions must account for the dynamics of the collective behavior emerging from cognitive agents in environmental contexts. Three types of models, dynamic-systems model, target-equilibria models, and transformation-agency (agent-based) model, will help us achieve these desiderata.\n\nThe exercises for this chapter offer a thorough introduction to the programming language Python, preparing you for the modeling exercises in the subsequent chapters.\n\n\n\n\nBarfuss, W., Flack, J. C., Gokhale, C. S., Hammond, L., Hilbe, C., Hughes, E., Leibo, J. Z., Lenaerts, T., Levin, S. A., Madhushani Sehwag, U., McAvoy, A., Meylahn, J. M., & Santos, F. P. (2024). Collective Cooperative Intelligence. Forthcomming in the Proceedings of the National Academy of Sciences.\n\n\nBrockmann, D. (2021). Im Wald vor lauter Bäumen: Unsere komplexe Welt besser verstehen. Deutscher Taschenbuch Verlag.\n\n\nConstantino, S. M., Schlüter, M., Weber, E. U., & Wijermans, N. (2021). Cognition and behavior in context: A framework and theories to explain natural resource use decisions in social-ecological systems. Sustainability Science, 16(5), 1651–1671. https://doi.org/10.1007/s11625-021-00989-w\n\n\nElsawah, S., Filatova, T., Jakeman, A. J., Kettner, A. J., Zellner, M. L., Athanasiadis, I. N., Hamilton, S. H., Axtell, R. L., Brown, D. G., Gilligan, J. M., Janssen, M. A., Robinson, D. T., Rozenberg, J., Ullah, I. I. T., & Lade, S. J. (2020). Eight grand challenges in socio-environmental systems modeling. Socio-Environmental Systems Modelling, 2, 16226–16226. https://doi.org/10.18174/sesmo.2020a16226\n\n\nFarahbakhsh, I., Bauch, C. T., & Anand, M. (2022). Modelling coupled human–environment complexity for the future of the biosphere: Strengths, gaps and promising directions. Philosophical Transactions of the Royal Society B: Biological Sciences, 377(1857), 20210382. https://doi.org/10.1098/rstb.2021.0382\n\n\nGiupponi, C., Ausseil, A.-G., Balbi, S., Cian, F., Fekete, A., Gain, A. K., Essenfelder, A. H., Martínez-López, J., Mojtahed, V., Norf, C., Relvas, H., & Villa, F. (2022). Integrated modelling of social-ecological systems for climate change adaptation. Socio-Environmental Systems Modelling, 3, 18161–18161. https://doi.org/10.18174/sesmo.18161\n\n\nLevin, S., & Xepapadeas, A. (2021). On the Coevolution of Economic and Ecological Systems. Annual Review of Resource Economics, 13(1), 355–377. https://doi.org/10.1146/annurev-resource-103020-083100\n\n\nMüller, B., Hoffmann, F., Heckelei, T., Müller, C., Hertel, T. W., Polhill, J. G., van Wijk, M., Achterbosch, T., Alexander, P., Brown, C., Kreuer, D., Ewert, F., Ge, J., Millington, J. D. A., Seppelt, R., Verburg, P. H., & Webber, H. (2020). Modelling food security: Bridging the gap between the micro and the macro scale. Global Environmental Change, 63, 102085. https://doi.org/10.1016/j.gloenvcha.2020.102085\n\n\nPage, S. E. (2018). The model thinker: What you need to know to make data work for you. Basic Books.\n\n\nRaworth, K. (2017). Doughnut economics: Seven ways to think like a 21st-century economist. Chelsea Green Publishing.\n\n\nSchill, C., Anderies, J. M., Lindahl, T., Folke, C., Polasky, S., Cárdenas, J. C., Crépin, A.-S., Janssen, M. A., Norberg, J., & Schlüter, M. (2019). A more dynamic understanding of human behaviour for the Anthropocene. Nature Sustainability, 2(12), 1075–1082.\n\n\nSchlüter, M., Baeza, A., Dressler, G., Frank, K., Groeneveld, J., Jager, W., Janssen, M. A., McAllister, R. R. J., Müller, B., Orach, K., Schwarz, N., & Wijermans, N. (2017). A framework for mapping and comparing behavioural theories in models of social-ecological systems. Ecological Economics, 131, 21–35. https://doi.org/10.1016/j.ecolecon.2016.08.008\n\n\nSmaldino, P. E. (2017). Models Are Stupid, and We Need More of Them. In R. R. Vallacher, S. J. Read, & A. Nowak (Eds.), Computational Social Psychology (1st ed., pp. 311–331). Routledge. https://doi.org/10.4324/9781315173726-14",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intoduction</span>"
    ]
  },
  {
    "objectID": "02-DynamicSystems.html",
    "href": "02-DynamicSystems.html",
    "title": "Dynamic Systems",
    "section": "",
    "text": "In this part, we cover dynamic-systems models. They operationalize systems knowledge, typically understood as knowledge concerning the existing system or issue. This understanding is primarily analytical and descriptive. For instance, in the context of sustainability transitions, systems knowledge assesses the risk triggering climate tipping points, biodiversity loss dynamics, or a specific region’s social-ecological dynamics. Systems knowledge is strongly associated with facts and asks what is?\nIn this regard, dynamic-systems models are often used to understand the system’s behavior under specific conditions.\n\n\n\nThree types of models based on three types of knowledge for transdisciplinary reserach\n\n\nSpecifically, we will cover\n\nNonlinearity and feedback loops in Chapter 02.01\nTipping elements and regime shifts in Chapter 02.02\nResilience in Chapter 02.03, and\nStochastic state transitions in Chapter 02.04.",
    "crumbs": [
      "Dynamic Systems"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html",
    "href": "02.01-Nonlinearity.html",
    "title": "2  Nonlinearity",
    "section": "",
    "text": "2.1 Motivation",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html#motivation",
    "href": "02.01-Nonlinearity.html#motivation",
    "title": "2  Nonlinearity",
    "section": "",
    "text": "The issue of climate change\nRead the following summary, adapted from the Intergovernmental Panel on Climate Change (IPCC) Third Assessment Report’s Summary for Policymakers.\n\n\n\nThe issue of climate change\n\n\nNow consider the following questions:\n\n\n\nA scenario of climate change\n\n\nTypical responses. This experiment was conducted with MIT graduate students, a group of highly educated adults (Sterman & Sweeney, 2007). Yet, they showed a widespread misunderstanding of fundamental stock and flow relationships. Most subjects believed that atmospheric greenhouse gas (GHG) can be stabilized while emissions into the atmosphere continuously exceed the removal of GHGs from it.\n\n\n\nTypical Responses\n\n\nThese beliefs support wait-and-see policies and neglecting the issue climate change as a top priority for the policy agenda.\n\n\nCarbon bathtub\nKnowledge of climatology or calculus is not needed to respond correctly. Think of it like a bathtub: the water level represents the amount of greenhouse gases (GHGs) in the atmosphere. The water flowing into the tub is like the rate of GHG emissions, and the water flowing out is like the rate of GHG removal. If more water is flowing in than out, the water level rises. To keep the water level stable, the inflow and outflow need to be equal. This is similar to stabilizing GHG concentrations - emissions must equal removals. The balance between inflows and outflows determines how GHGs accumulate, not just the level of inflows.\n\n\n\nCarbon Bathtub\n\n\n\n\nLearning goals\nAfter this chapter, students will be able to:\n\nDefine and describe the components of a dynamic system.\nRepresent dynamic system models in visual and mathematical form.\nExplain the concepts of feedback loops and delays.\nExplain two kinds of non-linearity and how they are related.\nImplement dynamic system models and visualize model outputs using Python, to interpret model results.\nAnalyze the stability of equilibrium points in dynamic systems using linear stability analysis.\n\nAfter motivating this chapter, we make ourselves ready for some computations by importing some Python libraries and setting up the plotting style.\n\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; plt.rcParams['grid.linewidth'] = 0.25;",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html#dynamic-systems",
    "href": "02.01-Nonlinearity.html#dynamic-systems",
    "title": "2  Nonlinearity",
    "section": "2.2 Dynamic systems",
    "text": "2.2 Dynamic systems\n\nA dynamic system1 is a system whose state is uniquely specified by a set of variables and whose behavior is described by predefined rules.\n\nYou can think of the state of the system as a collection of stocks (also known as state variables), and the rules as the flows that change the stocks over time. At the system boundaries, you can imagine sources and sinks, which represent in- and out-flows to the system we are currently looking at.\nFor example, in the case of the carbon bathtub, the state of the system is the amount of carbon in the atmosphere. The rules are that emissions increase and net removals decrease the amount of carbon in the atmosphere. The source is the origin of emissions, and the sink where the net removals go (i.e., mostly ocean and biosphere). Both, source and sink, are not explicitly represented in this model - but could be in another model.\n\nPictorial representation\nGraphically, we can compose (often called) stock-and-flow or causal (loop) diagrams via the following building blocks.\n\n\n\nGraphical elements of a dynamic system\n\n\nFor instance, the carbon bathtub could resemble this:\n\n\n\nGraphical elements of a dynamic system\n\n\nSuch pictorial models are a powerful tool to develop a dynamic systems model and communicate its structure. However, they are limited regarding specifying model details and analysis. For this, a mathematical representation is essential.\n\n\nMathematical representation\nMathematically, we use variables (such as \\(x\\), \\(y\\), and any other letter) as a placeholder for the value of a stock. We indicate the value at a specific time \\(t\\) by an index (such as \\(x_t\\), \\(y_t\\), etc.), assuming that time advances in discrete steps, i.e., \\(t \\in \\mathbb{Z}\\). To describe the change of stocks, we formulate an equation (with \\(+\\)’s and \\(-\\)’s for positive and negative changes). In its most general form, it looks like,\n\\[x_{t+1} = F(x_{t}).\\]\nThis means the value of the stock \\(x\\) at time \\(t+1\\) equals the value of the function \\(F\\), which depends on the value of stock at time \\(t\\). Note that another common name for dynamic systems in discrete time is maps.\nFor example, the carbon bathtub equations look like,\n\\[ x_{t+1} = x_{t} + e_{t} - o_{t}, \\]\nwhere \\(x\\) denotes the atmospheric carbon stock, and \\(e_t \\geq 0\\) and \\(o_t \\geq 0\\) the amount of emissions and outflow at time \\(t\\).\nThis equation shows that the stock at time \\(t+1\\) equals the stock at time \\(x_t\\) plus the inflow of emissions at time \\(e_t\\) minus the outflow at time \\(o_t\\). Thus, what ultimately determines the stock at time \\(t+1\\) is the difference between the emissions and the outflow at time \\(t\\). Let \\(n_t = e_t - o_t\\) be the net flow. Then, we can rewrite the equation as\n\\[ x_{t+1} = x_t + n_t.\\]\nNote that \\(n_t\\) can be positive or negative, depending on the emissions and the outflows.\nSometimes, it can be handy to represent the dynamic system in its difference form, directly indicating the change of stocks,\n\\[\n\\Delta x = x_{t+1} - x_{t} = F(x_{t}) - x_{t}.\n\\]\nIn analogy to the more common differential equations (which work with continuous time), we call this form difference equations.\nFor example, the carbon bathtub difference equations look like,\n\\[ \\Delta x = e_{t} - o_{t}.\\]\n\nDeepDive | Why discrete-time models\nMost dynamical system models consider the continous-time case; but we will focus on discrete time.\nDiscrete-time models are easy to understand, develop and simulate.\n\nComputer simulations require time-discretization anyway.\nExperimental data often already discret.\nThey can represent abrupt changes.\nThey are more expressive using fewer variables than their continuous-time counterparts.\n\nDiscrete-time models are a cornerstone in mathematical modeling due to their simplicity and adaptability. They align naturally with computer simulations, as digital systems process time in discrete intervals. This compatibility makes them essential for precise computational analysis. Additionally, experimental data is often recorded at specific intervals, such as daily or monthly, fitting seamlessly with discrete-time models without requiring transformation processes needed for continuous-time models. These models also excel at representing abrupt changes found in real-world phenomena, such as population dynamics or financial markets, capturing these shifts more directly than continuous models. Furthermore, discrete-time models often require fewer variables, enhancing both simplicity and interpretability. This efficiency allows researchers to focus on critical system aspects, making these models powerful tools for theoretical and practical applications alike. In essence, the strengths of discrete-time models lie in their alignment with digital computation, natural fit with discrete data, ability to capture sudden changes, and efficient expressiveness, making them indispensable for scientists and engineers.\n\n\n\nComputational representation\nThere are, in fact, many ways to translate the pictorial and mathematical models into a computer model. We start by defining the function, \\(F(x_t)\\), from above but give it a more descriptive name.\n\ndef update_stock(stock, inflow, outflow):\n    new_stock = stock + inflow - outflow\n    return new_stock\n\nNow, we are ready to perform our first model simulation.\nConceptually, we need to define the initial value of the stock. Let’s assume we start at 280 parts per million (ppm).\n\nstock = 280\n\nTechnically, we must define a container to store the simulation output. We create a Python list for this purpose and store the stock’s initial value inside.\n\ntime_series = [stock]\n\nConceptually, before we can start the simulation, we must decide how many time steps it should run, and on the values of inflow and outflow. Let’s simulate 150 steps, denoting yearly updates from 1850 to 2000, and assume a constant flow with an inflow of 75.6 ppm and an outflow of 75 ppm.\nTechnically, we loop over the range from 1851 to 2001, calling the update_stock function with the flow parameter values and appending the new stock value to the time_series list.\n\nfor t in range(1851, 2001): \n    stock = update_stock(stock, 75.6, 75); \n    time_series.append(stock)\n\nFinally, we can graphically investigate the output time series of our model simulation.\n\nplt.plot(list(range(1850, 2001)), time_series, '.-'); \nplt.xlabel('time [years]'); plt.ylabel('stock [ppm]');\n\n\n\n\n\n\n\nFigure 2.1: Linear growth\n\n\n\n\n\nThe above code cell plots the time_series data with dots at each data point and lines connecting them.\n\nplt is an alias for matplotlib.pyplot, a popular plotting library in Python.\nplot is a function that creates a 2D line plot.\nlist(range(1850, 2001)) represents the values to be plotted along the x-axis.\ntime_series is the data being plotted. It is expected to be a sequence of values (e.g., a list or a NumPy array).\n'.-' is a format string that specifies the style of the plot:\n\n'.' indicates that the data points should be marked with dots.\n'-' indicates that the data points should be connected with lines.\n\nThe second line equips the plot with an x- and a y-label.\n\nThe ; at the end of each statement allows for multiple statements in one line.\n\n\nWe observe a rise in CO2 concentration from 280 ppm to 370 ppm, as in the observation data.\nHowever, the shape of the curve is different. Here, we observe just a linear trend. The change of stock equals the net flow of inflows minus outflows. If they are constant, the stock evolution is linear.\nLinear here means that the change in stock is proportional to the in and outflows. If they are constant, the rate of change is constant and the stock evolution is linear.\n\n\nA general modeling framework\nThe carbon bathtub example is perhaps the simplest dynamic model one can imagine. Still, it illustrates the importance of differentiating between a stock and a flow, which is changing that stock.\nHowever, the real power of dynamic system models comes from their generality and the possibility to include feedbacks in the change of stocks.\nDynamic systems are a very general modeling framework. They can model many more phenomena than the evolution of atmospheric greenhouse gas concentrations, from classic examples, such as the bouncing of a ball, the swing of a pendulum, and the motions of celestial bodies, to more advanced dynamics, such as the evolution of populations, the weather and climate, neural networks in the brain, or the behavior of agents.\nFor example, the update_stock function we defined above is entirely agnostic regarding which kind of stock it models. The Python function does not refer to the stock of atmospheric greenhouse gas concentration. It can model any system with one stock where the stock change is independent of the current level of the stock. Or, in other words, systems without feedback.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html#feedback",
    "href": "02.01-Nonlinearity.html#feedback",
    "title": "2  Nonlinearity",
    "section": "2.3 Feedback",
    "text": "2.3 Feedback\nIn dynamic systems, feedback means that stock changes depend on current stock levels.\n\nPositive feedback loops\nConsider, for example, the following system, pictorially,\n\n\n\nPositive Feedback Loop\n\n\nor mathematically,\n\\[ x_{t+1} = x_{t} + rx_{t} \\quad \\Leftrightarrow \\quad \\Delta x = rx\\]\nwith \\(r &gt; 0\\).\nGuess what will happen, given a positive initial stock value.\n\nThe stock will grow in a straight line.\nThe stock will grow faster, i.e., on an upward-bending curve.\nThe stock will grow slower, i.e., on a downward-bending curve.\n\nWe will find out.\nLet’s first define our new update_stock function.\n\ndef update_stock(stock, rate):\n    new_stock = stock + rate*stock\n    return new_stock\n\nTo run the model, i.e., to iterate the update_stock function, we define an iterate_model function.\n\ndef iterate_model(nr_timesteps, initial_value, update_func, **update_params):\n    stock = initial_value\n    time_series = [stock]\n    for t in range(nr_timesteps):\n        stock = update_func(stock, **update_params)\n        time_series.append(stock)\n    return np.array(time_series)\n\nThis function takes, the number of time steps, the initial stock value as input arguments. Furthermore, it takes an update_func function and flexible **update_params as arguments, which allows us to use different stock update functions. It returns a numpy array of stock values over time.\nThe stock value starts at initial_value. The list time_series is initialized with the starting stock value. This will store the stock value at each timestep.\nThen, a loop runs nr_timesteps times. The update_func function is called in each iteration with the current stock value and the update_parameters. The result is assigned to stock, updating its value. The new stock value is appended to the time_series list, which tracks the stock’s value at each timestep.\nFor convenience, we will also define a plot_stock_evolution function, plotting the stock evolution.\n\ndef plot_stock_evolution(nr_timesteps, initial_value, update_func,\n                         **update_parameters):\n    time_series = iterate_model(nr_timesteps, initial_value, \n                                update_func, **update_parameters)\n    plt.plot(time_series, '.-', label=str(update_parameters)[1:-1]);\n    plt.xlabel(\"Time steps\"); plt.ylabel(\"Stock value\");\n    return time_series\n\nThe plot_stock_evolution function calls the iterate_model function with the given parameters and plots the time_series on the axis, with the format '.-' (dots connected by lines) and a legend label indicating the parameters used. The str(update_parameters)[1:-1] converts the update_parameters dictionary into a string and, with [1:-1], removes the first and last characters ({ and } in the case of a dictionary).\nFinally, the function returns the time_series list, which contains the stock values at each timestep.\nFor example, let’s consider the phenomenon of CO2 emissions.\n\n\n\n        \n        \n\n\nOur stock will be the annual CO2 emissions. We assume that we start with 0.01 gigatons of CO2 emissions around 1750 and assume a constant growth rate of 3.3% per year. Where are we 250 years later?\n\nplot_stock_evolution(250, 0.01, update_func=update_stock, rate=0.033);\nplt.xlabel(\"Time steps [years]\"); plt.ylabel(\"Anthropogenic\\nCO2 emissions [GtC/year]\"); plt.legend();\n\n\n\n\nExponential growth\n\n\n\n\nWe reach a level of annual CO2 emissions that resembles the empirical observation; additionally, the trajectory aligns more closely with the empirical data than the linear growth above (Figure 2.1).\nWith the generality of dynamic system models in mind, we can regard the simple positive feedback loop as the meta-level systems structure of the great acceleration. The great acceleration (Steffen et al., 2015) refers to the hockey-stick-like growth of many socio-economic and environmental indicators since the mid-20th century (Figure 2.2).\n\n\n\n\n\n\nFigure 2.2: The great acceleration\n\n\n\nAs these developments are not necessarily positive in a normative sense, positive feedback loops are better called reinforcing feedback loops.\nThe worth of a formal model lies in enabling us to conduct “experiments” safely and at low cost. By adjusting the input parameters, we see how the output shifts. Overall, our goal is to gain deeper insights into how the output depends on the inputs.\nFor example, how does the output change if we cut the rate of change in half? How does the output change if we double the rate?\nWhat do you think?\n\nA halved rate leads to about a quarter of the stock at the end, and a doubled rate leads to about four times the stock at the end.\nA halved rate leads to about a 16th of the stock at the end, and a doubled rate leads to about four times the stock at the end.\nA halved rate leads to about a quarter of the stock at the end, and a doubled rate leads to about 16 times of the stock at the end.\nA halved rate leads to about a 10th of the stock at the end, and a doubled rate leads to about 10 times the stock at the end.\nA halved rate leads to about a 10th of the stock at the end, and a doubled rate leads to about 100 times of the stock at the end.\nA halved rate leads to about a 100th of the stock at the end, and a doubled rate leads to about 10 times of the stock at the end.\n\nLet’s find out.\n\nts_half = plot_stock_evolution(150, 0.1, update_stock, rate=0.033/2)\nts_norm = plot_stock_evolution(150, 0.1, update_stock, rate=0.033)\nts_doub = plot_stock_evolution(150, 0.1, update_stock, rate=0.033*2)\nplt.legend();\n\n\n\n\n\n\n\n\nThus, a halved rate leads to about a 10th of the stock at the end:\n\nts_norm[-1]/ts_half[-1]\n\n11.192852530716676\n\n\nAnd a doubled rate leads to about 100 times of the stock at the end:\n\nts_doub[-1]/ts_norm[-1]\n\n111.82334406335414\n\n\n\n“The greatest shortcoming of the human race is our inability to understand the exponential function” - Albert Allen Bartlett\n\nIn summary, in positive or reinforcing feedback loops, positive stock values cause the stock to increase proportionally to the stock level, leading to exponential growth.\n\n\nNegative feedback loops\nAt first glance, negative feedback loops appear quite similar to positive ones.\n\n\n\nNegative Feedback Loop\n\n\nHowever, here, positive stock values cause the stock to decrease proportionally to the stock value.\nMathematically, we write\n\\[ x_{t+1} = x_{t} - rx_{t} \\quad \\Leftrightarrow \\quad \\Delta x = -rx\\]\nwith \\(r &gt; 0\\).\nGuess what will happen, given a positive initial stock value.\n\nThe stock will shrink in a straight line to \\(-\\infty\\).\nThe stock will shrink in a straight line to 0.\nThe stock will shrink faster, i.e., on a downward-bending curve to \\(-\\infty\\).\nThe stock will shrink faster, i.e., on a downward-bending curve to 0.\nThe stock will shrink slower, i.e., on an upward-bending curve to \\(-\\infty\\).\nThe stock will shrink slower, i.e., on an upward-bending curve to 0.\n\nLet’s find out.\nFortunately, there’s no need to create a new Python function. We can just insert negative growth rates into our current functions.\n\nts_half = plot_stock_evolution(150, 0.1, update_func=update_stock, rate=-0.033/2)\nts_norm = plot_stock_evolution(150, 0.1, update_func=update_stock, rate=-0.033)\nts_doub = plot_stock_evolution(150, 0.1, update_func=update_stock, rate=-0.033*2)\nplt.legend();\n\n\n\n\n\n\n\n\nThus, the stocks shrink faster than linearly toward zero, with more negative growth rates causing faster decay. This process is also called exponential decay.\nSince decay isn’t inherently negative in a normative sense—consider environmental degradation—it’s more accurate to refer to negative feedback loops as balancing feedback loops.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html#delays",
    "href": "02.01-Nonlinearity.html#delays",
    "title": "2  Nonlinearity",
    "section": "2.4 Delays",
    "text": "2.4 Delays\nSo far, we have considered only instantaneous feedback. The stock change at the next step was caused directly by the current stock level. This is not always the case.\nDelays are a common and crucial feature in many dynamic systems. They result from the time it takes for a signal to travel to a stock or, vice versa, for a stock to react to a signal.\nHow can we model the concept of delays in a dynamic system?\nIn short, we consider systems with multiple stocks.\n\nExample | Economy-Innovation interactions\nFor example, let’s consider the phenomenon of economic growth. The simplest model explanation is that economic development (e.g., measured by GDP) directly causes more economic development. A slightly refined model explanation might be that economic development causes more innovation (e.g., measured by number of patents), which in turn causes more economic development.\n\n\n\nTwo models of economic growth\n\n\nBoth models show a reinforcing feedback loop, so we should expect exponential growth again. But how do the rates of change relate to each other?\nLet’s first define a mathematical model. Let \\(x_t\\) be the level of economic development and \\(y_t\\) the level of innovations at time \\(t\\).\nModel 1: \\[x_{t+1} = x_t + rx_t\\] where \\(r&gt;0\\) denotes a positive growth rate.\nModel 2: \\[\\begin{align}\nx_{t+1} &= x_t + ay_t \\\\\ny_{t+1} &= y_t + bx_t\n\\end{align}\\] where \\(a&gt;0\\) denotes the rate of converting innovations to economic development and \\(b&gt;0\\) denotes the rate of converting economic development to innovations.\nNow, we convert the mathematical model into a computational model.\nFirst, we define the update_model functions.\n\ndef update_model1(x, r):\n    x_ = x + r*x\n    return x_\n\n\ndef update_model2(z, a, b):\n    x, y = z\n    x_ = x + a*y\n    y_ = y + b*x\n    return x_, y_\n\nSecond, we define two plot_evolution functions, detailing the plotting of the economic development and innovation levels.\n\ndef plot_model_evolution1(initial_value, nr_timesteps, **update_parameters):\n    time_series = iterate_model(nr_timesteps, initial_value, update_model1, \n                                **update_parameters)\n    plt.plot(time_series, '.--', label=\"Economy1 | \" + str(update_parameters)[1:-1],\n             color='purple');\n    return time_series\n\n\ndef plot_model_evolution2(initial_value_x, initial_value_y, nr_timesteps, \n                         **update_parameters):\n    z = [initial_value_x, initial_value_y]\n    time_series = iterate_model(nr_timesteps, z, update_model2, \n                                 **update_parameters)\n    plt.plot(time_series[:, 0], '.-', label=\"Economy2 | \"+str(update_parameters)[1:-1],\n             color='red');\n    plt.plot(time_series[:, 1], '.-', #label=\"Innovation2 | \"+str(update_parameters)[1:-1],\n             color='blue');\n    \n    return time_series\n\nLast, we define a compare_model function, which sets the initial levels of the economies as identical and has descriptive parameters for easy interpretation of the model results.\n\ndef compare_models(economy=1.0, innovation=1.0, timesteps=20, selfrate=0.2, innoTOecon=0.01, econTOinno=4.0,\n                  ymax=40):\n    plot_model_evolution2(economy, innovation, timesteps, a=innoTOecon, b=econTOinno);\n    plot_model_evolution1(economy, timesteps, r=selfrate);\n    plt.ylim(0, ymax)\n    plt.legend()\n\nWe set the default parameter so that in model 1, there is a default growth rate of 0.2. In model 2, we assume that innovations take a long time to result in economic development (i.e., the rate from innovation to the economy is small, a 20th compared to the default rate of model 1, to be precise). Even if the rate of converting economic development to innovation is 20 times larger than the default rate, economic growth in model 2 is slower than in model 1.\n\ncompare_models()\n\n\n\n\n\n\n\nFigure 2.3: Default Economy-Innovation interactions\n\n\n\n\n\nHow could be boost growth in model 2?\nOne way could be to increase the number of innovations.\nIt would require a 20-fold increase in the number of innovations to match the growth rate of model 1. This is also intuitive: if it takes 20 times longer for innovations to result in economic development, we need 20 times more innovations to achieve the same growth rate. Compare Figure 2.4 with Figure 2.3.\n\ncompare_models(innovation=20)\n\n\n\n\n\n\n\nFigure 2.4: Economy-Innovation interactions with a 20-fold increase in innovations\n\n\n\n\n\nIs there another way?\nHow much would the rate of converting economic development into innovations increase to match economic growth in model 1?\nHow much would we need to change the rate of converting innovations to economic development in model 2 to outperform the economic growth of model 1?\nWe need to increase either the innovation-to-economy or the economy-to-innovation rate by a factor of approx. 1.45 to match the sizes of the economies after 20 time periods (Figure 2.5).\n\nfig = plt.figure(figsize=(8,4))\nax1 = fig.add_subplot(121) # LEFT PLOT\ncompare_models(econTOinno=4.0*1.45)\nax2 = fig.add_subplot(122) # RIGHT PLOT\ncompare_models(innoTOecon=0.01*1.45)\n\n\n\n\n\n\n\nFigure 2.5: Economy-Innovation interactions with a 1.45-fold increase of a single conversion rate\n\n\n\n\n\nSince the innovation-to-economy rate is minimal initially, this might be the more accessible lever to pull.\n\nA key insight for policy interventions from dynamic system models is that it can be much more effective to intervene in the systems’ dynamics than in the systems’ state variables.\n\n\n\nExample | Economy-Nature interactions\nOne of the defining themes of this course is that we are embedded in the biosphere. Economic growth depends on an intact natural environment, whereas current economic practices negatively impact the state of nature. Let’s assume the following feedback structure.\n\n\n\n\n\n\nFigure 2.6: Economy-Nature interactions\n\n\n\nAssuming the feedback structure defined in Figure 2.6, we can reuse our code block from above.\nLet’s assume that economic and natural capital start at a base level of 1. Economic growth depends positively on the state of natural capital (assuming a base rate of 0.1). In contrast, natural capital changes depend negatively on economic capital but on a slower timescale (let’s take a rate of 0.005). Of course, these parameters serve mainly illustrative purposes.\n\ndef plot_EconomyNature(economy=1.0, nature=1.0, timesteps=100, natTOecon=0.1, econTOnat=-0.005):\n    plot_model_evolution2(economy, nature, timesteps, a=natTOecon, b=econTOnat);\n    plt.legend()\n\nThe economy starts growing linearly while nature degrades. At around 60 periods, the economy reaches a maximum and enters a recession while nature continues to degrade ({#fig-EconomyNature100}).\n\nplot_EconomyNature()\n\n\n\n\n\n\n\nFigure 2.7\n\n\n\n\n\nWhat happens if we continue the simulation for 1000 periods?\n\nplot_EconomyNature(timesteps=1000)\n\n\n\n\n\n\n\nFigure 2.8\n\n\n\n\n\nWe observe ossilations in the levels of economic and natural capital. The economy grows, but the environment degrades, leading to an economic recession. The environment recovers, and the economy starts growing again, leading to another recession. This cycle repeats indefinitely.\nMathematically, this behavior can be explained by the fact, that both economic and natural capital can have negative values. It is not straitforward to interpret negative values in this context. In effect, a negative value of economic capital results in a net positive effect on natural capital, and a negative value of natural capital results in a net negative effect on economic capital, i.e., when enviornmental damages are high, the economy is likely to suffer.\nHow special is this ossilatory behavior? Is it due to the specific parameters we chose? Or is it a general feature of the model structure?\nTo study the possible behavior of a sytem a bit of theory is useful.\n\n\nDeepDive | Autonomous first-order systems are all you need\nHere, we make sure that we do note forget to analyze some system structures. We show that so-called autonomous first-order systems are all we need to model any dynamic system.\nIn this DeepDive, we answer the question of why we did not consider dynamic equations which take into account the system state of longer than just one time step ago and why we did not model systems which depend explicitly on time. The short answer is, we do not need to. These modifications will not give rise to fundamentally different behavior. We can always represent these modification by introducing additional state variables.\nLet us first introduce some terminology.\nA system is called first-order if it depends only on the state of the system at the previous time step.\nFirst-order system: A difference equation whose rules involve state variables of the immediate past only, \\[x_t = F(x_{t-1}).\\]\nHigher-order system: Anything else, \\[x_t = F(x_{t-1}, x_{t-2}, x_{t-3}, \\dots).\\]\nA system is called autonomous if it does not depend explicitly on time.\nAutonomous system: A dynamical equation whose rules don’t explicitly include time or any other external variables \\[x_t = F(x_{t-1}).\\]\nNon-autonomous system: A dynamical equation whose rules do include time or other external variables explicitly, \\[x_t = F(x_{t-1}, t).\\]\nNon-autonomous, higher-order difference equations can always be converted into autonomous, first-order forms, by introducing additional state variables.\nFor example, the second-order difference equation, \\[x_t = x_{t-1} + x_{t-2} \\quad \\text{aka the Fibonacci sequence}\\] can be converted into a first-order form by introducing a “memory” variable, \\[y_t = x_{t-1} \\Leftrightarrow y_{t-1} = x_{t-2}\\]\nThus, the equation can be rewritten as follows\n\\[\\begin{align}\nx_{t} &= x_{t-1} + y_{t-1} \\\\\ny_{t} &= x_{t-1}\n\\end{align}\\]\nThis conversion technique works for any higher-order equations as long as the historical dependency is finite.\nSimilarly, a non-autonomous equation \\[ x_t = x_{t-1} + t\\] can be converted into an autonomous form by introducing a “clock” variable, \\[ z_t = z_{t-1} + 1, z_0 = 0\\]\nThen,\n\\[x_t = x_{t-1} + z_{t-1}\\]\nTake-home message. Autonomous first-order equations can cover all the dynamics of any non-autonomous, higher-order equations. We can safely focus on autonomous first-order equations without missing anything fundamental.\n\n\nMatrix representation\nIn this section, we will see how to represent a system of difference equations in matrix form. This representation is useful for analyzing the system’s behavior, especially when the system has multiple stocks.\nLet’s consider a general model with two stock variables,\n\\[\\begin{align}\nx_{t+1} &= ax_t + by_t, \\\\\ny_{t+1} &= dy_t + cx_t.\n\\end{align}\\]\nWe can rewrite these equations with a matrix multiplication,\n\\[\n\\left(\\begin{matrix}\nx_{t+1} \\\\\ny_{t+1}\n\\end{matrix}\\right) =\n\\left(\\begin{matrix}\na & b \\\\\nc & d\n\\end{matrix}\\right)\n\\left(\\begin{matrix}\nx_{t} \\\\\ny_{t}\n\\end{matrix}\\right)\n\\]\nThis idea generalizes to any number of stock variables. Consider a system with \\(n\\) stocks, denoted by \\(x^1\\), \\(x^2\\), …, \\(x^n\\) and influence coefficients \\(a_{ij}\\), for \\(i,j \\in \\{1,2,\\dots,n\\}\\), denoting the influence stock \\(x^{2}_t\\) at time \\(t\\) has on the stock \\(x^{1}_{t+1}\\) at time \\(t+1\\). We can convert this logic into the following system of update equations,\n\\[\\begin{align}\nx^{1}_{t+1} &= a_{11} x^{1}_t + a_{12} x^{2}_t + \\dots + a_{1n} x^{n}_t \\\\\nx^{2}_{t+1} &= a_{21} x^{1}_t + a_{22} x^{2}_t + \\dots + a_{2n} x^{n}_t \\\\\n  \\vdots \\quad &= \\quad \\vdots \\qquad\\qquad \\cdots \\qquad\\qquad \\vdots \\\\  \nx^{n}_{t+1} &= a_{n1} x^{1}_t + a_{n2} x^{2}_t + \\dots + a_{nn} x^{n}_t.\n\\end{align}\\]\nEquivalently, we can summarize all stocks \\(x^1\\), \\(x^2\\), …, \\(x^n\\) into a vector \\(\\mathbf{x}\\) and all influence coefficients into a matrix \\(\\mathbf A,\\) with\n\\[\n\\mathbf{x} =\n\\left(\\begin{matrix}\nx^1 \\\\\nx^2 \\\\\n  \\vdots \\\\  \nx^n\n\\end{matrix}\\right),\n\\qquad\n\\mathbf{A} =\n\\left(\\begin{matrix}\na_{11} & a_{12} & \\dots  &a_{1n} \\\\\na_{21} & a_{22} & \\dots &a_{2n} \\\\\n  \\vdots & \\vdots & \\cdots &\\vdots \\\\  \na_{n1} & a_{n2} & \\dots &a_{nn}\n\\end{matrix}\\right).\n\\]\nDoing so simplifies the form of the update equation to\n\\[ \\mathbf x_{t+1} = \\mathbf A \\mathbf x_t. \\]\nSome definitions.\nWe call the number of state variables needed to specify the system’s state uniquely the degrees of freedom.\nA phase space of a dynamic system is the theoretical space where every state of the system is mapped to a unique spatial location.\nThus, the degrees of freedom of a dynamic system equals the dimensionality of its phase space.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html#long-term-behavior-and-stability-analysis",
    "href": "02.01-Nonlinearity.html#long-term-behavior-and-stability-analysis",
    "title": "2  Nonlinearity",
    "section": "2.5 Long-term behavior and stability analysis",
    "text": "2.5 Long-term behavior and stability analysis\nPlaying with a computer model is fun, but the range of possibilities becomes enormous too quickly.\nIn this section, we will obtain a so-called closed-form solution for the time evolution of our systems. This means we write down an equation that gives us the system state for each point in time without the need to iterate the difference equation forward. This is particularly useful if we want to understand the very long-term behavior of systems, as this would require many simulation steps. These steps are crucial to understanding what it means for a system state to be stable.\n\nClosed-form solution for 1D systems\nFor one-dimensional systems, we can write\n\\[ x_{t+1} = ax_t \\]\nThis means the system state at time \\(t=1\\) is \\(x_1 = ax_0\\). At time \\(t=2\\), the system state is \\(x_2 = ax_1 = aax_0 = a^2 x_0\\). Thus, generalizing this pattern yields the system state at time \\(t\\) to be\n\\[ x_t = x_0 a^t \\]\nThis means, we can directly calculate the system state at any point in time without the need to iterate the difference equation forward.\nFor example, let’s say we want to calculate only each 10th time step,\n\nt = np.arange(0, 101, 10)\nt\n\narray([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100])\n\n\nThe system state is,\n\nx_0 = 1.2; r = 0.05\nx_0 * (1 + r)**t\n\narray([  1.2       ,   1.95467355,   3.18395725,   5.18633085,\n         8.44798645,  13.76087974,  22.41502307,  36.51171064,\n        59.47372928,  96.87643806, 157.80150942])\n\n\nHere, we made use of the element-wise exponentiation of NumPy arrays. This means that each element of the array is raised to the power of the corresponding element of the other array. This a very convenient feature of NumPy, as it allows us to perform operations on arrays without the need for explicit loops.\nTo check, whether our closed-form solution works, we compare it to the simulation results.\n\ndef compare_solutions(initial_value=1.2, nr_timesteps=100, rate=0.05):\n    plot_stock_evolution(initial_value=initial_value, nr_timesteps=nr_timesteps,\n                         update_func=update_stock, rate=rate)\n    t = np.arange(0, nr_timesteps+1, 5); \n    plt.plot(t, initial_value*(1+rate)**t, 'X', color='red', label='Analytical solution');\n    plt.legend()\n\n\ncompare_solutions()\n\n\n\n\n\n\n\n\nTry it out with different parameter values and observe that the closed-form solution matches the simulation results perfectly.\n\n\nCobweb plots\nCobweb plots are a graphical tool to understand the dynamics of one-dimensional systems.\nThe idea is to plot the system state at time \\(t+1\\) against the system state at time \\(t\\). Including the system’s udpate function \\(F(x_t)\\), together with the identity line, \\(y=x\\), allows us to see how the system evolves over time. The next system state is obtained by a vertical line from the current state to the system update function. From this point, a horizontal line to the identity line makes the next system state, the current system state. Thus, the system evolution is represented by horizontal and vertical lines, hence the name, because the resulting picture resembles a cobweb.\n\ndef cobweb(update_func, initial_value, nr_timesteps=10, ax=None, **update_params):\n    x=initial_value; h=[x]; v=[x];  # lists for (h)orizontal and (v)ertical points\n    for _ in range(nr_timesteps):  # iterate the dynamical system\n        x_ = update_func(x, **update_params)  # get the next system's state \n        h.append(x); v.append(x_)  # going vertically (changing v)\n        h.append(x_); v.append(x_)  # going horizontially (changing h)\n        x = x_  # the new system state becomes the current state\n\n    fix, ax = plt.subplots(1,1) if ax is None else None, ax  # get ax\n    ax.plot(h, v, 'k-', alpha=0.5)  # plot on ax\n    if np.allclose(h[-2],h[-1]) and np.allclose(v[-1],v[-2]):\n        # if last points are close, assume convergence\n        ax.plot([h[-1]], [v[-1]], 'o', color='k', alpha=0.7)  # plot dot\n        \n    return h, v\n\nWe study the simple system \\(x_{t+1} = ax_t\\).\n\ndef Flin(x, a): return a*x\ndef plotF(a, x0=1.4):   \n    fix, ax = plt.subplots(1,2, figsize=(12, 3.5));  # axes and limits\n    ax[0].set_xlim(-1,2); ax[0].set_ylim(-1,2), ax[1].set_ylim(-1,2)\n\n    xs = np.linspace(-1, 2, 101); # plot F(x) and x\n    ax[0].plot(xs, Flin(xs, a), label=\"F(x)\"); ax[0].plot(xs, xs, label=\"x\")\n    ax[0].legend(); ax[0].set_xlabel('system state x'); ax[0].set_ylabel('system state x')\n\n    h,v = cobweb(update_func=Flin, initial_value=x0, a=a, nr_timesteps=20, ax=ax[0]);  # include cobweb\n    \n    plot_stock_evolution(initial_value=x0, nr_timesteps=20, update_func=Flin, a=a);\n    plt.xlabel(\"Time steps\"); plt.tight_layout()  # make axis fit nicely\n\nFor example, with \\(a=0.8\\), we observe the cobweb plot in Figure 2.9.\n\nplotF(a=0.8)\n\n\n\n\n\n\n\nFigure 2.9\n\n\n\n\n\nConvince yourself about the following observations:\n\n\\(\\ \\ \\ 1 &lt; a \\qquad \\ {}\\): Divergences to infinity\n\\(\\qquad \\ a= \\ \\ \\ 1\\): Conserved behavior\n\\(\\ \\ \\ 0 &lt; a&lt; \\ \\ \\ 1\\): Convergence to fixed point\n\\(-1&lt;a&lt; \\ \\ \\ 0\\): Convergence to fixed point with transient oscillatory behavior\n\\(\\qquad \\ a=-1\\): Conserved oscillatory behavior\n\\(\\qquad \\ a&lt;-1\\): Divergend ossiliaroty behavior\n\nWe can summarize these observations into three qualitatively distinct cases for the asymptotic behavior of linear systems.\n\n\\(|a|&lt;1\\): The system converges to fixed point\n\\(|a|&gt;1\\): The system diverges to infinity\n\\(|a|=1\\): The system is conserved\n\nHow do these observations generalize to multi-dimensional systems?\n\n\nMulti-dimensional phase space vizualization\nLet us first create a general, multi-dimensional update function.\n\ndef update_general_model(x, A): return A@x\n\nHere, the @ operator is used for matrix multiplication.\nThen, we create a plot_flow function, using the matplotlib.quiver function.\n\ndef plot_flow(A, extent=10, nr_points=11, ax=None):\n    if ax is None: _, ax = plt.subplots(1,1, figsize=(6,6))\n\n    x = y = np.linspace(-extent, extent, nr_points)  # the x and y grid points\n    X, Y = np.meshgrid(x, y)  # transformed into a meshgrid\n    \n    dX = np.ones_like(X); dY = np.ones_like(Y)  # containers for the changes\n    for i in range(len(x)):  # looping through the x grid points\n        for j in range(len(y)):  # looping through the y grid points\n            s = np.array([x[i], y[j]]) # the current state\n            s_ = update_general_model(s, A) # the next state\n            ds = s_ - s # the change in state\n            dX[j,i] = ds[0]  # capturing the change along the x-dimension\n            dY[j,i] = ds[1]  # captuaring the change along the y-dimension\n    \n    q = ax.quiver(X, Y, dX, dY, angles='xy')  # plot the result\n    ax.set_xlabel('x-stock level'); ax.set_ylabel('y-stock level')\n\nLet’s test our plot_flow function with a random two-by-two matrix.\n\nA = np.random.randn(2,2)\nplot_flow(A)\n\n\n\n\n\n\n\n\nNow, we can visualize the flow of any two-dimensional system, including a trajectory, in the phase space.\nWe create a plot_flow_trajectory function, which plots the flow of a system with a given matrix, and the trajectory of the system over time.\n\ndef plot_flow_trajectory(a=1,b=0.05,c=-0.05,d=1, nr_timesteps=200):\n    fix, ax = plt.subplots(1,2, figsize=(12, 4));  # axes and limits\n    # ax[0].set_xlim(-1,2); ax[0].set_ylim(-1,2), ax[1].set_ylim(-1,2)\n    \n    A = np.array([[a, b], [c, d]])\n    ts = iterate_model(nr_timesteps, [3, 2], update_general_model, A=A)\n\n    plot_flow(A, ax=ax[0])\n    ax[0].plot(ts[:,0], ts[:,1], '.-', label='Model trajectory', color='purple')\n    ax[0].set_xlim(-10, 10); ax[0].set_ylim(-10, 10);\n    \n    ax[1].plot(ts[:,0], '.-', label='x-stock level', color='red')\n    ax[1].plot(ts[:,1], '.-', label='y-stock level', color='blue')\n    ax[1].legend()\n    return ts\n\n\nplot_flow_trajectory();\n\n\n\n\n\n\n\n\nSuch a phase space visulaization is a powerful tool, connecting the time-evolution of a dynamic systems with a geometrical representation.\nIt allows us to understand the long-term behavior of a system, and eventually, how a system’s fate depends on its initial state.\n\n\nClosed-form solutions of multi-dimensional systems\nSimilarly to one-dimensional systems with direct feedback only, a closed-form solution to the time evolution of multi-dimensional systems with delays, \\(\\mathbf x_{t+1} = \\mathbf A \\mathbf x_{t}\\), has the form,\n\\[ \\mathbf x_{k} = \\mathbf A^k \\mathbf x_0\\]\nto calculate the system state at time \\(k\\) and study how the system behaves when \\(k \\rightarrow \\infty\\). The only problem is how to calculate the exponential of a matrix, \\(\\mathbf A^t\\).\nTo study the long-term behavior of multi-dimensional systems with delays, we turn the equation \\(\\mathbf x_k = \\mathbf A^k \\mathbf x_0\\) into a more manageable form. For this purpose, we utilize the eigenvalues and eigenvectors of matrix \\(\\mathbf A\\). To recap, eigenvalues \\(\\lambda_i\\) and eigenvectors \\(\\mathbf v_i\\) of \\(\\mathbf A\\) are the scalars and vectors satisfying,\n\\[ \\mathbf A \\mathbf v_i = \\lambda_i \\mathbf v_i.\\]\nThus, when applying an eigenvector to its matrix effectively turns the matrix into a scalar number (the corresponding eigenvalue). Raising a scalar number to a power is easy. If we repeatedly apply this technique, we get\n\\[ \\mathbf A^k \\mathbf v_i = \\mathbf A^{k-1} \\lambda_i \\mathbf v_i = \\mathbf A^{k-2} \\lambda_i^2 \\mathbf v_i = \\dots = \\lambda_i^k \\mathbf v_i.\\]\nDecomposable components. Last, we need to represent the initial system state \\(\\mathbf x_0\\) using the eigenvectors of matrix \\(\\mathbf A\\) as the basis vectors, i.e.,\n\\[\\mathbf x_0 = c_1 \\mathbf v_1 + c_2 \\mathbf v_2 + \\dots + c_n \\mathbf v_n,\\]\nwhere \\(n\\) is the dimension of the state space and the coefficients \\(c_1, c_2, \\dots, c_n\\) represent the vector \\(\\mathbf x_0\\) in the eigenvector basis of the \\(n \\times n\\) matrix \\(\\mathbf A\\).\nIn practice, most \\(n \\times n\\) matrices are diagonalizable2 and thus have \\(n\\) linearly independent eigenvectors. Therefore, we assume we can use them as the basis vectors to represent any initial state \\(\\mathbf x_0\\). Representing \\(\\mathbf x_0\\) in the eigenbasis of \\(\\mathbf A\\) gives us\n\\[\\begin{align}\n\\mathbf x_{k} &= \\mathbf A^k \\mathbf x_0 \\\\\n&=  \\mathbf A^k (c_1 \\mathbf v_1 + c_2 \\mathbf v_2 + \\dots + c_n \\mathbf v_n) \\\\\n&=  c_1 \\mathbf A^k \\mathbf v_1 + c_2 \\mathbf A^k \\mathbf v_2 + \\dots + c_n \\mathbf A^k \\mathbf v_n \\\\\n&=  c_1 \\lambda_1^k \\mathbf v_1 + c_2 \\lambda_2^k \\mathbf v_2 + \\dots + c_n \\lambda_n^k \\mathbf v_n \\\\\n\\end{align}\\]\nNow, we can clearly see that the system’s time evolution, \\(\\mathbf x_t\\), is described by a summation of multiple exponential terms of \\(\\lambda_i\\).\n\nDynamics of a linear system are decomposable into multiple independent one-dimensional exponential dynamics, each of which takes place along the direction given by an eigenvector. A general trajectory from an arbitrary initial condition can be obtained by a simple linear superposition of those independent dynamics.\n\nAn eigenvalue tells us whether a particular component of a system’s state (given by its corresponding eigenvector) grows or shrinks over time. * When the eigenvalue is greater than 1, the component grows exponentially. * When the eigenvalue is less than 1, the component shrinks exponentially. * When the eigenvalue is equal to 1, the component is conserved.\nDominant components and systems stability. In the long term, the exponential term with the largest absolute eigenvalue \\(|\\lambda_i|\\) will eventually dominate the others. Suppose \\(\\lambda_1\\) has the largest absolute value (\\(|\\lambda_1| &gt; |\\lambda_2|, \\dots, |\\lambda_n|\\)), and we factor our \\(\\lambda_1\\) from the closed-form solution for \\(\\mathbf x_t\\),\n\\[\n\\mathbf x_t =  \\lambda_1^t \\left(c_1  \\mathbf v_1 + c_2 {\\lambda_2^t \\over \\lambda_1^t} \\mathbf v_2 + \\dots + c_n {\\lambda_n^t \\over \\lambda_1^t} \\mathbf v_n \\right).\n\\]\nWe can see that, eventually, the first term will dominate,\n\\[\n\\lim_{t\\rightarrow \\infty} \\mathbf x_t \\approx \\lambda^t_1 c_1 \\mathbf v_1.\n\\]\nThe eigenvalue with the largest absolute value is known as the dominant eigenvalue, while its related eigenvector is termed the dominant eigenvector. This eigenvector determines the asymptotic direction of the system’s state. This means if a linear difference equation (\\(\\mathbf x_{t+1} = \\mathbf A \\mathbf x_t\\))’s coefficient matrix, \\(\\mathbf A\\), has a single dominant eigenvalue, its system state will eventually align with the direction of its corresponding eigenvector, no matter the initial state.\n\nIf the absolute value of the dominant eigenvalue is greater than 1, then the system will diverge to infinity, i.e., the system is unstable.\nIf less than 1, the system will eventually shrink to zero, i.e., the system is stable.\nIf it is precisely 1, then the dominant eigenvector component of the system’s state will be conserved with neither divergence nor convergence, and thus the system may converge to a non-zero equilibrium point.\n\nOscillating behavior. Where does oscillating behavior come from?\nIn short, when some eigenvalues of a coefficient matrix are complex numbers. Why? The answer lies in Euler’s Formula, which states that for any real number \\(x\\),\n\\[\ne^ix = \\cos(x) + i\\sin(x),\n\\]\nbridging the world of trigonometric functions (i.e., oscillations) with exponential functions (i.e., the closed-form solutions of linear difference equations). Thus, when some eigenvalues of a coefficient matrix are complex numbers, the resulting system’s behavior is rotations around the origin of the system’s phase space.\nThe meaning of the absolute values of those complex eigenvalues is still the same as before:\n\nif the eigenvalue’s absolute value is larger than one, \\(|\\lambda| &gt; 1\\), we have instability in the form of rotations with an expanding amplitude;\nif the eigenvalue’s absolute value is smaller than one, \\(|\\lambda| &lt; 1\\), we have stability in the form of rotations with a shrinking amplitude; and\nif the eigenvalue’s absolute value equals one, \\(|\\lambda| = 1\\), we have conservation in the form of rotations with a sustained amplitude.\n\nEigenvalue spectrum.\nFor higher-dimensional systems, various kinds of eigenvalues can appear in a mixed way; some of them may show exponential growth, some may show exponential decay, and some others may show rotation. This means that all of those behaviors are going on simultaneously and independently in the system. A list of all the eigenvalues is called the eigenvalue spectrum of the system (or just spectrum for short). The eigenvalue spectrum carries a lot of valuable information about the system’s behavior, but often, the most important information is whether the system is stable or not, which can be obtained from the dominant eigenvalue.\nHow to put this into practice/Python?\nWe use scipy.linalg.eig to calculate the eigenvalues and eigenvectors of a matrix.\nFrom the documentation (scipy.linalg.eig?) we note that it return two objects, an iterable of eigenvalues w and an iterable of eigenvalues v. The normalized eigenvector corresponding to the eigenvalue w[i] is the column v[:,i].\n\nevals, evecs = scipy.linalg.eig(A)\n\nFor example, the eigenvalues are\n\nevals\n\narray([ 0.90140379+0.j, -1.1914619 +0.j])\n\n\nThe eigenvectors are\n\nevecs\n\narray([[ 0.99099361,  0.60104809],\n       [-0.13390914,  0.79921285]])\n\n\nFor readbility, we store eigenvalues and eigenvectors in new variables.\n\neval1, eval2 = evals\nprint(eval1)\nprint(eval2)\n\n(0.9014037889457369+0j)\n(-1.1914619046393624+0j)\n\n\nSince the eigenvectors are return in the format in which they are return we need to transpose them to assign them to two separate variables.\n\nevec1, evec2 = evecs.T\n# We check that we did not make any mistake:\nprint(\"This should be zeros:\", evec1 - evecs[:,0])\nprint(\"This too:\", evec2 - evecs[:,1])\n\nThis should be zeros: [0. 0.]\nThis too: [0. 0.]\n\n\nWe can autmate such checks with the assert statement.\n\nassert np.allclose(evec1, evecs[:,0]), \"The first eigenvector is not correct\"\nassert np.allclose(evec2, evecs[:,1]), \"The second eigenvector is not correct\"  \n\nNow, we create a plot_eigenvectors function.\n\ndef plot_eigenvectors(a,b,c,d, extent=10, ax=None):\n        if ax is None: _, ax = plt.subplots(1,1, figsize=(6,6))\n\n        A = np.array([[a, b], [c, d]])\n        evals, evecs = scipy.linalg.eig(A)\n        eval1, eval2 = evals\n        evec1, evec2 = evecs.T\n\n        # plotting the real part of the eigenvectors\n        ax.plot([0, extent*evec1[0].real], [0, extent*evec1[1].real], '-', \n                lw=2, color='deepskyblue', \n                label='$|\\lambda_1|$ = {}'.format(np.abs(eval1).round(4)))\n        ax.plot([0, extent*evec2[0].real], [0, extent*evec2[1].real], '-', \n                lw=2, color='teal', \n                label='$|\\lambda_2|$ = {}'.format(np.abs(eval2).round(4)))\n\n        ax.legend(loc='upper right', bbox_to_anchor=(-0.15, 1))\n    \n\n\nplot_eigenvectors(a=1, b=1, c=1.0, d=0)\n\n\n\n\n\n\n\n\nPutting it all togehter, we observe how the eigenvector represent the long-term behavior of the system (Figure 2.10).\n\nplot_flow_trajectory_with_ev()\n\n\n\n\n\n\n\nFigure 2.10\n\n\n\n\n\n\n\nSummary | Systems with linear changes\n\n\n\n\n\n\n\n\n\n\\({}\\)\n\\(k\\) th-component is …\nif \\(\\lambda_k\\) is complex-conjugate, the \\(k\\)th-component is rotating around the origin\nif \\(\\lambda_k\\) is dominant\n\n\n\n\n\\(\\| \\lambda_k \\| &gt; 1\\)\ngrowing\nwith an expanding amplitude.\nsystem unstable, diverging to infinity\n\n\n\\(\\| \\lambda_k \\| &lt; 1\\)\nshrinking\nwith a shrinking amplitude.\nsystem stable, converging to the origin.\n\n\n\\(\\| \\lambda_k \\| = 1\\)\nconserved\nwith a sustained amplitude.\nsystem stable, dominant eigenvector component conserved,system may converge to a non-zero equilibrium point\n\n\n\n\nLinear dynamical systems can show only exponaential growth/decay, periodic oscillation, stationary states (no change), or their hybrids (e.g., exponentially growing oscillation) .\n\nSometimes they can also show behaviors that are represented by polynomials (or products of polynomials and exponentials) of time. This occurs when their coefficient matrices are non-diagonalizable. (Sayama, 2023)\nIn other words, these behaviors are signatures of linear systems. If you observe such behavior in nature, you may be able to assume that the underlying rules that produced the behavior could be linear.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html#non-linear-changes",
    "href": "02.01-Nonlinearity.html#non-linear-changes",
    "title": "2  Nonlinearity",
    "section": "2.6 Non-linear changes",
    "text": "2.6 Non-linear changes\nSystems with non-linear changes, often called just non-linear systems, are defined as systems who are not linear (\\(\\mathbf x_{t+1}=\\mathbf A \\mathbf x_t\\)). In other words, they are systems whose rules involve non-linear combinations of state variables.\nWhile linear systems exhibit relatively simple behavior (exponential growth/decay, periodic oscillation, stationary states (no change), or their hybrids (e.g., exponentially growing oscillation)), non-linear systems can exhibit a much wider range of behaviors, including chaotic dynamics, bifurcations, and limit cycles. As a result, there is no general way to obtain a closed-form solution for non-linear systems, making them much more challenging to analyze and predict than linear systems.\nThe logisitc map is a classic example of a one-dimensional nonlinear system. It is defined as\n\\[x_{t+1} = x_t + rx_{t}(1 - \\frac{x_{t}}{C}),\\]\nwhere \\(r\\) is the growth rate and \\(C\\) the carrying capacity. In Python, it can be implemented as follows:\n\ndef logistic_map(x, r, C=1): return x + r*x*(1-x/C)\n\nPlotting the logistic map for a growth rate of 0.25, and a carrying capacity of 3.0, results in the following time evolution:\n\nplot_stock_evolution(50, 0.01, update_func=logistic_map, r=0.25, C=3.0);\n\n\n\n\n\n\n\n\n\nFinding equilibrium points\nAn equilibirum point \\(x_e\\) (also called fixed points or steady states) is a point in the state space, where the system can stay unchanged over time.\n\\[x_e = F(x_e)\\]\nIn other words, if the system state is at an equilibrium point, it will remain there indefinitely. Fixed points are theoretically important as a meaningful reference when we understand the structure of the phase space. They are of practical relevance when we want to sustain the system at a certain desirable state.\nTo find the equilibrium points of a system, we need to solve the equation \\(x_e = F(x_e)\\) for \\(x_e\\). This can be done by setting \\(x_{t+1} = x_t = x_e\\) in the system’s update function and solving for \\(x_e\\).\nFor example, in the logistic map, we obtain,\n\\[\\begin{align}\nx_e &= x_{e} + rx_{e}(1 - \\frac{x_{e}}{C}) \\\\\n0 &= 0+ rx_{e}(1 - \\frac{x_{e}}{C}) \\\\\n0 &= rx_{e}(1 - \\frac{x_{e}}{C})\n\\end{align}\\]\nThis equation is fulfilled if either \\(x_e = 0\\) or \\(x_e = C\\). Thus, the logistic map has two equilibrium points, \\(x_e = 0\\) and \\(x_e = C\\).\nGraphically, fixpoints of an iterative map are the intersections between \\(F(x)\\) and \\(x\\).\n\nxs = np.linspace(-1, 6, 101)  # Resolution of the x axis\nplt.plot(xs, logistic_map(xs, 1.5, 5), label=\"F(x)\")  # Plot the map x_=F(x)\nplt.plot(xs, xs, label=\"x\")  # Plot the diagonal x_=x\nplt.legend();  # Include the legend\n\n\n\n\n\n\n\n\nWe can enrich this representation with the cobweb plot, which shows the system’s time evolution from an initial state to the equilibrium point.\n\nxs = np.linspace(-1, 6, 101)\nplt.plot(xs, logistic_map(xs, 1.5, 5), label=\"F(x)\")\nh,v = cobweb(logistic_map, initial_value=0.1, r=1.5, C=5,\n             nr_timesteps=100, ax=plt.gca()); \nplt.plot(xs, xs, label=\"x\")\nplt.legend();\n\n\n\n\n\n\n\n\n\n\nLinear stability in nonlinear systems\nUnfortunately, it is impossible to forcast the asymptotic behaviors of nonlinear systems in the same way as for linerar systems.\nHowever, the concept of stability in linear systems can be applied to equilibirum points of non-linear systems. &gt; The basic idea of linear stability analysis is to rewrite the dynamics of the system in terms of a small perturbation added to the equilibrium point of your interest.\nConsider the system \\(x_{t+1} = F(x_{t})\\) with steady state \\(x_e\\).\nTo analyze its stability around this equilibrium point, we switch our perspective from a global coordinate system to a local one. We zoom in and capture a small perturbation added to the equilibrium point, \\(z_t = x_t - x_e\\). Inserting \\(x_t\\) into the update equation, yields \\[ x_e + z_t = F(x_e+z_{t-1}).\\]\nSince \\(z\\) is small (by assumption), we can approximate the right-hand side as a Taylor expansion, \\[ x_e + z_t = F(x_e) + F'(x_e)z_{t-1} + O(z_{t-1}^2)  \\] where, \\(F'\\) is the derivative of \\(F\\) with respect to \\(x\\).\nUsing \\(x_e=F(x_e)\\), we obtain the simple linear difference equation, \\[z_t \\approx F'(x_e) z_{t-1}.\\]\n\nTo determine the stability of fixed points in non-linear systems, we need to look at the derivative of \\(F(x_e)\\) at the fixed point.\n\nThere are three qualitatively distinct cases for the linear stability of a steady state in a non-linear system.\n\n\\(|F'(x_e)|&lt;1\\): The equilibirum point \\(x_e\\) is stable.\n\\(|F'(x_e)|&gt;1\\): The equilibirum point \\(x_e\\) is unstable.\n\\(|F'(x_e)|=1\\): The equilibirum point \\(x_e\\) is neutral 3\n\nFor example, for the logistic map,\n\\[x_{t+1} = F(x_t) = x_{t} + rx_{t}(1 - \\frac{x_{t}}{C})\\]\nwe calculate the derivative of \\(F(x_t)\\) as\n\\[F'(x) = 1+  r - 2r\\frac{x}{C}.\\]\nAt the fixed points \\(x_e = 0\\) and \\(x_e = C\\), we have\n\\[F'(x)|_{x=0} = 1+r,  \\quad  \\text{and} \\quad  F'(x)|_{x=C} = 1-r.\\]\nGraphically, we obtain Figure 2.11.\n\nplot()\n\n\n\n\n\n\n\nFigure 2.11: Linear stability shown at the nonlinear logistic map.\n\n\n\n\n\nWe observe, that when \\(0&lt;r&lt;2\\), the system convergens to \\(x_e=C\\). When \\(r&gt;2\\), the system converges to \\(x_e=0\\). When \\(r&gt;2\\), \\(F'(x)|_{x=C} = 1 - r &lt; -1\\) causing unstable ossilations.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html#learning-goals-revisited",
    "href": "02.01-Nonlinearity.html#learning-goals-revisited",
    "title": "2  Nonlinearity",
    "section": "2.7 Learning goals revisited",
    "text": "2.7 Learning goals revisited\n\nDefine and describe the components of a dynamic system.\n\nAt their core, dynamic systems consist of stocks and flows.\n\nRepresent dynamic system models in visual and mathematical form.\n\nIn general, a dynamic system iterates via \\(\\mathbf x_{t+1} = F(\\mathbf x_t)\\).\n\nExplain the concepts of feedback loops and delays.\n\nReinforcing (positive) feedback loops lead to divergence/instability.\nBalancing (negative) feedback loops lead to convergence/stability.\nConsidering delays makes system more complicated.\n\nExplain two kinds of non-linearity and how they are related.\n\nDynamic systems with linear changes can be represented as \\(\\mathbf x_{t+1} = \\mathbf A \\mathbf x_t\\), and can exhibit non-linear behavior, such as exponential growth or decay, periodic osscilations, or stationary states, or their combinations.\nDynamic systems with non-linear changes can exhibit more kinds of behaviors.\n\nImplement dynamic system models and visualize model outputs using Python, to interpret model results.\nAnalyze the stability of equilibrium points in dynamic systems using linear stability analysis.\n\n\nBibliographical and Historical Remarks\nRaworth (2017) (Chapter 4) and Page (2018) (Chapter 18) provide great conceptual introductions to the topic without going into mathematical details.\nSayama (2023) heavily inspired some of the material in this chapter.\n\n\n\n\nPage, S. E. (2018). The model thinker: What you need to know to make data work for you. Basic Books.\n\n\nRaworth, K. (2017). Doughnut economics: Seven ways to think like a 21st-century economist. Chelsea Green Publishing.\n\n\nSayama, H. (2023). Introduction to the Modeling and Analysis of Complex Systems. https://math.libretexts.org/Bookshelves/Scientific_Computing_Simulations_and_Modeling/Book%3A_Introduction_to_the_Modeling_and_Analysis_of_Complex_Systems_(Sayama)\n\n\nSteffen, W., Broadgate, W., Deutsch, L., Gaffney, O., & Ludwig, C. (2015). The trajectory of the Anthropocene: The Great Acceleration. The Anthropocene Review, 2(1), 81–98. https://doi.org/10.1177/2053019614564785\n\n\nSterman, J. D., & Sweeney, L. B. (2007). Understanding public complacency about climate change: Adults’ mental models of climate change violate conservation of matter. Climatic Change, 80(3), 213–238. https://doi.org/10.1007/s10584-006-9107-5",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.01-Nonlinearity.html#footnotes",
    "href": "02.01-Nonlinearity.html#footnotes",
    "title": "2  Nonlinearity",
    "section": "",
    "text": "In mathematics, the term “dynamical system” is more commonly used. But there is also the related field of “system dynamics”, with its own scientific community. I don’t want to get into the details of the differences between these two fields. For the purpose of this course, we will use the term “dynamic system” to refer to the overarching ideas.↩︎\nThis assumption doesn’t apply to defective (non-diagonalizable) matrices that don’t have \\(n\\) linearly independent eigenvectors. However, such cases are rare in real-world applications because any arbitrarily small perturbations added to a defective matrix would make it diagonalizable. Problems with such sensitive, ill-behaving properties are sometimes called pathological in mathematical modeling.↩︎\nalso know as Lyapunov stable. More advanced nonlinear analysis is required to show that an equilibrium point is truly neutral.↩︎",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nonlinearity</span>"
    ]
  },
  {
    "objectID": "02.02-TippingElements.html",
    "href": "02.02-TippingElements.html",
    "title": "3  Tipping elements",
    "section": "",
    "text": "3.1 Motivation\nThink of the term “tipping point” in the context of sustainability. What do you associated with it? What does it mean? What are examples of tipping points in the context of human-environment interactions?",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tipping elements</span>"
    ]
  },
  {
    "objectID": "02.02-TippingElements.html#motivation",
    "href": "02.02-TippingElements.html#motivation",
    "title": "3  Tipping elements",
    "section": "",
    "text": "Tipping points, elements and regime shifts\n\n\n\nTipping points, elements and regime shifts\n\n\nThe concepts of tipping elements and regime shifts are closely related aspects of complex systems dynamics. While tipping elements refer more to the components of a system with the potential for abrupt change, regime shifts refer more to the actual transitions that occur when these elements cross their critical thresholds. Also, the term tipping elements is used more in the context of Earth system science, while regime shifts are used more in the context of social-ecological systems.\n\n\nClimate tipping risks\n\n\n\nClimate tipping risks\n\n\nClimate tipping points are thresholds in the Earth’s climate system that, such as a slight increase in global average temperature, when crossed, can lead to significant and potentially irreversible changes. These changes can trigger reinforcing feedback loops that push the system into a new equilibrium, potentially leading to severe consequences like accelerated ice melt or shifts in ocean currents. For instance, the collapse of the West Antarctic ice shelves is a potential climate tipping point that could lead to substantial sea level rise and other impacts. While some tipping points may be triggered within the 1.5-2°C Paris Agreement range, many more become likely at 2-3°C of warming (Armstrong McKay et al., 2022).\n\n\nGlobal Tipping Points\n\n\n\nParts of the earth system with tipping points\n\n\nWhile climate tipping points are specific to the Earth’s climate systems and their feedback mechanisms, global tipping points (Lenton et al., 2023) consider a wider array of interconnected systems, including human and ecological dimensions, highlighting the complex interplay between natural and societal changes. Natural tipping points may occur over the entire Earth system, from the Biosphere to the Cryosphere, the Oceans and the Atmosphere.\nCurrently, several major tipping points are at imminent risk due to global warming, with more projected as temperatures rise above 1.5°C. The cascading effects of these negative tipping points could overwhelm global social and economic systems, outpacing some countries’ adaptive capacities. Addressing these crises requires a transformative shift away from incremental changes towards a robust global governance framework that prioritizes rapid emission reductions and ecological restoration.\n\n\nSocial tipping points\nto the rescue?\n\n\n\nSocial tipping points\n\n\nSimultaneously, it’s crucial to identify and harness positive tipping points, where beneficial changes can become self-sustaining, potentially offsetting some negative impacts. There is an urgent need to build resilient societies capable of withstanding impending challenges and seizing opportunities for sustainable progress. The paradigm of ‘business as usual’ is obsolete; instead, a proactive approach to governance and global cooperation is essential to navigate towards a sustainable future, leveraging both the threats and opportunities posed by tipping points (Lenton et al., 2023).\n\n\nChallenges\nSo what exacltly are tipping elements and regime shifts?\nHow can we identify them?\nAnd how can we manage them?\nHere, the mathematics of bifurcations can help.\n\n\nLearning goals\nAfter this chapter, students will be able to:\n\nExplain the concept of a bifurcaton and how it relates to tipping points and regime shifts.\nExplain a simple dynamic system to model a tipping element or regime shift.\nExplain what an attractor, transient, basin of attraction and separatrix are.\nConduct a bifurcation analysis in a simple dynamic system using Python.\nConstruct a potential function and explain its role in bifurcation analysis.\nExplain and recognize hysteresis and its consequences for sustainability transitions.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tipping elements</span>"
    ]
  },
  {
    "objectID": "02.02-TippingElements.html#bifurcations-the-mathematics-of-tipping-elements",
    "href": "02.02-TippingElements.html#bifurcations-the-mathematics-of-tipping-elements",
    "title": "3  Tipping elements",
    "section": "3.2 Bifurcations | The mathematics of tipping elements",
    "text": "3.2 Bifurcations | The mathematics of tipping elements\nThe key question bifurcation theory addresses is: How does the system’s long-term behavior depend on its parameters?\nThe distinction between a system’s state and its parameters is crucial. The state of a system is the set of variables that describe the system at a given time, while the parameters are the constants that define the system’s behavior.\nOften, a small change in parameter values causes only a small or even no quantitative change in the system’s state. However, sometimes, a slight change in parameter values causes a drastic, qualitative change in the system’s behavior.\n\nA bifurcation is a qualitative, topological change of a dynamic system’s phase space that occurs when some parameters are slightly varied across their critical thresholds.\n\nHere, we cover the very basics of bifurcation theory in dynamic systems. These provide a rich understanding of tipping points and regime shifts in the sustainability sciences.\nWe start by importing the necessary libraries and setting up the plotting environment.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, interactive\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; plt.rcParams['grid.linewidth'] = 0.25; \n\nWe also inlcude some keys functions to simulate dynamic systems from 02.01-Nonlinearity.\nTo run the model we copy and refine an iterate_model function.\n\ndef iterate_model(nr_timesteps, initial_value, update_func, **update_params):\n    stock = initial_value\n    time_series = [stock]\n    for t in range(nr_timesteps):\n        stock = update_func(stock, **update_params)\n        if np.abs(stock)&gt;10e9: break  # stop the simulation when x becomes too large\n        time_series.append(stock)\n    return np.array(time_series)\n\nWe also copy define a plot_stock_evolution function, plotting the stock evolution.\n\ndef plot_stock_evolution(nr_timesteps, initial_value, update_func,\n                         **update_parameters):\n    time_series = iterate_model(nr_timesteps, initial_value, \n                                update_func, **update_parameters)\n    plt.plot(time_series, '.-', label=str(update_parameters)[1:-1]);\n    plt.xlabel(\"Time steps\"); plt.ylabel(\"System state\");\n    return time_series\n\nAnd last, we copy the cobweb plot function over.\n\ndef cobweb(F, x0, params, iters=10, ax=None):\n    h=[x0]; v=[x0]; x=x0  # lists for (h)orizontal and (v)ertical points\n    for _ in range(iters):  # iterate the dynamical system\n        x_ = F(x, **params)  # get the next system's state\n        if np.abs(x)&gt;10e9: break  # stop the simulation when x becomes too large\n        h.append(x); v.append(x_)  # going vertically (changing v)\n        h.append(x_); v.append(x_)  # going horizontially (changing h)\n        x = x_  # the new system state becomes the current state\n\n    fix, ax = plt.subplots(1,1) if ax is None else None, ax  # get ax\n    ax.plot(h, v, 'k-', alpha=0.5)  # plot on axv\n    if np.allclose(h[-2],h[-1]) and np.allclose(v[-1],v[-2]):\n        # if last points are close, assume convergence\n        ax.plot([h[-1]], [v[-1]], 'o', alpha=0.7)  # plot dot\n        \n    return h, v\n\n\nA minimal model of tipping elements\nLet \\(x\\) denote the property of a system we are interested in, such as the amount of ice in the Arctic, the population of a species, or the fraction of a lake’s surface coverd by vegetation (Scheffer et al., 2001). Thus, we describe the system’s state over time \\(t\\) by \\(x_t\\).\nConceptually, the system’s dynamics are influenced by a reinforcing feedback loop, a balancing feedback loop, an external influence \\(c\\). The exertnal influence \\(c\\), for example, represents the global mean temperature in the case of climate tipping elements, or the level of nutrients in the example of a lake regime shift.\n\n\n\nTipping element model\n\n\nA simple mathematical representation of such a system has the difference euqation,\n\\[\\Delta x = (x - ax^3 + c) \\frac{1}{\\tau},\\]\nwhere \\(\\tau\\) represents the typical time scale of the system, and thus, inverse strength of the system’s change, and \\(a\\) is a parameter that determines the strength of the balancing feedback loop in relation to the reinforcing feedback loop (with unit stength).\nWe define the update_stock function, \\(F(x_{t}\\) for the udpate \\(x_{t+1}=F(x_t)\\), to iterate the stock \\(x\\) according to the difference equation above.\n\ndef F_tipmod(x, c, a=1, tau=10): return x + (x - a*x**3 + c)/tau\n\nWe explore the stock’s evolution over time from two different initial conditions by iterating the model for 200 time steps.\n\ndef compare_initial_conditions(nr_timesteps=200, c=0.0, tau=50, a=1):\n    plot_stock_evolution(nr_timesteps, 1.2, F_tipmod, c=c, tau=tau, a=a);\n    plot_stock_evolution(nr_timesteps, -1.2, F_tipmod, c=c, tau=tau, a=a);\n    paramstring = f\"c={c}, tau={tau}, a={a}\"\n    plt.gca().annotate(paramstring, xy=(0, 1.0), xycoords='axes fraction', va='bottom', ha='left'); plt.show()\n\n\ncompare_initial_conditions()\n\n\n\n\n\n\n\n\nWe observe bi-stability. Depending on where the dynamical system starts, it will either converge to the fixed point \\(x_e = 1.0\\), or the the fixed point \\(x_e = -1.0\\)\nHow do the parameters influence the system’s evolution? We can convince ourselves that the timescale parameter \\(\\tau\\) determines the speed of the system’s evolution (vary \\(\\tau\\) and the total number of simulation staps proportionally: the curves’s shapes look identical). The parameter \\(a\\) scales the system’s fixed points (vary \\(a\\) and observe the system’s behavior). Finally, We can also observe, that the external influence \\(c\\) can change the system’s equilibrium state. Run this notebook interactivly and confirm these observations for yourself!\n\n\nCobweb plot\nLet us observe thhis phenomenon of bi-stability in a cobweb plot.\n\ndef cobweb_plot(c=0, tau=1.5, a=1): \n    xs = np.linspace(-2,2,101); plt.xlabel('x'); plt.ylim(-1.3,1.3); plt.xlim(-1.6,1.6);\n    plt.plot(xs, F_tipmod(xs, c,a,tau), label='F(x)'); \n    plt.plot(xs, xs, label='x', color='k', alpha=0.5); plt.legend();\n    cobweb(F_tipmod, x0=0.3, params=dict(c=c, a=a, tau=tau), iters=100, ax=plt.gca());\n    cobweb(F_tipmod, x0=-0.3, params=dict(c=c, a=a, tau=tau), iters=100, ax=plt.gca());\n\n\ncobweb_plot()\n\n\n\n\n\n\n\n\nWe see, that it depends on where the update function \\(F(x_t)\\) intersects the diagonal line \\(y=x\\) whether an inital condition converges to the fixed point \\(x_e = 1.0\\) or \\(x_e = -1.0\\). The external influence parameter \\(c\\) determines this intersection point\nSome definitions.\nAn attractor is a set of states toward which a dynamic system tends to evolve over time. These states represent the system’s long-term behavior. Once the system reaches an attractor, it typically remains there. For example, in the system above the attractors are the fixed points \\(x_e = 1.0\\) and \\(x_e = -1.0\\).\nA transient refers to the behavior of a system during a limited period of time before it reaches an attractor. For example, the cobweb plot shows the transient behavior of the system.\nA basin of attraction the set of all the initial states from which you will eventually end up falling into that attractor. For example, in the system above, the basin of attraction for the fixed point \\(x_e = 1.0\\) are all point greater than the intersection point between \\(F(x_t)\\) and \\(y=x\\). The basin of attraction for the fixed point \\(x_e = -1.0\\) are all points less than the intersection point between \\(F(x_t)\\) and \\(y=x\\).\nIf there are more than one attractor in the phase space, you can divide the phase space into several different regions. In this case, a separatrix is the boundary between distinct basins of attractions. For example, in the system above, the separatrix consists only of the intersection point between \\(F(x_t)\\) and \\(y=x\\).\n\n\nEmpirical bifurcation diagram\nA bifurcation diagram is a powerful tool to visualize the system’s long-term behavior as a function of its parameters. To create a bifurcation diagram, we iterate the model for a range of parameter values and plot the system’s equilibrium states.\n\ndef simulate_bifurcation_diagram(F, x0s, params, iters=1000, cextent=[-0.5,0.5],\n                                 pointsize=2.0):\n    c_s = np.linspace(cextent[0], cextent[1], 501) # The external parameter to be varied\n    for x0 in x0s:  # Loop through all initial conditions\n        endpoints = []  # Container to store the endpoints\n        for c in c_s:  # Loop through all external parameter values\n            trj = iterate_model(iters, x0, F, c=c, **params)  # Simulate the system\n            endpoints.append(trj[-10:])  # Taking the last 10 points of the trajectory\n        # Plotting the endpoints\n        cpoints = [[c_s[i]]*l for i, l in enumerate(map(len, endpoints))]  # create cpoints that may work for different endpoint lengths\n        plt.scatter(np.hstack(cpoints), np.hstack(endpoints), c='k', alpha=0.5,\n                    s=pointsize);  # np.hstack unpacks everything      \n    plt.ylabel(r'Equilibrium state $x$'); plt.xlabel(r'External influence $c$')\n\n\nsimulate_bifurcation_diagram(F_tipmod, x0s=[-1.5, 1.5], params=dict(tau=10 , a=1.0), iters=1000)\n\n\n\n\n\n\n\n\nThis empirical bifurcation diagram allows us to identify the system’s stable fixed points as a function of the parameter \\(c\\). We observe the range of parameter values for which the system converges to the fixed points around \\(x_e = 1.0\\) and \\(x_e = -1.0\\), as well as the range where the system is bi-stable. We also observe the critical values of \\(c\\) where the system undergoes a qualitative change in its behavior. When the external parameter \\(c\\) changes around these critical values in \\(c\\) (close to \\(-0.4\\) and \\(0.4\\) here), a tiny change causes a drastic effect on the system state.\n\n\nConducting a bifurcation analysis\nLocal bifurcations occur when the stability of an equilibrium point changes between stable and unstable.\n\nDetermine the equilibirum points in dependence of the model parameters\nDetermine the stability of the equilibirum points in dependence of the model parameters. For one-dimensional systems \\(x_{t+1} = F(x_{t})\\), an equilibrium point is stable when \\(|F'(x_e)| &lt; 1\\).\nBifurcations occur at parameter values at which the stability changes. For one-dimensional systems \\(x_{t+1} = F(x_{t})\\), local bifurcations occur when \\(|F'(x_e)|=1\\).\n\n\n\nStep 1 | Equilibrium points\nThe equilibrium points for \\(\\Delta x = \\frac{1}{\\tau}(x - ax^3 + c)\\) fulfill,\n\\[c = ax^3 - x.\\]\nIt is not straightforward to solve the equation, \\(c=ax^3-x\\), analytically, i.e., to give an expression for how the system’s equilibirum depends on the parameters \\(c\\) and \\(a\\). However, we can plot the parameter \\(c\\) as a function of the equilibirum points \\(x_e\\) and the parameter \\(a\\).\n\ndef plot_equilibirum_points_tipmod(a=1.0, cextent=[-2.0,2.0]):\n    xe=np.linspace(-2.0,2.0,501) # equilibrium points\n    c = a*xe**3 - xe # parameter c\n    plt.plot(c, xe, \"--\"); # plot\n    plt.xlabel(r'External parameter $c$'); plt.ylabel(r'Equilibrium points $x_e$');\n    # plt.xlim(cextent); \n\n\nplot_equilibirum_points_tipmod()\n\n\n\n\n\n\n\n\n\n\nStep 2 | Stability\nComputing the derivative of the update function \\(F(x_t) = x + \\frac{1}{\\tau}(x - ax^3 + c)\\) , we find,\n\\[\\frac{dF}{dx} = 1+\\frac{1}{\\tau} (1 - 3ax^2).\\]\nWe create a Python function to plot the whether an equilibrium point is stable or not using the np.logical_and function.\n\ndef plot_stability_tipmod(a=1.0, tau=3.0, cextent=[-3.0,3.0]):\n    xe=np.linspace(-3, 3, 1001) # equilibrium points\n    c = a*xe**3 - xe # parameter c\n    \n    def F_(x, a,tau): return 1 + (1-3*a*x**2)/tau\n    cond=np.logical_and(F_(xe, a,tau)&lt;1, F_(xe, a,tau)&gt;-1)\n    plt.plot(c[cond], xe[cond], \".\", c='red')\n    \n    plt.xlabel(r'External parameter $c$'); plt.ylabel(r'Equilibrium points $x_e$');\n    plt.xlim(cextent); \n\nBrining stability and equilibrium points together, we can plot an analytical bifurcation diagram.\n\na = 1.0; tau=3.0\nplot_equilibirum_points_tipmod(a=a);\nplot_stability_tipmod(a=a, tau=tau)\n\n\n\n\n\n\n\n\n\n\nStep 3 | Bifurcation diagram\nWe enrich this bifurcation diagram by solving \\(\\frac{dF}{dx} = 1+\\frac{1}{\\tau} (1 - 3ax^2)\\) for \\(\\frac{dF}{dx} = 1\\) and \\(\\frac{dF}{dx} = -1\\) yields the stability boundaries\n\\[x_b=\\pm\\sqrt{\\frac{1}{3a}} \\quad \\text{and} \\quad x_b=\\pm\\sqrt{\\frac{2\\tau + 1}{3a}}.\\]\nWe create a Python function to plot the stability boundaries.\n\ndef plot_stability_boundaries_tipmod(a= 1.0, tau=3, cextent=[-3.0, 3.0]):\n    styl = dict(ls=\":\", lw=0.75, color='green')\n    plt.plot(cextent,[np.sqrt(1/(3*a)), np.sqrt(1/(3*a))], **styl)\n    plt.plot(cextent,[-np.sqrt(1/(3*a)), -np.sqrt(1/(3*a))], **styl) \n    plt.plot(cextent,[np.sqrt((2*tau + 1)/(3*a)), np.sqrt((2*tau + 1)/(3*a))], **styl)\n    plt.plot(cextent,[-np.sqrt((2*tau + 1)/(3*a)), -np.sqrt((2*tau + 1)/(3*a))], **styl)\n\nBringing all togehter, we obtain our analytical bifurcation diagram.\n\ndef plot_analytical_bifurcation_tipmod(a = 1.0, tau = 3.0, extent=3.0):\n    plot_equilibirum_points_tipmod(a=a, cextent=[-extent, extent]);\n    plot_stability_tipmod(a=a, tau=tau, cextent=[-extent, extent]);\n    plot_stability_boundaries_tipmod(a=a, tau=tau, cextent=[-extent, extent]);\n\n\na = 1.0; tau = 3.0\nplot_analytical_bifurcation_tipmod(a = a, tau = tau, extent=1.0)\n\n\n\n\n\n\n\n\nLastly, we can compare the empirical bifurcation diagram with the analytical bifurcation diagram, and observe that both match perfectly.\n\na = 1.0; tau = 3.0\nplot_analytical_bifurcation_tipmod(a = a, tau = tau, extent=2.0)\nsimulate_bifurcation_diagram(F_tipmod, x0s=[-0.5, 0.5], params=dict(tau=tau , a=a), \n                             iters=500, pointsize=30, cextent=[-2.0, 2.0])\n\n\n\n\n\n\n\n\nOur bifurcation analysis produces the same diagram we observed in the literature. What is still missing is the changing stability landscape portrayed in Figure 3.1?\n\n\n\n\n\n\nFigure 3.1: Conceptual Regime Shift\n\n\n\n\n\nPotential function\nA potential is a function that describes the energy of a system. In the context of dynamic systems, a potential function can help us understand the system’s behavior by visualizing the system’s energy landscape.\n\n\n\nIllustrations of potential functions\n\n\nIn general, there are multiple ways to define a potential function. Here, we define a potential function \\(G\\) as the negative integral of the system change \\(\\Delta x\\). Thus, for a system \\(x_{t+1} = F(x_{t}) = x_{t} - \\left.\\frac{G(x)}{dx}\\right|_{x=x_{t}}\\), we have\n\\[\\Delta x =  - \\frac{G(x)}{dx}.\\]\nThe idea is, that the system changes as if rolling downard (according to the first derivative of) the potential landscape \\(G(x)\\).\nThus, for the difference equation \\(\\Delta x = \\frac{1}{\\tau}(x - ax^3 + c)\\), we have\n\\[G(x) = - \\frac{1}{\\tau} \\left(\\frac{1}{2}x^2 - \\frac{1}{4}ax^4 + cx\\right).\\]\nConverting this into Python yields,\n\ndef G_tipmod(x, c,a,tau): return - (x**2/2 - a*x**4/4 + c*x)/tau\n\nwhich we use in a plot_potential function to visualize the potential landscape.\n\ndef plot_tipmod_potential(c=0.2, a=1.0, tau=2):\n    xs=np.linspace(-2,2,501); plt.ylim(-0.5, 0.5); \n    plt.plot(xs, G_tipmod(xs, c,a,tau), color='blue')\n    plt.ylabel(r'Potential $G(x)$'); plt.xlabel(r'System state $x$')\n    \n    #  numerically find and plot equilibrium points\n    c_ = a*xs**3 - xs\n    xeq = xs[np.isclose(c_-c, 0.0, atol=0.02)]\n    plt.plot(xeq, G_tipmod(xeq, c, a, tau), 'o', ms=12, color='k')\n\n\nplot_tipmod_potential()\n\n\n\n\n\n\n\n\nFinally, we bring all pieces together to visualize the system’s potential landscape, bifurcation diagram and time evolution.\n\ndef plot_all_tipmod(c=0.2, a=1.0, tau=2):\n    fig = plt.figure(figsize=(9, 4))\n    \n    fig.add_subplot(221)\n    plot_tipmod_potential(c=c, a=a, tau=tau)\n\n    fig.add_subplot(222)\n    plot_analytical_bifurcation_tipmod(a=a, tau=tau, extent=1.4)\n    plt.plot([c,c], [-2,2], \"-\", color='black')\n\n    fig.add_subplot(313)\n    compare_initial_conditions(nr_timesteps=50, c=c, tau=tau, a=a)\n    \n    plt.tight_layout();\n\n\nplot_all_tipmod();\n\n\n\n\n\n\n\n\n&lt;Figure size 2340x750 with 0 Axes&gt;\n\n\n\n\nHysteresis\nThe last phenomenon we want to explore is hysteresis. Hysteresis occurs when the system’s behavior depends on its history, i.e., the system’s current state depends on its past states.\nWe let our tipping element model iterate until it reaches an equilibirum point and then slighlty change the external influence parameter \\(c\\).\n\nWe start from a low value of external influence parameter \\(c\\) such that the system equilibriates toward the negative equilibrium point and then increae \\(c\\) into the range where only the positive equilibrium point is stable.\nThen, we decrease \\(c\\) back to the range where only the negative equilibrium point is stable.\n\n\ndef plot_hysteresis():\n    x=-1; xs = [] # inital condition and container for the system state\n    cvs = np.linspace(-0.8, 0.8, 101);  # values of parameter a to go through\n    cvs = np.concatenate((cvs, cvs[::-1])); # first we go up, then we go back down\n\n    for c in cvs: # looping through all parameter values\n        for _ in range(100): x=F_tipmod(x, c=c, a=1.0, tau=2.0);  # iterating the system 100 times\n        xs.append(x); # storing the last system state\n\n    plt.plot(cvs, xs,'-',alpha=0.5, color='gray',zorder=-1) # Plot background line\n    plt.scatter(cvs, xs, alpha=0.9, s=np.arange(len(cvs))[::-1]+1, c=np.arange(len(cvs)), cmap='viridis'); # Colorful plot\n    plt.xlabel(r\"Influence parameter $c$\"); plt.ylabel(r\"System state $x$\");\n\n\nplot_hysteresis()\n\n\n\n\n\n\n\n\nTime moves from large to small and dark to light dots.\nHysteresis is not only a theoretical construct. It occurs in many practical real-world domains from physics, chemistry, engineering, biology, to economics.\n\n\n\n\n\n\nFigure 3.2: Hysteresis of the Antarctic Ice Sheet\n\n\n\nThe hysteresis of the Antarctic Ice Sheet refers to the phenomenon where the ice sheet’s response to temperature changes is not symmetric; i.e., the thresholds for ice growth and decline differ significantly (Figure 3.2). This behavior has critical implications for understanding future sea-level rise under global warming scenarios.\nThe Antarctic Ice Sheet exhibits multiple temperature thresholds, beyond which ice loss becomes irreversible. For instance, at 2°C warming, West Antarctica faces long-term partial collapse due to marine ice-sheet instability. A significant loss of over 70% of the ice volume is anticipated with 6 to 9°C warming, primarily driven by surface elevation feedback (Garbe et al., 2020).",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tipping elements</span>"
    ]
  },
  {
    "objectID": "02.02-TippingElements.html#learning-goals-revisited",
    "href": "02.02-TippingElements.html#learning-goals-revisited",
    "title": "3  Tipping elements",
    "section": "3.3 Learning goals revisited",
    "text": "3.3 Learning goals revisited\nIn this chapter, we have explored the concept of a bifurcation and its significance in understanding tipping points and regime shifts. We examined how small changes in system parameters can lead to substantial shifts in behavior, highlighting the importance of bifurcations as precursors to critical transitions.\nWe then introduced a simple dynamic system model to represent a tipping element or regime shift, giving us a framework to analyze and simulate how systems behave under the influence of varying forces and feedback mechanisms. In this context, we explained key concepts such as attractors, transients, basins of attraction, and separatrices. These elements helped us understand the structure of the system’s state space, illustrating how it is shaped by stable and unstable regions.\nTo deepen our analysis, we conducted a bifurcation analysis, demonstrating how a system’s behavior changes as we adjust specific parameters. This analysis allowed us to identify potential tipping points and provided a practical approach to studying system dynamics.\nFurthermore, we constructed a potential function and examined its role in bifurcation analysis, as it represents the energy landscape and stability of a system. By analyzing the shape and contours of this potential function, we gained insight into where attractors are located and how the system may transition between states.\nFinally, we discussed hysteresis and its implications for sustainability transitions. We observed that hysteresis introduces a kind of path-dependence, where returning to an original state may require more than simply reversing parameter changes. This phenomenon has critical consequences for sustainability, underscoring the challenges in restoring systems after they have undergone significant transformations. Together, these insights equip us with a deeper understanding of complex system dynamics, emphasizing the importance of identifying and managing critical transitions effectively.\n\n\n\n\nArmstrong McKay, D. I., Staal, A., Abrams, J. F., Winkelmann, R., Sakschewski, B., Loriani, S., Fetzer, I., Cornell, S. E., Rockström, J., & Lenton, T. M. (2022). Exceeding 1.5\\(^\\circ\\)C global warming could trigger multiple climate tipping points. Science, 377(6611), eabn7950. https://doi.org/10.1126/science.abn7950\n\n\nGarbe, J., Albrecht, T., Levermann, A., Donges, J. F., & Winkelmann, R. (2020). The hysteresis of the Antarctic Ice Sheet. Nature, 585(7826), 538–544. https://doi.org/10.1038/s41586-020-2727-5\n\n\nLenton, T. M., Armstrong McKay, D. I., Loriani, S., Abrams, J. F., Lade, S. J., Donges, J. F., Milkoreit, M., Powell, T., Smith, S. R., Zimm, C., Buxton, J. E., Bailey, E., Laybourn, L., Ghadiali, A., & Dyke, J. G. (Eds.). (2023). The Global Tipping Points Report 2023. https://global-tipping-points.org\n\n\nScheffer, M., Carpenter, S., Foley, J. A., Folke, C., & Walker, B. (2001). Catastrophic shifts in ecosystems. Nature, 413(6856), 591–596. https://doi.org/10.1038/35098000",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tipping elements</span>"
    ]
  },
  {
    "objectID": "02.03-Resilience.html",
    "href": "02.03-Resilience.html",
    "title": "4  Resilience",
    "section": "",
    "text": "4.1 Motivation | Resilience in sustainability contexts\nThink of the term “resilience” in the context of sustainability and human-environment interactions. What does it mean to you? How can we model it? How can we measure it? What are the key challenges and opportunities in this field?",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Resilience</span>"
    ]
  },
  {
    "objectID": "02.03-Resilience.html#motivation-resilience-in-sustainability-contexts",
    "href": "02.03-Resilience.html#motivation-resilience-in-sustainability-contexts",
    "title": "4  Resilience",
    "section": "",
    "text": "Resilience everywhere\n\nCapacity of a system to cope with shocks\n\nfrom latin resiliō (“to spring back”)\nResilience is a widly used term in many different fields, from psychology to engineering, from ecology to social-ecological systems.\n\nPsychology\n\n\n\nhttps://www.mind-berry.com/wp-content/uploads/2023/06/Resilience-image.png\n\n\nsource: mind-berry.com | What is resilience?\nResilience is the capacity to recover from challenges and use them as learning opportunities. Resilient peole are perceived as having a positive outlook, handling difficulties calmly, and managing negative emotions effectively.\n\n\nEngineering\n\n\n\nResilience engineering\n\n\nsource: rote.se | Resilience engineering\nResilience in engineering refers to the ability of complex systems to anticipate, adapt to, and recover from unexpected disruptions or failures. This field emphasizes not just the prevention of failures but also the capacity to maintain functionality and performance in the face of unforeseen challenges.\n\n\nEcology\n\n\n\nResilience in ecology | the adaptive cycle\n\n\nsource: resalliance.org | Adaptive cycle\nResilience in ecology refers to the capacity of an ecosystem to endure disturbances while preserving its core functions, structures, and processes. This concept includes the ability to recover and adapt to environmental changes, allowing ecosystems to endure challenges and thrive.\n\n\nHuman-environment interactions\n\n\n\nResilience in human-environment interactions\n\n\nResilience in sustainability and human-environment interactions means the ability of social and ecological systems to absorb disturbances, adapt to changes, and maintain functionality. This concept is crucial as it highlights how communities and ecosystems withstand environmental stressors like climate change, pollution, and resource depletion, but also social stressors like economic crises, conflicts, and pandemics.\nAccording to (Reyers et al., 2022), resilience has reshaped sustainable development in six ways by 1) shifting focus from static capitals to dynamic capacities, 2) emphasizing relational processes over isolated objects, 3) prioritizing adaptive processes over fixed outcomes, 4) considering systems as open and interconnected rather than closed, 5) tailoring interventions to specific contexts rather than applying generic solutions, and 6) recognizing complex causality over linear cause-effect relationships. These shifts have led to innovative practices that better address the complexities of sustainability, although challenges remain in aligning practice with theoretical and methodological advancements in resilience science.\n\n\n\nResilience vs. dynamics\nResilience differs from merely being static or unchanging over time; resilient systems are often quite dynamic. Conversely, systems that remain constant over time can lack resilience. Acknowledging the difference between static stability and resilience is crucial (Meadows, 2009).\n\nStatic stability is observable; it can be assessed by analyzing changes in the system’s conditions over weeks or years.\nResilience, on the other hand, is often only noticeable when the system is pushed beyond its limits and breaks down. Because resilience may not be apparent without a systems view, individuals frequently prioritize stability, productivity, or other more immediately observable characteristics over resilience.\n\nFor example, just-in-time deliveries of products to retailers and parts to manufacturers have minimized inventory fluctuations and lowered costs across various industries. Nonetheless, this mode of operation has rendered the production system more vulnerable to disruptions in fuel supply, computer failures, labor shortages, and other potential shocks.\nAnother example constitutes the intensive management of European forests. Over centuries, it has transformed native ecosystems into single-age, single-species plantations, frequently composed of nonnative trees. These plantations aim to produce wood and pulp consistently over time. However, these forests have lost their resilience without multiple species interacting with each other and their environment. As a result, we are witnessing their vulnerability to threats such as industrial air pollution and pests like the bark beetle.\n\n\nResilience in the sustainability sciences\n\nResilience as a metaphor related to sustainability\nResilience and sustainability are closely related concepts in the context of social-ecological systems (SES). Resilience refers to the capacity of a system to absorb disturbances, adapt to changes, and maintain its core functions and structures. Sustainability, on the other hand, is the ability to meet the needs of the present without compromising the ability of future generations to meet their own needs. A resilient system can be more sustainable because it can withstand and adapt to shocks and stresses, ensuring long-term stability and functionality. Therefore, enhancing resilience is often seen as a key strategy for achieving sustainability.\n\n\nResilience as a property of dynamic systems\nDynamic systems are those that change over time, often in response to internal or external stimuli. Resilience in these systems is about how well they can absorb shocks and continue to operate effectively. For example, an ecosystem might experience a natural disaster but still maintain its biodiversity and functionality.\n\n\nResilience as a measurable quantity\nResilience is considered a measurable quantity through various indicators and metrics that capture the capacity of systems to absorb disturbances, adapt to changes, and maintain functionality. In field studies of social-ecological systems (SES), resilience can be assessed using indicators related to ecological, social, economic, and institutional dimensions. These indicators help researchers quantify resilience and understand how different systems respond to various stressors and shocks.\nIt is important to acknowledge these different meanings of resilience when discussing sustainability and human-environment interactions.\n\n\n\nResilience of what to what?\nThe resilience of what to what is a key question when applying the concept of resilience to sustainability and human-environment interactions (Carpenter et al., 2001).\nThe ‘of what’ refers to system function or configuration to be sustained, such as biodiversity, ecosystem services, social cohesion, or economic stability.\nThe ‘to what’ refers to the disturbances, shocks, or changes that the system needs to withstand, such as climate change, natural disasters, economic crises, or social conflicts.\n\n\nSpecified vs. general resilience\nAnother key distinction in resilience research is between specified and general resilience (Folke et al., 2010).\nSpecified resilience refers to the resilience of a system function to specific challenges or disturbances (i.e., a narrowly defined what to what). For example, - a community might have specified resilience to flooding by building dams and flood protection walls; - a farmer might use pest-resistant crops to increase resilience to specific pest infestations; or - an individual might get vaccinated against a particular disease to increase resilience to that disease.\nGeneralized resilience refers to the system’s capacity to deal with the unknown, uncertainty, and surprise (i.e., a broadly defined what to what). For example, - a community or society with functioning institutions and social networks has generalized resilience to various shocks and stresses; - a framer with a healthy business model and diversified crops has generalized resilience to various economic and environmental changes; - a functioning immune system can provide generalized resilience to a wide range of diseases;\n\n\nExample | Resilience of farming systems\n\n\n\nFramework to assess resilience of farming systems\n\n\n\n\nChallenges\nA lot of resilience scholarship utilizes qualitative methods, case studies, and conceptual frameworks (i.e., mental, verbal and pictorial models) to understand the dynamics of social-ecological systems. While these approaches are invaluable for generating diverse insights and hypotheses, they have difficulty in providing a precise understanding of resilience that allows for quantitative predictions and generalizable results in the sense of identifying universal system structures of relevance.\nHere, the mathematics of stochastic dynamics and bifurcations can help.\n\n\nLearning goals\nAfter this lecture, students will be able to:\n\nExplain how resilience concepts related within the context of sustainability science.\nImplement and simulate nosiy dynamic system models to illustrate different resilience types using Python.\nQuantify changes in resilience to measure when a systems approaches a tipping point.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Resilience</span>"
    ]
  },
  {
    "objectID": "02.03-Resilience.html#resilience-types",
    "href": "02.03-Resilience.html#resilience-types",
    "title": "4  Resilience",
    "section": "4.2 Resilience types",
    "text": "4.2 Resilience types\nWhile resilience, in general, is defined as the capacity of a system to absorb disturbances, adapt to changes, and maintain functionality, it is useful to differentiate between three types of resilience that can be distinguished based on the system’s response to stressors and shocks.\nThese three types are are often illustrated by ball-and-cup diagrams\n\n\n\nResilience types\n\n\nHow to formalize these concepts?\nWe start by importing the necessary libraries and setting up the plotting environment.\n\nimport numpy as np  \nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, fixed\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; plt.rcParams['grid.linewidth'] = 0.25; \n\n\nRobustness resilience\n\nThe capacity to resist (or absorb) change and continue to function in its present state\n\nRobustness resilience (sometime only called robustness (Anderies et al., 2013)) is the most established and straightforward type of resilience. It refers to the system’s ability to resist or absorb changes and continue to function in its present state. A system with high robustness resilience can withstand disturbances and shocks without significant changes to its structure or function. This type of resilience is often associated with stability and persistence in the face of external stressors.\nWhile it is the simplest form of resilience, it acknowledges that the future is inherently unpredictable. We cannot observe the current state to full precision. And we cannot process and extrapolate all the information and uncertainty. Therefore, robustness resilience is a key concept in the context of uncertainty and complexity.\n\nRobustness | Ball-and-cup diagram\nThe ball-and-cup pictorial model of the robustness resilience portrays a fixed cup (reprensting the potential) and a ball (representing the system state)\n\n\n\nRobustness resilience\n\n\nExternal shocks change the system state along the x-axis.\nHowever, this pictorial model leaves crucial questions unanswered:\n\nHow does the system state change over time?\nHow large and frequent are the shocks?\nWhat happens if the system state exceeds the cup?\n\nConverting this pictorial model into a mathematical model requires us to become more specific.\n\n\nRobustness | System dynamics and potenial\nWe formalize the idea of having a single basin of attraction by the following difference equation,\n\\[\\Delta x = x^3 - cx,\\]\nwhere \\(c\\) is a parameter that controls the system’s stability and \\(x\\) is the system state. As a side note, this model is also known as the normal form of a subcritical pitchfork bifurcation in dynamical systems theory (see exercise on Tipping Elements).\nIntegrating the negative difference equation, we obtain the potenial function \\(G(x)\\) by \\(\\Delta x =  - G(x)/dx\\) as\n\\[G(x) = \\frac{c}{2}x^2 - \\frac{1}{4}x^4.\\]\nConverting the potential function into Python yields,\n\ndef G_robustness(x, c): return c/2*x**2 - x**4/4\n\nWe devise a function to plot the potential function, togehter with the system’s equilibirum points and their stability (if \\(c&gt;0\\), \\(x_e=0\\) is stable and \\(x_e = \\sqrt{c}\\) and \\(x_e= -\\sqrt{c}\\) are unstable; if \\(c&lt;0\\), \\(x_e=0\\) is unstable; see the exercise on Tipping Elements).\n\ndef plot_robustness_potential(c=2):\n    xs=np.linspace(-2,2,101); plt.plot(xs, G_robustness(xs, c), '--', color='blue')\n    plt.xlabel(r\"System state $x$\"); plt.ylabel(r\"Potential $G(x)$\")\n\n    if c&gt;0:  # draw fixed points\n        xs=np.linspace(-np.sqrt(c), np.sqrt(c), 101); plt.plot(xs, G_robustness(xs, c), color='blue')\n        plt.scatter(np.sqrt(c), G_robustness(np.sqrt(c), c), s=200,  c='w', edgecolor='blue')\n        plt.scatter(-np.sqrt(c), G_robustness(-np.sqrt(c), c), s=200,  c='w', edgecolor='blue')\n        plt.scatter(0, G_robustness(0, c), s=200,  color='blue')\n    else:\n        plt.scatter(0, G_robustness(0, c), s=200,  c='w', edgecolor='blue')\n    plt.ylim(-0.2, 1.1)\n\n\nplot_robustness_potential()\n\n\n\n\n\n\n\n\nWe observe a single basin of attraction for the system state \\(x\\). For \\(c&gt;0\\), the unstable fixed points indicate where boundaries of the cup lie.\n\n\nRobustness | Stochastic dynamics\nTo account for shocks or external changes to the system, we refine the update equation as follows,\n\\[x_{t+1} = F_N(x_{t}) = F_D(x_{t}) + n\\eta_{t} =  x_{t} + (cx_{t} + x_{t}^3) + n\\eta_{t}.\\]\nThe new stochastic or nosiy udpate equation \\(F_N(x_t)\\) is composed of the orignial deterministic map \\(F_D\\), plus a stochastic random variable \\(\\eta_{t}\\) of mean zero. The paramter \\(n\\) regulates the strength of the noise term.\nWe model the shocks by a normally distributed random variable \\(n_{t}\\) with mean zero and unit variance. The corresponding Python function is,\n\ndef F_robustness_noise(x, c, n): return x + x**3 - c*x + n*np.random.randn()\n\nWe define a plotting function to illustrate the system dynamics under stochasticity. It also shows the basin of attraction, i.e., the region where the system state converges to the stable fixed point under the purely deterministic dynamics.\n\ndef plot_robustness_noise(noiselevel=0.1, c=1.0, xinit=0.5):\n    iters=250\n    params=dict(c=c, n=noiselevel)\n\n    x = xinit  # re-storing the initial values\n    trajectory = [x]  # container to store the trajectory\n    for t in range(iters):  # looping through the iterations\n        x_ = F_robustness_noise(x, **params)  # the ** notation extracts the dict. into the func. as parameters\n        if np.abs(x)&gt;3: break  # stop the simulation when x becomes too large\n        trajectory.append(x_)  # storing the new state in the container\n        x = x_  # the new state of the system `x_` becomes the current state `x`\n    \n    plt.plot(trajectory, 'k'); plt.xlabel('time steps'); plt.ylabel('system state');  # makes plot nice\n    plt.fill_between([0, iters], [-np.sqrt(c), -np.sqrt(c)],  [np.sqrt(c), np.sqrt(c)], color='blue', alpha=0.25)\n    plt.xlim(0,250); plt.ylim(-3,3)\n\nIf the noise level is low, the system is resilient to shocks and remains in the basin of attraction.\n\nplot_robustness_noise()\n\n\n\n\n\n\n\n\nHowever, if the noise level is high, the system can escape the basin of attraction and diverge.\n\nnp.random.seed(42); # fixing the random seed for reproducibility\nplot_robustness_noise(noiselevel=0.3)\n\n\n\n\n\n\n\n\nThe level of resilience of the system is the width between the unstable fixed points. This quantity gives the maximum magnitude of a shock the system can still tolerate.\nHowever, resilience in (social-)ecological systems is not always adequately described by this form of resilience.\nThis has lead scholars to broaden the meaning of resilience.\n\n\nRobustness | Real-world examples\nInfrastructure and technial systems, such as bridges, buildings, and buildings, are often engineered for robustness with a safety margin to withstand natural disasters like earthquakes or hurricanes. For example, buildings in earthquake-prone areas are constructed with materials and designs that allow them to absorb and dissipate seismic energy, minimizing damage and maintaining structural integrity. Or an elevater can carry more weight than its maximum load capacity to account for unexpected situations. However, these systems are not able to adapt to changing conditions or recover from severe damage without external intervention.\nRobust software systems are often designed to maintain functionality in the face of errors or unexpected inputs. However, this robustness might be achieved through rigid error-handling mechanisms that don’t necessarily scale or adapt to the severity of the issue. For instance, in cyber security, multi-layered security protocols, such as encryption, two-factor authentication, and fraud detection algorithms, help maintain the security and reliability of digital systems. However, these systems might not be able to adapt quickly to new types of cyber threats or changing regulatory requirements without significant investment and effort.\nIn the mobility sector, the german car industry’s adherence on combustion engines can also be seen as robustness resilience. The industry has been able to maintain its market share and profitability for some time despite increasing pressure to transition to electric vehicles. However, this robustness might not be sustainable in the long term as the industry faces challenges related to climate change, air pollution, and changing consumer preferences.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Resilience</span>"
    ]
  },
  {
    "objectID": "02.03-Resilience.html#adaptation-resilience",
    "href": "02.03-Resilience.html#adaptation-resilience",
    "title": "4  Resilience",
    "section": "4.3 Adaptation resilience",
    "text": "4.3 Adaptation resilience\n\nCapacity of a system to adjust its responses to changing external drivers and continue developing within the current stability domain or basin of attraction\n\n\nAdaptation | Ball-and-cup diagram\nThe ball-and-cup pictorial model of the adaptation resilience portrays a variable cup (reprensting the potential) and a ball (representing the system state). As before, shocks change the system state along the x-axis.\n\n\n\nAdaptation resilience\n\n\nIn adaptation resilience, the system can adjust its responses to changing external impacts. The capacity of a system to absorb shocks is linked to the strenght of the shocks. Adaptation resilience makes the resilience concept more flexible and adequate for (social-)ecological systems.\nHowever, this pictorial model leaves the crucial question of how the system adjusts its responses unanswered.\nConverting this pictorial model into a mathematical model requires us to become more specific. How could we convert the adaptation ball-and-cup diagram into a mathematical model?\n\n\nAdaptation | System dynamics\nWe start from our previous dynamic systems model,\n\\[x_{t+1} =  x_{t} + (x_{t}^3 - cx_{t}) + n\\eta_{t},\\]\nwhere \\(x\\) is the system state, \\(c\\) is the parameter that controls the system’s stability, and \\(n\\) is the parameter that regulates the strength of the noise term \\(\\eta_t\\).\nTo link the system’s responses to the external drivers, we introduce a feedback mechanism that adjusts the parameter \\(c\\) based on the magnitude of the shock. In other words, the parameter \\(c\\) becomes a function of the shock’s strength \\(c(n)\\).\n\\[x_{t+1} =  x_{t} + (x_{t}^3 - c(n) x_{t}) + n\\eta_{t}.\\]\n\n\nAdaptation | Feedback mechanism\nThe crucial question is how to formulate the feedback mechanism that adjusts the parameter \\(c\\) based on the magnitude of the shock \\(n\\).\nFrom a stability analysis of the subcritical pitchfork bifurcation, we know that the system is stable for \\(0&lt;c&lt;2\\) (see Tipping Elements Exercise). Thus, we want the maximal value of \\(c(n)\\) to be \\(2\\). This is an upper limit of how much noise the system can tolerate beyond which it cannot adapt anymore. We use the tanh function \\(\\tanh(x)\\) to achieve this, which results in values from \\(-1\\) to \\(1\\). Thus we shift it up by \\(1\\).\nFurthermore, we want the minimal value of \\(c(n)\\) at \\(n=0\\) to have a base level \\(b\\). This is a lower limit of how much noise the system can tolerate. Thus, we add \\(b\\) to the tanh function and devide the \\(\\tanh\\)-part by \\((2-b)/2\\) to scale it to the interval \\([b,2]\\).\nLast, we model the location where the \\(\\tanh\\) function switches from \\(b\\) to \\(2\\) by the parameter \\(l\\) and controll the steepness of the transition by the parameter \\(s\\).\nTogether, the feedback mechanism is formulated as,\n\\[c(a; b, s, l) = b + \\left(1+\\tanh\\big(s(a-l)\\big)\\right)\\frac{(2-b)}{2}.\\]\nIn Python, this function is implemented as,\n\ndef cfunc(n, base, loc, steep): return base + ((1+np.tanh(steep*(n-loc))))*(2-base)/2\n\nVisualizing the function c(a) for different parameters yields\n\ndef plot_cfunc(n, base=0.25, loc=0.5, steep=5): \n    plt.plot(n, cfunc(n, base, loc, steep), label=f\"b={base}, l={loc}, s={steep}\")\n    plt.xlabel(r\"Noise level $n$\"); plt.ylabel(r\"Control function $c(n)$\")\n\n\nn = np.linspace(0, 1.5, 101)\nplot_cfunc(n, base=0.6, loc=0.6, steep=4)\nplot_cfunc(n, base=0.4, loc=0.8, steep=8)\nplot_cfunc(n, base=0.2, loc=1.0, steep=12)\nplt.ylim(-0.05, 2.05); plt.legend();\n\n\n\n\n\n\n\n\nWe include this feedback mechanism in the dynamic systems update,\n\ndef F_adaptation_noise(x, n, base, loc, steep): \n    return x + x**3 - cfunc(n,base,loc,steep)*x + n*np.random.randn()\n\nand define a plotting function which illustrates the feedback mechansim of how the control parameter \\(c\\) responds to the noise strength \\(a\\) together with the time evolution of the system under stochasticity.\n\ndef plot_adaptation_noise(noiselevel=0.01, base=0.75, loc=0.5, steep=5.0):\n    iters=250; xinit = 0.5; ylim=(-1.5, 2.01)\n    params=dict(n=noiselevel, base=base, loc=loc, steep=steep)\n\n    fig = plt.figure(figsize=(10, 4))\n    basinstyle = {'color':'blue', 'alpha':0.25}\n                     \n    plt.subplot(1,2,1)                     \n    plt.plot(n, cfunc(n, base, loc, steep), label='$c(n)$', color='green')\n    plt.plot(n, np.sqrt(cfunc(n, base, loc, steep)), label='$\\sqrt{c(n)}$', **basinstyle)\n    plt.plot(n, -np.sqrt(cfunc(n, base, loc, steep)), **basinstyle)\n    plt.plot([noiselevel, noiselevel], [-2, 2], 'k--')\n    plt.ylim(ylim); plt.xlabel('Noise level $n$');\n    plt.legend()\n       \n    plt.subplot(1,2,2)                     \n    x = xinit  # re-storing the initial values\n    trajectory = [x]  # container to store the trajectory\n    for t in range(iters):  # looping through the iterations\n        x_ = F_adaptation_noise(x, **params)  # the ** notation extracts the dict. into the func. as parameters\n        if np.abs(x)&gt;3: break  # stop the simulation when x becomes too large\n        trajectory.append(x_)  # storing the new state in the container\n        x = x_  # the new state of the system `x_` becomes the current state `x`\n    \n    plt.plot(trajectory, 'k'); \n    plt.xlabel('Time steps $t$'); plt.ylabel('System state $x$');\n    cval = cfunc(**params)\n    plt.fill_between([0, iters], [-np.sqrt(cval), -np.sqrt(cval)],\n                     [np.sqrt(cval), np.sqrt(cval)], **basinstyle)\n    plt.xlim(0,250); plt.ylim(ylim)\n\nFor a small noise level \\(n\\), the system is resilient to shocks and remains in the basin of attraction, independent of the designed feedback mechanism.\n\nplot_adaptation_noise(noiselevel=0.01, base=0.5, loc=0.5, steep=5.0)\n\n\n\n\n\n\n\n\nFor large noise leves, the location \\(l\\) where the feedback mechanism kicks in becomes crucial. If \\(l\\) is too large, the system cannot adapt to the shocks and diverges.\n\nnp.random.seed(42); # fixing the random seed for reproducibility\nplot_adaptation_noise(noiselevel=0.35, base=0.5, loc=0.4, steep=5.0)\n\n\n\n\n\n\n\n\nDecreasing the location \\(l\\) allows the system to adapt to the shocks and remain in the basin of attraction.\n\nnp.random.seed(42); # fixing the random seed for reproducibility\nplot_adaptation_noise(noiselevel=0.35, base=0.5, loc=0.2, steep=5.0)\n\n\n\n\n\n\n\n\nIt is important to note, that the way we implemented adaptation resilience is just one of many possible ways to make this concept more precise. Instead of widening the basin of attraction with increased noise levels, the location of the basins minimum could be shifted gradually to areas with less noise.\nAdaptation resilience makes the resilience concept more flexible and adquate for (social-)ecological systems. However, sometimes, a system response to shocks by a complete reorganization, instead of just absorbing a shock.\n\n\nAdaptation | Real-world examples\nNatural ecosystems are being used as part of adaptation strategies to enhance resilience. For example, coastal mangrove forests show adaptation to sea level rise and storm surges. As water levels increase, mangroves accumulate sediment and organic matter to elevate their root systems, allowing them to keep pace with gradual sea level changes. This natural adaptation helps protect coastlines from erosion and storm damage (UNEP).\nEven infrastructure can be designed to adapt to changing conditions. Instead of building higher and more robust defenses, the Netherlands adopted a “Room for the River” strategy (Dutch Water Sector). The key idea is to restore the river’s natural flood plain in places where it is least harmful in order to protect those areas that need to be defended., i.e., to live with the water instead of fighting it: the strategy includes the lowering the levels of flood plains, creating water buffers, relocating levees, increasing the depth of side channels, and the construction of flood bypasses.\nIn the mobility sector, electric cars can be seen as another example of adaptation resilience. As the world shifts towards sustainable energy sources, electric vehicles are becoming more popular, replacing fossil fuel-powered cars. Yet, while the transition to electric vehicles requires a significant change in infrastructure, including charging stations, battery production, and recycling facilities, the dominance of private cars as a mode of transportation remains largely unchanged.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Resilience</span>"
    ]
  },
  {
    "objectID": "02.03-Resilience.html#transformation-resilience",
    "href": "02.03-Resilience.html#transformation-resilience",
    "title": "4  Resilience",
    "section": "4.4 Transformation resilience",
    "text": "4.4 Transformation resilience\n\nCapacity to create a fundamentally new system when ecological, economic, or social structures make the existing system untenable\n\n\nTransformation | Ball-and-cup diagram\nThe ball-and-cup pictorial model of the transformation resilience portrays multiple cups (reprensting the potential) and a ball (representing the system state). As before, shocks change the system state along the x-axis.\n\n\n\nTransformation resilience\n\n\nIn transformation resilience, the system can reorganize into a fundamentally new regimes, or state when the existing system state or regime becomes untenable. Thus, in contrast to the other resilience types, transformation resilience conceptualized one existing state and at least one new state. There must be at least two basins of attraction.\nHowever, this pictorial model leaves the crucial questions unswered how the basins of attraction are shaped, in addition to how the system state changes over time and how large and frequent the shocks are.\nConverting this pictorial model into a mathematical model requires us to become more specific.\nHow could we convert the transformation ball-and-cup diagram into a mathematical model?\nWe need a dynamical system with multiple stable states.\n\n\nTransformation | Alternative-stable-states system\nWe refine the system from the lecture on Tipping Elements with the difference equation,\n\\[\\Delta x = (x - ax^3 + c + n\\eta) \\frac{1}{\\tau},\\]\nwhere \\(\\eta\\) represents the noise term with mean zero and \\(n\\) the strength of the stochasticity. As before, \\(\\tau\\) represents the typical time scale of the system, and thus, inverse strength of the system’s change, and \\(a\\) is a parameter that determines the strength of the balancing feedback loop in relation to the reinforcing feedback loop (with unit stength). The parameter \\(c\\) represents the external driver that can push the system over the tipping point.\nAgain, we model the shocks by a normally distributed random variable \\(\\eta_{t}\\) with mean zero and unit variance. The corresponding Python function is,\n\ndef F_tipmod_noise(x, drive, shape=1, timescale=0.1, noiselevel=0): \n    return x + (x - shape*x**3 + drive + noiselevel*np.random.randn())/timescale\n\nWe define a plotting function to illustrate the system dynamics over time under stochasticity. We set the default values for the shape parameter \\(a=1\\) and the timescale paramter \\(\\tau=2\\).\n\ndef plot_transformation_trajectory(drive=-0.3, shape=1, timescale=2, noiselevel=0.0, initalstate=0.5):\n    iters=2500; params=dict(drive=drive, shape=shape, timescale=timescale, noiselevel=noiselevel)\n\n    x = initalstate  # re-storing the initial values\n    trajectory = [x]  # container to store the trajectory\n    for t in range(iters):  # looping through the iterations\n        x_ = F_tipmod_noise(x, **params)\n        if np.abs(x)&gt;3: break  # stop the simulation when x becomes too large\n        trajectory.append(x_)  # storing the new state in the container\n        x = x_  # the new state of the system `x_` becomes the current state `x`\n    \n    plt.plot(trajectory, 'purple'); plt.ylim(-1.4, 1.4)\n    plt.xlabel('Time steps $t$'); plt.ylabel('System state $x$');  # makes plot nice\n    return np.array(trajectory); \n\nWith the right system characteristics (i.e., its parameters) we observe a noise induced transition between the two stable states.\n\nnp.random.seed(0); # fixing the random seed for reproducibility\nplot_transformation_trajectory(drive=-0.3, noiselevel=0.15);\n\n\n\n\n\n\n\n\nWithout noise, the system converges and remains to the positive equilibrium point.\n\nplot_transformation_trajectory(drive=-0.3, noiselevel=0.0);\n\n\n\n\n\n\n\n\nUnder which conditions does the system transition between the two stable states as a result of the random shocks?\nHow can we understand better under what conditions the system switches between the two stable states under stochastic shocks?\n\n\nTransformation | Potential function\nWe make use of the potential function (see Tipping Elements) to improve our understanding of the system dynamics. As a reminder, the potential function \\(G(x)\\) is defined as the negative integral of the system change \\(\\Delta x\\). Thus, for the difference equation \\(\\Delta x = \\frac{1}{\\tau}(x - ax^3 + c)\\), we have\n\\[G(x) = - \\frac{1}{\\tau} \\left(\\frac{1}{2}x^2 - \\frac{1}{4}ax^4 + cx\\right).\\]\nIn Python, we have,\n\ndef G_tipmod(x, drive, shape, timescale): return - (x**2/2 - shape*x**4/4 + drive*x)/timescale\n\nTo visualize the potential function, we define\n\ndef plot_tipmod_potential(drive=-0.3, shape=1.0, timescale=2):\n    xs=np.linspace(-2,2, 501); plt.ylim(-0.5, 0.5); \n    plt.plot(xs, G_tipmod(xs, drive, shape, timescale), color='k')\n    plt.ylabel(r'Potential $G(x)$'); plt.xlabel(r'System state $x$')\n    \n    #  numerically find and plot equilibrium points\n    drive_ = shape*xs**3 - xs\n    xeq = xs[np.isclose(drive_-drive, 0.0, atol=0.02)]\n    plt.plot(xeq, G_tipmod(xeq, drive, shape, timescale), 'o', ms=12, color='k')\n\n\nplot_tipmod_potential()\n\n\n\n\n\n\n\n\n\n\nTransformation | Bifurcation diagram\nWe also visualize the bifurcation diagram (see Tipping Elements) to understand the system’s stability and the location of the tipping point.\n\ndef plot_bifurcation_tipmod(shape=1.0, timescale=2.0, cextent=[-1.4, 1.4]):\n    xe=np.linspace(*cextent, 1001) # equilibrium points\n    driver = shape*xe**3 - xe # parameter c\n    plt.plot(driver, xe, \"--\", color='k'); # equilibrim point\n    \n    # stability\n    def F_(x, shape, timescale): return 1 + (1-3*shape*x**2)/timescale\n    cond=np.logical_and(F_(xe, shape, timescale)&lt;1, F_(xe, shape, timescale)&gt;-1)\n    plt.plot(driver[cond], xe[cond], \".\", c='green')\n    \n    plt.xlabel(r'External driver $c$'); plt.ylabel(r'Equilibrium points $x_e$');\n    plt.xlim(cextent); \n\n\nplot_bifurcation_tipmod()\n\n\n\n\n\n\n\n\n\n\nTransformation | Combined analysis\nPutting all together shows us how potential, bifurcation diagramm and the noisy trajectories interact.\n\ndef plot_tranformation(drive=-0.3, shape=1, timescale=2, noiselevel=0.0, initalstate=0.5):\n    fig = plt.figure(figsize=(10,5))\n    \n    ax1 = fig.add_subplot(2,2,1)\n    plot_tipmod_potential(drive=drive, shape=shape, timescale=timescale)\n    \n    ax2 = fig.add_subplot(2,2,2)\n    plot_bifurcation_tipmod(shape=shape, timescale=timescale)\n    ax2.plot([drive , drive], [-1.5, 1.5], 'k-') # include driver value\n    ax2.set_xlim(-0.5, 0.5)\n    \n    ax3 = fig.add_subplot(2,1,2)\n    traj = plot_transformation_trajectory(drive=drive, shape=shape,\n        timescale=timescale, noiselevel=noiselevel, initalstate=initalstate)\n    \n    # include trajectory in the potential    \n    ax1.scatter(traj, G_tipmod(traj, drive=drive, shape=shape, timescale=timescale), \n                alpha=0.5, s=40, c=np.arange(len(traj)), cmap='plasma_r',zorder=10)\n\n    # include trajectory in the bifurcation diagram    \n    ax2.scatter(np.ones_like(traj)*drive, traj,\n                alpha=0.5, s=40, c=np.arange(len(traj)), cmap='plasma_r',zorder=10)\n    \n    plt.tight_layout()\n\nNow, we can reanalyze our sitation from above. When there is one fixed point dominating (i.e., having a larger basin of attraction), we observe a noise-induced transition to that fixed point. The timeseries is shown in the potential and the bifurcation diagram with time going from light to dark colors.\n\nnp.random.seed(0); plot_tranformation(drive=-0.3, noiselevel=0.15)\n\n\n\n\n\n\n\n\nWhere there is only one stable fixed point, also the stochastic system will evolve around that state.\n\nplot_tranformation(drive=-0.42, noiselevel=0.15)\n\n\n\n\n\n\n\n\nWe observe bistable flickering when both fixed points are stable and the noise is not too small and not too large.\n\nnp.random.seed(0); plot_tranformation(drive=0, noiselevel=0.45)\n\n\n\n\n\n\n\n\nThese results show that richness of phenomena that our relativly simple model can explain.\n\n\nTransformation | Real-world examples\nIn the mobility sector, moving away from system where private cars dominate to a system where public transportation, cycling, and walking are the primary modes of transportation can be seen as an example of transformation resilience. This shift requires a fundamental reorganization of the transportation system, including changes in infrastructure, policies, and social norms. While this transformation is challenging, it can lead to significant benefits, such as reduced traffic congestion, air pollution, and greenhouse gas emissions.\nIn the energy sector, transitioning from fossil fuels to renewable energy sources, such as solar, wind, and hydropower, is another example of transformation resilience. This shift requires a fundamental reorganization of the energy system, including changes in energy production, distribution, and consumption. While this transformation is complex and costly, it can lead to significant benefits, such as reduced greenhouse gas emissions, air pollution, and dependence on finite resources.\nIn the agriculture sector, transitioning from conventional farming practices to regenerative agriculture is another example of transformation resilience. This shift requires a fundamental reorganization of the food system, including changes in farming methods, land use, and food production. While this transformation is challenging, it can lead to significant benefits, such as improved soil health, biodiversity, and food security.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Resilience</span>"
    ]
  },
  {
    "objectID": "02.03-Resilience.html#quantifying-resilience",
    "href": "02.03-Resilience.html#quantifying-resilience",
    "title": "4  Resilience",
    "section": "4.5 Quantifying resilience",
    "text": "4.5 Quantifying resilience\nSo far, we conceptualized multiple facets of resilience through simple dynamical system models. While these models are crucial for understanding the underlying mechanisms of resilience, it is difficult to apply these models to real-world systems directly.\nFor instance, critical questions around resilience, tipping elements, and regime shifts of real-world systems is how resilient is the system? and how far away is the system from a tipping point?. These questions are challenging to answer in empirical systems because we cannot and do not want to trigger the regime shift to find out. Tipping elments are often hidden and uncertain.\nIn the following, we will discuss how we can quantify resilience and thus measure whether a system is approaching a tipping point, based on the conceptual models we discussed above.\n\nCritical slowing down\nCritical slowing down describes the phenomenon that the internal time scale a system operates on increases, when the system approaches as tipping point. As a consequence, a system close to a tipping point tends to undergo larger changes in response to perturbations and takes longer to recover from them.\nHow can we capture this phenomenon using a quantitative model?\nLet’s reuse the robustness resilience model, the deterministic subcritical pitch-fork bifurcation, \\(\\Delta x = x^3 - cx,\\) to illustrate that phenomenon.\n\nplot_robustness_potential()\n\n\n\n\n\n\n\n\nWe know, the fixed point at \\(x_e=0\\) is stable for \\(c&gt;0\\). Its basin of attraction extends from \\(-\\sqrt{c}\\) to \\(\\sqrt{c}\\).\nWe want to simulate how long in takes on average from all points in the basin of attraction to reach the fixed points.\nTo do so, we have to define a notion of convergence.\nLet’s look an exemplary trajectory:\n\nc = -1.2; x = 1.0\ntrajectory = [x]\nfor _ in range(100):\n    x_ = F_robustness_noise(x, c=1.2, n=0) # n=0 noiseless\n    trajectory.append(x_)\n    x = x_\ntrajectory = np.array(trajectory)\nplt.plot(trajectory);\n\n\n\n\n\n\n\n\nWhen it reached the equilibirum, it does not change any further.\nInvestigating the change between timesteps on a logarithmic axis,\n\nplt.plot(np.abs(trajectory[0:-1] - trajectory[1:])); plt.yscale('log');\n\n\n\n\n\n\n\n\nwe find, the changes do become smaller.\nBut it is sufficient for us to set a threshold tolerance level, below which we want to consider a trajectory as converged. Let’s use \\(10^{-9}\\).\n\ndef simulate_trajectory(xinit, c, threshold=10e-9, maxiter=10000):\n    x = xinit\n    trajectory = [x]\n    for _ in range(maxiter): \n        x_ = F_robustness_noise(x, c, n=0)\n        trajectory.append(x_)\n        if np.abs(x-x_) &lt; threshold:\n            break\n        x = x_\n    return np.array(trajectory)\n\nNote, we still keep a maximum number of iterations to not get stuck here, should the threshold never be reached.\nNow, plotting the simulated trajectory, we observe that it automatically stopped when the threshold was reached.\n\nplt.plot(simulate_trajectory(xinit=1.0, c=1.2));\n\n\n\n\n\n\n\n\nUsing this threshold, we can set up our simulation,\n\neps = 10e-9  # The numberical threshold to be use for a small quantity\ncvs = np.linspace(0+eps, 0.8, 501)  # The external parameters a to be varried\n\naverage_lens = []\nfor c in cvs:\n    xs = np.linspace(-np.sqrt(c)+eps, np.sqrt(c)-eps, 101)\n        # starting slightly off the unstable fixed points  \n    lens = [len(simulate_trajectory(xinit, c, threshold=eps, \n                                    maxiter=10000)) for xinit in xs]\n    assert max(lens) &lt; 10000\n    average_lens.append(np.mean(lens))\n\nto demonstrate the phenomenon of critical slowing down,\n\nplt.plot(cvs, average_lens, '.'); plt.ylabel('&lt;Timesteps to convergence&gt;'); plt.xlabel('External drive $c$');\n\n\n\n\n\n\n\n\nWhen we approach the tipping point at \\(c=0\\), the average number of timesteps it takes to converge to the equilibirum increases sharply.\n\n\nEarly-warning signals\nNext, we use the phenomenon of critical slowing down as a sign of resilience loss to create an early-warning signal. Early-warning signals are important because they allow us to anticipate critical transitions before they occur.\nEarly-warning signals are based on statistical indicators of the system behavior. Specifically, we will use the autocorrelation of the system’s time series. Autocorrelation is the correlation of a signal with a delayed copy of itself as a function of the delay. It measures the degree of similarity between a given time series and a lagged version of itself.\n\nConceptual model\nLet’s reuse the our tipping elment model showing alternative stable states,\n\\[\\Delta x = (x - ax^3 + c + n\\eta) \\frac{1}{\\tau},\\]\nwhere \\(\\eta\\) represents the noise term with mean zero and \\(n\\) the strength of the stochasticity. As before, \\(\\tau\\) represents the typical time scale of the system, and thus, inverse strength of the system’s change, and \\(a\\) is a parameter that determines the strength of the balancing feedback loop in relation to the reinforcing feedback loop (with unit stength). The parameter \\(c\\) represents the external driver that can push the system over the tipping point.\nWe create a synthetic time series, along which we slighlty reduce the resilience of one equilibirum by changing the drive parameter \\(c\\) with each iteration step.\n\nxinit=-1.1  # the system's initial condition\niters=1000  # how long to simulate\nparams=dict(b=0.5,c=0.5,d=0.05) # other parameter values\n\nnp.random.seed(0)  # fixing the random seed to make this reproducible\nc=0  # initial value of the 'resilience' parameter\ntrajectory = []  # container to store the trajectory\nx = xinit  # re-storing the initial values\nfor t in range(iters):  # looping through the iterations\n    x_ = F_tipmod_noise(x, drive=c, shape=1, timescale=2, noiselevel=0.25)\n    \n    # F(x, a=a, **params)  # the ** notation extracts the dict. into the func. as parameters\n    if np.abs(x)&gt;3: break  # stop the simulation when x becomes too large\n    trajectory.append(x_)  # storing the new state in the container\n    x = x_  # the new state of the system `x_` becomes the current state `x`\n    c += 0.00038  # we slightly increase c (i.e., reduce the resilience)\n\n\nplt.plot(trajectory, 'k'); plt.xlabel('time step t'); plt.ylabel('system state x');  # makes plot nice\n\n\n\n\n\n\n\n\nNow imagine, that we do not know the underlying model, but only have the time series. How can we detect the loss of resilience?\n\n\nScatter plot\nTo visualize the autocorrelation, (lag-1 temporal autocorrelation or AR(1) to be specific), we create a scatter plot of 200 points of our time series versus the 200 points of our time series at the next time step (lag-1). In this case, 200 is the size of our data window.\n\ndef scatter_autocorrelation(start=0): \n    fig, axs = plt.subplots(1,2, figsize=(12,2.8))  # creates the two axes\n    axs[0].scatter(trajectory[start:start+200], trajectory[start+1:start+201], s=10, c='blue'); \n    axs[0].set_xlabel(r'$x_{t+1}$'); axs[0].set_ylabel(r'$x_{t}$'); # makes first axis nice \n\n    axs[1].plot(trajectory, 'k');  # plot the time series\n    axs[1].fill_betweenx([-1.25, 1.5], [start, start], [start+200, start+200], color='blue', alpha=0.5) # show window\n    axs[1].set_xlabel('time steps'); axs[1].set_ylabel(r'system state $x$'); axs[1].set_xlim(0, 1000) # makes axis nice\n\nHow do we see the autocorrelation in this plot?\n\nscatter_autocorrelation(start=0)\n\n\n\n\n\n\n\n\nThe autocorrelation is the correlation between the time series and a lagged version of itself. Thus, it is the correlation between the x-axis and the y-axis of the scatter plot on the left.\nThen we slide our window of 200 system points through our time series. How does the scatter plot change? And what does the mean for the autocorrleation?\n\nscatter_autocorrelation(start=310)\n\n\n\n\n\n\n\n\n\nscatter_autocorrelation(start=620)\n\n\n\n\n\n\n\n\nWe visually can tell that the autocorrelation increases as we approach the tipping point.\n\n\nAutocorrelation\nTo quantify our visual understanding, we finally calculate the lag-1 temporal autocorrelation.\nFor that, we use the numpy.corrcoef function. Thus, the correlation matrix between the time series points from index 0 to 200 and from index 1 to 201 is given by,\n\nnp.corrcoef(trajectory[0:200], trajectory[1:201])\n\narray([[1.        , 0.20713096],\n       [0.20713096, 1.        ]])\n\n\nThus, we have to extract one of the off-diagonal elements,\n\nnp.corrcoef(trajectory[0:200], trajectory[1:201])[0,1]\n\n0.20713096137087478\n\n\nSliding through the time series from the beginning until the 600nd time step,\n\nAR1 = [np.corrcoef(trajectory[start:start+200], trajectory[start+1:start+201])[0,1] for start in range(0,599)]\n\n\nplt.plot(AR1); plt.ylabel('AR(1)'); plt.xlabel('Time step $t$');\n\n\n\n\n\n\n\n\nshows a clear rise of the lag-1 autocorrelation when approching the tipping point, indicating a loss of resilience.\nThis method can be used on time series data only. It does not require knowledge about the exact systems equation.\n\n\n\nExample | Greenland Ice Sheet\nA detected critical slowing down of its melt rates suggests that the western Greenland Ice Sheet is close to a tipping point (Boers & Rypdal, 2021).\n\n\n\nCritical slowing down in the Greenland Ice Sheet",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Resilience</span>"
    ]
  },
  {
    "objectID": "02.03-Resilience.html#learning-goals-revisited",
    "href": "02.03-Resilience.html#learning-goals-revisited",
    "title": "4  Resilience",
    "section": "4.6 Learning goals revisited",
    "text": "4.6 Learning goals revisited\nIn this chapter, we have explored the concept of resilience in the context of sustainability science and human-environment interactions.\nWe have studied different types of resilience, including robustness, adaptation, and transformation resilience, and how they can be modeled using dynamic systems theory.\nLast, we have examined how resilience can be quantified using early-warning signals based on the phenomenon of critical slowing down as indicators of system stability and resilience. By understanding these concepts and methods, we can better assess the resilience of social-ecological systems and anticipate critical transitions before they occur.\n\n\n\n\nAnderies, J. M., Folke, C., Walker, B., & Ostrom, E. (2013). Aligning Key Concepts for Global Change Policy: Robustness, Resilience, and Sustainability. Ecology and Society, 18(2). https://www.jstor.org/stable/26269292\n\n\nBoers, N., & Rypdal, M. (2021). Critical slowing down suggests that the western Greenland Ice Sheet is close to a tipping point. Proceedings of the National Academy of Sciences, 118(21), e2024192118. https://doi.org/10.1073/pnas.2024192118\n\n\nCarpenter, S., Walker, B., Anderies, J. M., & Abel, N. (2001). From Metaphor to Measurement: Resilience of What to What? Ecosystems, 4(8), 765–781. https://doi.org/10.1007/s10021-001-0045-9\n\n\nFolke, C., Carpenter, S., Walker, B., Scheffer, M., Chapin, T., & Rockström, J. (2010). Resilience Thinking: Integrating Resilience, Adaptability and Transformability. Ecology and Society, 15(4). https://doi.org/10.5751/ES-03610-150420\n\n\nMeadows, D. H. (2009). Thinking in systems: A primer. Earthscan.\n\n\nReyers, B., Moore, M.-L., Haider, L. J., & Schlüter, M. (2022). The contributions of resilience to reshaping sustainable development. Nature Sustainability, 1–8. https://doi.org/10.1038/s41893-022-00889-6",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Resilience</span>"
    ]
  },
  {
    "objectID": "02.04-StateTransitions.html",
    "href": "02.04-StateTransitions.html",
    "title": "5  State transitions",
    "section": "",
    "text": "5.1 Motivation | State-transitions models\nState-and-transition models are useful tools to explain the causes and consequences of ecosystem change.\nFigure 5.1 shows a state-transition model of sandy-loamy alluvails soils in the dry steppe of eastern central Mongolia.\nThe model describes the dynamics of the vegetation in the region. The vegetation can be in one of four states: a reference state, Forb decreased state, Stipa grandis decreased state, or Degraded state. The arrows indicate the possible transitions between the states (Biggs et al., 2021).\nFigure 5.2 shows possible state transitions between states of different land cover types in the Brazilian Amazon.\nThe thickness of the arrows indicates the probability of the transition. The land cover can be in one of five states: Annual crops, Forest, Dirty Pasture, Clean Pasture, Secondary Vegetation, plus an Other state representing all other possible land cover types (Müller-Hansen et al., 2017).\nState-and-transition models are often co-developed with stakeholders and are used as heuristic tools to understand the dynamics of ecosystems. They are also used in scenario development to explore possible futures.\nNote, that terms such as state-and-transition model and transition model have been widely used in the literature without a clear, formal definition (Daniel et al., 2016).\nThere are various computational variants of state-and-transition models. The simplest and most basic model is the one of a Markov Chain.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>State transitions</span>"
    ]
  },
  {
    "objectID": "02.04-StateTransitions.html#motivation-state-transitions-models",
    "href": "02.04-StateTransitions.html#motivation-state-transitions-models",
    "title": "5  State transitions",
    "section": "",
    "text": "Figure 5.1: State transitions in the dry steppe of Mongolia\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Land-cover transitions in Brazilian Amazon\n\n\n\n\n\n\n\n\nLearning goals\nAfter this lecture, students will be able to:\n\nName and explain the components of a Markov chain model and how the model relates to gernal dynamic system models to embed this model in the context of integrated nature-society models.\nSimulate and visualize Markov chain models stochastically, with ensembles, and via its state distribution\nCompute the stationary distribution of a Markov chain model numerically, analytically and explain the conditions for its existence to understand the long-term behavior of the model.\nInvestigate the transient behavior of a Markov chain model to understand the short-term behavior of the model.\nCompute the typical timesacle of a Marko chain transition to relate the model to real-world systems.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>State transitions</span>"
    ]
  },
  {
    "objectID": "02.04-StateTransitions.html#markov-chains",
    "href": "02.04-StateTransitions.html#markov-chains",
    "title": "5  State transitions",
    "section": "5.2 Markov chains",
    "text": "5.2 Markov chains\nMarkov chains model systems that transition probabilistically between a finite set of states.\nIn fact, Markov chains are a very general model that can be applied to all kinds of transitions. For example, a political system might transition between democratic and dictatorial, a market between volatile and stable regimes, or a person between happy, contemplative, anxious, and sad (Page, 2018, Chapter 17).\nThe movements between states occur according to fixed probabilities. The probability that a country transitions from authoritarian to democratic in a given year might be 5%; the likelihood that a person transitions from anxious to tired within an hour maybe 20%.\nNamed after Russian mathematician Andrey Markov, the essential element of the model is the Markov property. This property states that the probability of transitioning to any particular state depends solely on the current state and not on the history of states that preceded it.\nMore formally, we define a Markov chain by the following elements:\n\nA discrete set of states \\(\\mathcal S = \\{S_1, S_2, \\ldots, S_Z\\}\\).\nA transition matrix \\(\\mathbf T\\) with transition probabilities \\(T(i,j)\\), for \\(1 &lt; i,j &lt; Z\\), where \\(T(i,j)\\) is the probability of transitioning from state \\(S_i\\) to state \\(S_j\\).\nA disscrete-time index \\(t=0,1,2,\\dots\\).\nAn initial state distribution \\(\\mathbf p_0 = \\big(p_0(S_1), p_0(S_2), \\dots, p_0(S_Z) \\big)\\), with \\(p_t(s)\\) denoting the probability or fraction of state \\(s \\in \\mathcal S\\) at time \\(t\\).\n\nThus, the transition matrix \\(\\mathbf T\\) is a square matrix of size \\(Z \\times Z\\) with \\(\\sum_{s'} T(s, s') = 1\\), where \\(s\\) denotes the current, and \\(s'\\) the next state. Transition probabilites have to sum up to 1. We must go somewhere.\n\nA simple example\nLet us consider a simple example of a Markov chain with two states modeling a prosperous and a degraded state of Nature.\n\n\n\n\n\n\nFigure 5.3: A simple Markov chain\n\n\n\n\n\nComputational model\nLet us convert the mathematical into computationa model in Python. We start by importing the necessary libraries and setting up the plotting environment.\n\nimport numpy as np  \nimport sympy as sp\nimport matplotlib.pyplot as plt\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; plt.rcParams['grid.linewidth'] = 0.25; \n\nBut how to model the transition?\nTransition matrix. We know that a the rows of our transition matrix have to sum up to one, \\(\\sum_{s'} T(s, s') = 1\\). Thus, we can simplify the transition matrix by only giving a collpase probability \\(p_c\\) and a recovery probability \\(p_r\\),\n\\[\n\\left(\\begin{array}{cc}\n    T(\\mathsf{p,p}) & T(\\mathsf{p,d}) \\\\\n    T(\\mathsf{d,p}) & T(\\mathsf{d,d}) \\\\\n\\end{array}\\right)\n=\n\\left(\\begin{array}{cc}\n    1-p_c & p_c \\\\\n    p_r & 1-p_r \\\\\n\\end{array}\\right).\n\\]\nLets fix the values for the transition probabilities \\(p_c\\) and \\(p_r\\), for now,\n\npc = 0.05\npr = 0.01\n\nThen we can implement the transition matrix as a two-dimensional numpy array,\n\nT = np.array([[1-pc, pc],\n              [pr, 1-pr]])\nT\n\narray([[0.95, 0.05],\n       [0.01, 0.99]])",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>State transitions</span>"
    ]
  },
  {
    "objectID": "02.04-StateTransitions.html#simulations",
    "href": "02.04-StateTransitions.html#simulations",
    "title": "5  State transitions",
    "section": "5.3 Simulations",
    "text": "5.3 Simulations\n\nA single simulation run\nConsidering the system to be in exaclty one of its states at each time step, we can simulate the Markov chain stochastically by choosing the next state (using numpy.random.choice) according to the transition probabilites as given in the transition matrix (by specifing the probabilities as p=TransitionMatrix[current_state]).\n\ndef simulate_markov_chain(TransitionMatrix, InitialState, NrTimeSteps):\n    trajetory = -1*np.ones(NrTimeSteps, dtype=int)    \n    trajetory[0] = InitialState\n    for t in range(1, NrTimeSteps):\n        trajetory[t] = np.random.choice([0, 1], # sample next state \n                                        p=TransitionMatrix[trajetory[t-1]])\n    return trajetory\n\nVisualizing the results, we can see how the system evolves over time.\n\nnp.random.seed(1818)\ntrajectory = simulate_markov_chain(T, 0, 500)\nplt.plot(1-np.array(trajectory), ls='-', marker='.', color='purple');\nplt.xlabel('Time steps $t$'); plt.ylabel('System state $s$');\n\n\n\n\n\n\n\n\nThis stochastic simulation of a single run has a strong resemblance to the previous dynamic system models we introduced. We can interpret the Markovian states as the stable equilibirum points of dynamic system with nonlinear changes and noise (see Figure 5.4 from 02.03-Resilience).\n\n\n\n\n\n\nFigure 5.4: Noise induced transitions\n\n\n\nHowever, due to the stochastic nature of the Markov chain, it is hard to judge the system’s behavior from a single run. We need to average over many runs to get a clearer picture of the system’s behavior.\n\n\nEnsemble simulation\nLet’s repeat the previous simulation to create an ensemble of stochastic simulation runs. Let’s assume we want an ensemble of 100 runs.\n\nensemble = []\nfor _ in range(100):\n    state = 0\n    trajectory = simulate_markov_chain(T, state, 500)\n    ensemble.append(trajectory)\nensemble = np.array(ensemble)\n\nIt is always a good idea to investigate the object one has just created for consistency, for instance, checking the shape of the ensemble.\n\nensemble.shape\n\n(100, 500)\n\n\nThe first dimension of the ensemble is the number of runs, the second dimension is the number of time steps.\nVisualizing the ensemble by takeing the mean over the first dimension (using ensemble.mean(axis=0)),\n\nensemble.mean(axis=0).shape\n\n(500,)\n\n\nwe can see how the system evolves over time on average.\n\nplt.plot(1-ensemble.mean(0), ls='-', marker='.', color='blue')\nplt.ylim(0, 1); plt.xlabel('Time steps $t$'); plt.ylabel('Average system state $s$');\n\n\n\n\n\n\n\n\nWe observe two phases. First a drop. Second, some fluctuations around approx. 0.2.\nThus, after around 50-100 iterations, the system is in approximatly 20 of 100 runs in the the prosperous state.\nThis is the statistical equlibirum or the long-run stationary distribution of the Markov chain.\nCalculating ensembles is computationlly expensive. Can we make the computation more efficient? To do so, we investigate how to update the state distribution of the Markov chain at each time step.\n\n\nMarkov chain update\nA Markov chain update can be nicely represented by a matrix operation. The new state \\(\\mathbf p_{t+1}\\) equals the old state \\(\\mathbf p_t\\) applied to the transition matrix \\(\\mathbf T\\),\n\\[\n\\mathbf p_{t+1} = \\mathbf p_t \\mathbf T.\n\\]\nWe write the state distribution \\(\\mathbf p_t\\) before the transition matrix \\(\\mathbf T\\) on the right hand side to indicate the flow of information from left to right. We can also write this as follows, \\[\np_{t+1}({s'}) = \\sum_s p_t(s) T(s, s'),\n\\] where \\(s\\) denotes the current, and \\(s'\\) the next state. The next system state \\(s'\\) depends on the current state \\(s\\) and the transition from \\(s\\) to \\(s'\\) as given by the transition matrix \\(\\mathbf T\\).\nTransposing the transition matrix, we can rewrite the Markov chain update as \\(p_{t+1}(s') = \\sum_s T^\\mathrm{T}(s',s) p_t(s)\\) or\n\\[\n\\mathbf p_{t+1} = \\mathbf T^\\mathrm{T} \\mathbf p_t\n\\]\nwhere \\(\\mathbf T^\\mathrm{T}\\) is the transpose of the transition matrix. Rewriting this update as such highlights the fact that Markov chains can be intepreted as a special kind of dynamic system with linear changes (see 02.01-Nonlinearity) but with one additional property. The sum of all variables is always one.\n\n\nState distribution evolution\nWe use the matrix update to simulate how the state distribution evolves.\n\nps = [1, 0]  # initial state distribution\np_trajectory = [ps] # store the state distribution over time\n\nfor i in range(500): \n    ps = ps @ T  # matrix update for the state distribution\n    p_trajectory.append(ps)\np_trajectory = np.array(p_trajectory)\n\nThe trajectory of the state distribution has the number of time steps as the first dimension and the number of states as the second dimension.\n\np_trajectory.shape\n\n(501, 2)\n\n\nSince the state distribution is a probability distribution, the sum of all states at each time step should be one. We can check this by summing over the states at each time step.\n\nnp.allclose(p_trajectory.sum(axis=-1), 1.0)\n\nTrue\n\n\nVisualizing the state distribution evolution together with the ensemble average reveals a close resembles between the two.\n\nplt.plot(1-ensemble.mean(0), ls='-', marker='.', color='blue', label='Ensemble average')\nplt.plot(p_trajectory[:, 0], ls='-', marker='.', color='red', label='State distribution')\nplt.xlabel('Time steps $t$'); plt.ylabel('Average system state $s$'); plt.legend();\n\n\n\n\n\n\n\n\nThe fluctuations of the ensemble average around the long-run stationary distribution are due to the finite number of runs in the ensemble. The more runs we have, the closer the ensemble average will be to the long-run stationary distribution.\nThe flat line of the state distribution evolution indicates that the system has reached a statistical equilibirum.\nFor example, a statistical equilibrium in a Markov model of ideology would allow for people to transition between liberal, conservative, and independent, but the proportions of people of each ideology would remain unchanged. When applied to a single entity, a statistical equilibrium means that long-run probability of the entity being in each state does not change. A person could be in a statistical equilibrium in which he is happy 60% of the time and sad 40% of the time. The person’s mental state could change from hour to hour, but his long-run distribution across those states does not (Page, 2018).\nHowever, can we compute the stationary distribution somehow directly?",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>State transitions</span>"
    ]
  },
  {
    "objectID": "02.04-StateTransitions.html#stationary-distribution",
    "href": "02.04-StateTransitions.html#stationary-distribution",
    "title": "5  State transitions",
    "section": "5.4 Stationary distribution",
    "text": "5.4 Stationary distribution\nAfter showing how to compute the stationary distribution directly, we will first, compute it numerically, second, compute it symbolically, and last, we will discuss the conditions for its existence.\nFrom the update equation, \\[\n\\mathbf p_{t+1} = \\mathbf p_t \\mathbf T.\n\\] we know that the stationary distribution \\(\\mathbf p^*\\) must satisfy, \\[\n\\mathbf p^* = \\mathbf p^* \\mathbf T\n\\]\nThis looks like the defining equation of a (left) eigenvector with the eigenvalue 1, \\[\n1 \\mathbf  p^* = \\mathbf p^* \\mathbf T\n\\]\n\nNumerical stationary distribution\nFortunately, numpy has built-in routine to compute the eigenvectors of a matrix. Since the standard routine np.linalg.eig only computes the right eigenvectors, we need to apply the routine to the transposed matrix:\n\neigvv = np.linalg.eig(T.T)\neigvv\n\nEigResult(eigenvalues=array([0.94, 1.  ]), eigenvectors=array([[-0.70710678, -0.19611614],\n       [ 0.70710678, -0.98058068]]))\n\n\n\neigvv[1][:,1]\n\narray([-0.19611614, -0.98058068])\n\n\nNormalizing the eigenvector, such that it entries comprise a probability distribution, yields\n\npstar = eigvv[1][:,1] / sum(eigvv[1][:,1])\npstar\n\narray([0.16666667, 0.83333333])\n\n\nVisualizing the stationary state distribution together with the state distribution evolution and the ensemble average reveals that the calculated stationary distribution fits perfectly to the distribution evolution in (statistical) equilibirum.\n\nplt.plot(1-ensemble.mean(0), ls='-', marker='.', color='blue', label='Ensemble average')\nplt.plot(p_trajectory[:, 0], ls='-', marker='.', color='red', label='State distribution')\nplt.plot([0,500], [pstar[0], pstar[0]], '-', color='k', label='Stationary distribution')\nplt.xlabel('Time steps $t$'); plt.ylabel('Average system state $s$'); plt.legend();\n\n\n\n\n\n\n\n\nHow does this result depend on the model parameters, the collapse probability \\(p_c\\) and the recovery probability \\(p_r\\)?\nCan we compute the stationary distribution analytically, i.e., can we derive a mathematical equation which says how the stationary distribution depends on the collapse probability \\(p_c\\) and the recovery probability \\(p_r\\)?\n\n\nAnalytical stationary distribution\nFortunately, we can use Python’s library for basic symbolic calculations sympy, to compute the eigenvectors of the transition matrix symbolically.\n\np_c, p_r = sp.symbols(\"p_c, p_r\")\n\n\nT_ = sp.Matrix([[1-p_c, p_c],\n                [p_r, 1-p_r]])\nT_\n\n\\(\\displaystyle \\left[\\begin{matrix}1 - p_{c} & p_{c}\\\\p_{r} & 1 - p_{r}\\end{matrix}\\right]\\)\n\n\nApplying the .eigentvects() method\n\nT_.T.eigenvects()\n\n[(1,\n  1,\n  [Matrix([\n   [p_r/p_c],\n   [      1]])]),\n (-p_c - p_r + 1,\n  1,\n  [Matrix([\n   [-1],\n   [ 1]])])]\n\n\nshows us which eigenvector corresponds to the eigenvalue 1. Upon normalizing the eigenvector, we obtain the analytical stationary distribution,\n\npstar_ = T_.T.eigenvects()[0][2][0] # selecting the eigenvector\npstar_ = pstar_ / (pstar_[0] + pstar_[1]) # normalizing the eigenvector\nsp.simplify(pstar_)\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{p_{r}}{p_{c} + p_{r}}\\\\\\frac{p_{c}}{p_{c} + p_{r}}\\end{matrix}\\right]\\)\n\n\nThus, only the fraction of \\(p_c\\) and \\(p_r\\) determines the stationary distribution. The stationary distribution remains the same, if you multiply the collapse and recovery probabilities by the same factor. The fraction of the stationary distribution in the prosperous state is proportional to the recovery probability, \\(p_r\\), i.e., the probability to enter the prosperous state from the degraed state. And vice versa, the fraction of the stationary distribution in the degraded state is proportional to the collapse probability, \\(p_c\\), i.e., the probability to enter the degraded state from the prosperous state.\nLast, to compare this analyitical solution with the numerical calculation we insert the values for \\(p_c\\) and \\(p_r\\) into the analytical solution.\n\npstar_.subs(p_c, pc).subs(p_r, pr)\n\n\\(\\displaystyle \\left[\\begin{matrix}0.166666666666667\\\\0.833333333333333\\end{matrix}\\right]\\)\n\n\n\npstar\n\narray([0.16666667, 0.83333333])\n\n\nWe observed the the statistical equilibrium in the Markov chain simulation and calculated the long-run stationary distribution both numerically and symbolically. But what are the conditions that such a unique statistical equilibrium exists?\n\n\nStationary distribution | Existence\nAny Markov model with a finite set of states, fixed transition probabilities between them, the potential to move from any state to any other in a series of transitions, and no fixed cycles between states converges to a unique equilibrium. These are the conditions of the Perron-Frobenius Theorem (Page, 2018).\n\nPerron-Frobenius Theorem\nA Markov process converges to a unique statistical equilibrium provided it satisfies four conditions:\n\nFinite set of states: \\(\\mathcal S = {S_1, S_2, \\dots, S_Z}\\).\nFixed transition rule: The probabilities of moving between states are fixed, for example, the probability of transitioning from state \\(S_i\\) to state \\(S_j\\) equals \\(T(S_i, S_j)\\) in every period.\nErgodicity (state accessibility): The system can get from any state to any other through a series of transitions.\nNoncyclic: The system does not produce a deterministic cycle through a sequence of states.\n\n\nThe theorem implies that if those four assumptions are satisfied, the initial state, history, and interventions that change the state cannot change the long-run equilibrium.\nThe unique statistical equilibrium implies that long-run distributions of outcomes cannot depend on the initial state or on the path of events. In other words, initial conditions do not matter, and history does not matter in the long run. Nor can interventions that change the state matter. Any one-time change in the state of a system has at most a temporary effect.\nFor example,\n\nif nations move between dictatorships and democracies according to fixed probabilities, then interventions that impose or encourage democracies in some countries have no long-term effects.\nIf fluctuations in dominant political ideologies satisfy the assumptions, then history cannot influence the long-run distribution over ideologies.\nAnd if a person’s mental state can be represented as a Markov model, then words of encouragement or supportive gestures have no long-run impact.\n\nThe takeaway from the theorem should not be that history cannot matter but that if history does matter, one of the model’s assumptions must be violated.\nTwo assumptions—the finite number of states and no simple cycle—almost always hold.\nErgodicity can be violated. However, in practice, it is often possible to ensure ergodicity by adding a tiny transition probability between states that are not directly connected. This tiny transition probability can justified by our lack of knowledge about the system.\nThus, the assumption of fixed transition probabilities between states is the least likely to be valid. When history is important, something must alter the transition probabilities.\nFor example, take the issue of assisting families in escaping poverty. The forces that create social inequality have proven immune to policy interventions. In Markov models interventions that change families’ states—such as special programs for underperforming students or a one-day food drive—can provide temporary boosts. They cannot change the long-run equilibrium. In contrast, interventions that provide resources and training that improve people’s ability to keep jobs, and therefore change their probabilities of moving from employed to unemployed, could change long-run outcomes (Page, 2018).\nWe investigated the long-run behavior of the Markov chain model. But what about the short-term behavior?",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>State transitions</span>"
    ]
  },
  {
    "objectID": "02.04-StateTransitions.html#transient-behavior",
    "href": "02.04-StateTransitions.html#transient-behavior",
    "title": "5  State transitions",
    "section": "5.5 Transient behavior",
    "text": "5.5 Transient behavior\nWe found that our example system’s statistical equilibirum, its stationary distribution, depends only on the fraction of the collapse and recovery probabilities, \\(p_c\\) and \\(p_r\\).\nTo investigate the short-term behavior before the evolution of the state distribuion reaches its equlibirum, we create different pairs of collapse and recovery probabilities while keeping their fraction constant.\n\nprs = np.array([0.1, 0.03, 0.01, 0.003, 0.001])\npcs = 5 * prs\npcs\n\narray([0.5  , 0.15 , 0.05 , 0.015, 0.005])\n\n\nWe can insert these in the sympy matrix as follows,\n\nnp.array(T_.subs(p_c, pc).subs(p_r, pr), dtype=float)\n\narray([[0.95, 0.05],\n       [0.01, 0.99]])\n\n\n\nSimulating different transition probabilities\nIt is convenient to define a function to obtain the time evolution for the state distributions.\n\ndef compute_distribution_trajectory(T):  # Transition matrix \n    ps = [1, 0]\n    p_trajectory = []\n\n    for i in range(500): \n        ps = ps @ T\n        p_trajectory.append(ps)\n    return np.array(p_trajectory)\n\nWith that function, we simply compute the time evolution of the state distribution for the different values of \\(p_r\\) and \\(p_c\\).\n\ntrajs = []\nfor pr, pc in zip(prs, pcs):\n    Tmat = np.array(T_.subs(p_c, pc).subs(p_r, pr), dtype=float)\n    trajs.append(compute_distribution_trajectory(Tmat))\nnp.array(trajs).shape\n\n(5, 500, 2)\n\n\n\n\nVisualizing distribution trajectories\n\nfor i, traj in enumerate(trajs):\n    plt.plot(traj[:, 0], ls='-', marker='.', label=prs[i])\nplt.plot([0,500], [pstar[0], pstar[0]], '-', color='black')\nplt.ylim(0, 1); plt.xlabel('Time steps'); plt.legend(title='Recovery probability');\n\n\n\n\n\n\n\n\nThe smaller the transition probabilities, the longer it takes to reach the stationary distribution.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>State transitions</span>"
    ]
  },
  {
    "objectID": "02.04-StateTransitions.html#timescales",
    "href": "02.04-StateTransitions.html#timescales",
    "title": "5  State transitions",
    "section": "5.6 Timescales",
    "text": "5.6 Timescales\nSometimes, it can be interesting or adquate to think about the typical timescales of a system. For example, consider the typical timescale it takes a system to tip into another state. How can we identify the notion of timescale in a Markov chain?\nLet us identify a notion of timescale as the average number of time steps spend in a particular state before a transition occurs. How can we calculate it?\nWe will first compute these timescales numerically, showcasing somewhat more advanced maniumplation using Python. Then we turn to an analytical formula and compare the results.\n\nNumerical computation\nTo investigate this question nummerically, we re-create a (long) trajectory of states.\n\nstate = 0\npc = 0.2\npr = 0.04\nT = np.array(T_.subs(p_c, pc).subs(p_r, pr), dtype=float)\ntrajectory = []\nfor i in range(500000):\n    state = np.random.choice([0,1], p=T[state])\n    trajectory.append(state)\ntrajectory = np.array(trajectory)\n\nLooking at the first 100 states\n\nshorttraj = trajectory[:99]\nshorttraj\n\narray([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n\n\nHow do we obtain the lengths of the 0 and 1 sequences?\nWe can subtract the trajectory by itsel with an offset of one time step. For the first 100 time steps, this looks like\n\nshorttraj[0:-1] - shorttraj[1:]\n\narray([-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, -1,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0])\n\n\nWith np.nonzero with obtain the ‘cutpoints’ where the states change,\n\ncutpoint = np.nonzero(trajectory[0:-1] - trajectory[1:])\ncutpoint\n\n(array([     0,     19,     26, ..., 499904, 499960, 499985]),)\n\n\nWith np.diff, we obtain the differences between the cutpoints. These are the lengths of the sequences.\n\nlengths = np.diff(cutpoint)\nlengths \n\narray([[19,  7, 22, ..., 10, 56, 25]])\n\n\nThe lengths of the prosperous states are in the odd elements of the lengths iterable. Taking the average yields\n\nlength_prosperous = np.mean(lengths[0][1::2])\nlength_prosperous\n\n4.982404866992724\n\n\nThe lengths of the degraded states are in the even elements of the lengths iterable. Taking the average yields\n\nlength_degraded = np.mean(lengths[0][0::2])\nlength_degraded\n\n24.838959799594416\n\n\n\n\nAnalytical computation\nThe average number of time steps \\(T\\) it takes until a transition occurs, given a transition probability \\(p\\), is \\[\nT = \\sum_{n=0}^\\infty n (1-p)^{n-1} p = \\frac{1}{p}.\n\\]\nFor a sequence of length \\(n\\) it took \\(n-1\\) time steps to remain in state \\(s\\) before the transition. The probability of remaining in state \\(s\\) for \\(n-1\\) time steps is \\((1-p)^{n-1}\\). At the \\(n\\)’th time step, the transition occurs with probability \\(p\\).\nThis is the expected value of a geometric random variable with parameter \\(p\\). The number of steps \\(T\\) spent in state \\(s\\) before a transition occurs can be thought of as the number of trials until the first success in a sequence of Bernoulli trials, where each trial has a success probability of \\(p\\) of transitioning out of \\(s\\). In probability theory, the number of trials required to get the first success in such a situation is described by a geometric random variable. It expected value is \\(\\mathbb{E}[T] = 1/p\\).\nFor the prosperous state, we have\n\n1/pc\n\n5.0\n\n\ncompared to our numerical estimate\n\nlength_prosperous\n\n4.982404866992724\n\n\nFor the degraded state, we have\n\n1/pr\n\n25.0\n\n\ncompared to our numerical estimate\n\nlength_degraded\n\n24.838959799594416\n\n\nThus, the average number of time steps before a transition occurs equals the inverse transition probability \\(1/p\\). This gives us an indication of the typical timescale of the system.\n\n\nExample | Regime shifts timescales\nTypical collapse and recovery timescales of regime shifts from the Regime Shifts Database have been mapped to the transition probabilities of a Markov chain in this way (Barfuss et al., 2024) (Figure 5.5).\n\n\n\n\n\n\nFigure 5.5: Timescales",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>State transitions</span>"
    ]
  },
  {
    "objectID": "02.04-StateTransitions.html#learning-goals-revisited",
    "href": "02.04-StateTransitions.html#learning-goals-revisited",
    "title": "5  State transitions",
    "section": "5.7 Learning goals revisited",
    "text": "5.7 Learning goals revisited\n\nWe introduced the components of a Markov chain model and covered how the model relates to gernal dynamic system models to place this model in the context of integrated nature-society models.\nWe simulated and visualized Markov chain models in multiple ways: stochastically, with ensembles, and via its state distribution to understand how the model behaves.\nWe computed the stationary distribution of a Markov chain model numerically, analytically and explain the conditions for its existence to understand the long-term behavior of the model.\nWe investigated the transient behavior of a Markov chain model to understand the short-term behavior of the model.\nWe computed the typical timesacle of a Marko chain transition to relate the model to real-world systems.\n\n\n\n\n\nBarfuss, W., Donges, J., & Bethge, M. (2024). Ecologically-mediated collective action in commons with tipping elements. OSF. https://doi.org/10.31219/osf.io/7pcnm\n\n\nBiggs, R., Preiser, R., de Vos, A., Schlüter, M., Maciejewski, K., & Clements, H. (2021). The Routledge Handbook of Research Methods for Social-Ecological Systems (1st ed.). Routledge. https://doi.org/10.4324/9781003021339\n\n\nDaniel, C. J., Frid, L., Sleeter, B. M., & Fortin, M.-J. (2016). State-and-transition simulation models: A framework for forecasting landscape change. Methods in Ecology and Evolution, 7(11), 1413–1423. https://doi.org/10.1111/2041-210X.12597\n\n\nMüller-Hansen, F., Cardoso, M. F., Dalla-Nora, E. L., Donges, J. F., Heitzig, J., Kurths, J., & Thonicke, K. (2017). A matrix clustering method to explore patterns of land-cover transitions in satellite-derived maps of the Brazilian Amazon. Nonlinear Processes in Geophysics, 24(1), 113–123. https://doi.org/10.5194/npg-24-113-2017\n\n\nPage, S. E. (2018). The model thinker: What you need to know to make data work for you. Basic Books.",
    "crumbs": [
      "Dynamic Systems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>State transitions</span>"
    ]
  },
  {
    "objectID": "03-TargetEquilibria.html",
    "href": "03-TargetEquilibria.html",
    "title": "Target Equilibria",
    "section": "",
    "text": "In this part, we cover target-equilibria models or equilibrium-based models. They operationalize target knowledge, which is knowledge about the desired future and the values that indicate which direction to take. It relies on deliberation by different societal actors and is based on values and norms. In sustainability transitions, ways of producing target knowledge include participatory vision, scenario development with a wide range of stakeholders, and the public discourse at large. Target knowledge is strongly associated with values and asks what ought to be?.\n\n\n\nThree types of models based on three types of knowledge for transdisciplinary reserach\n\n\nTarget-equilibrium (or equilibrium-based models applied to sustainability transitions) are primarily used in economics. The overarching idea of the model type is to find a target equilibrium, a state of the system that is considered desirable. In contrast to the dynamic systems models, target equilibrium models introduce at least one decision-maker into the model. Given our assumptions about how the world works and that we can precisely specify what we want, we can use optimization techniques to find the best possible course of action. Having found the best course of action, we are in a ‘target equilibrium.’\nThe decision-maker is sometimes called an agent or actor. It can be a single individual or a group of individuals, such as a household, a company, a government, or a non-governmental organization. It can be a human, an animal, or a machine. It may even be conceivable that a single human consists of multiple agents. Thus, when introducing the concept of an agent, we obtain an abstract but flexible modeling tool to represent the agency and decision-making of a wide range of possible entities.\nSpecifically, we will cover\n\nSequential decisions of a single agent in a dynamic environment in Chapter 03.01\nStrategic interactions of multiple agents in a static environment in Chapter 03.02, and\nDynamic interactions of multiple agents in a dynamic environment in Chapter 03.03.",
    "crumbs": [
      "Target Equilibria"
    ]
  },
  {
    "objectID": "03.01-SequentialDecisions.html",
    "href": "03.01-SequentialDecisions.html",
    "title": "6  Sequential Decisions",
    "section": "",
    "text": "6.1 Motivation | Sequential decision-making under uncertainty\nWe will introduce the model framework of Markov Decision Processes (MDPs) to model sequential decision-making under uncertainty. MDPs are a powerful tool to model decision-making processes in various applications, such as robotics, finance, and environmental management.\nMDPs highlight the trade-off between current and future wellbeing in the presence of uncertainty.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sequential Decisions</span>"
    ]
  },
  {
    "objectID": "03.01-SequentialDecisions.html#motivation-sequential-decision-making-under-uncertainty",
    "href": "03.01-SequentialDecisions.html#motivation-sequential-decision-making-under-uncertainty",
    "title": "6  Sequential Decisions",
    "section": "",
    "text": "Markov Decision Processes (MDPs) are models for sequential decision-making when outcomes are uncertain.\nThey extend Markov Chains by a single agent, executing an action at each time step, trying to optimize its long-term wellbeing.\n\n\nApplications in human-environment interactions\nMDPs are widely used in environmental management and conservation biology to model the trade-off between current and future wellbeing in the presence of uncertainty (Marescot et al., 2013; Williams, 2009). Application areas cover the whole spectrum of natural resource ecology, management, and conservation, including\n\nforestry and forest management\nfisheries and aquatic management\nwildlife and range management\nweeds, pest, and disease control\n\nIn ecology, the term stochastic dynamic programming (SDP) is often used to refer to both the mathematical model (MDP) and its solution techniques (SDP per see).\n\n\nAdvantages of Markov decision processes\nUsing MDPs to model human-environment interactions has several advantages:\n\ninherently stochastic - to account for uncertainty\nnonlinear - to account for structural changes\nagency - to account for human behavior\nfuture-looking - to account for the trade-off between short-term and long-term\nfeedback - between one agent and the environment\n\nIn addition to these structural advantages, MDPs can also be solved in a computationally efficient way using a variety of algorithms, such as dynamic programming and reinforcement learning. This makes them also a scalable modeling framework. However, as our focus lies on transparent analysis and interpretation, we will focus on minimalistic models and won’t cover the computational aspects. But in principle, MDPs can be used to model high-dimensional systems with many states and actions.\n\n\nLearning goals\nAfter this lecture, students will be able to:\n\nDescribe the elements of a Markov Decision Process (MDP) and how they relate to applications in human-environment interactions\nSimulate and visualize the time-evolution of an MDP\nExplain what value functions are, why they are useful, and how to relate to the agent’s goal and Bellman equation.\nCompute value functions and visualize the best policy in simple MDPs",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sequential Decisions</span>"
    ]
  },
  {
    "objectID": "03.01-SequentialDecisions.html#markov-decision-processes-mdps",
    "href": "03.01-SequentialDecisions.html#markov-decision-processes-mdps",
    "title": "6  Sequential Decisions",
    "section": "6.2 Markov Decision Processes (MDPs)",
    "text": "6.2 Markov Decision Processes (MDPs)\n\nimport numpy as np  \nimport sympy as sp\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, interactive, fixed\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; plt.rcParams['grid.linewidth'] = 0.25; \n\nGraphically, a Markov decision process can be represented by the agent-environment interface in Figure 6.1.\n\n\n\n\n\n\nFigure 6.1: Agent-Environment Interface\n\n\n\nFormally, we define a Markov Decision process by the following elements:\n\nA discrete-time variable \\(t=0,1,2,\\dots\\)\nA discrete set of contexts or states \\(\\mathcal S = \\{S_1, \\dots, S_Z\\}\\).\nA discrete set of options or actions \\(\\mathcal A = \\{A_1, \\dots, A_M\\}\\).\nA transition function \\(T: \\mathcal S \\times \\mathcal A \\times \\mathcal S \\rightarrow [0,1]\\).\n\n\\(T(s, a, s')\\) is the transition probability from current state \\(s\\) to the next state \\(s'\\) under action \\(a\\).\n\nA welfare or reward function \\(R: \\mathcal S \\times \\mathcal A \\times \\mathcal S \\rightarrow \\mathbb R\\).\n\n\\(R(s, a, s')\\) is the current/immediate/short-term reward the agent receives when executing action \\(a\\) in state \\(s\\) and transitioning to state \\(s'\\).\n\nThe agent’s goal or gain function \\(G\\), including a discount factor \\(\\gamma \\in [0, 1)\\), denoting how much the agent cares for future rewards\nThe agent’s policy or strategy \\(x: \\mathcal S \\times \\mathcal A \\rightarrow [0,1]\\).\n\n\nExample model overview\nWe will illustrate the concept of MDPs using a simple example (Barfuss et al., 2018), modeling the trade-off between short-term gains and environmental collapse with long-term consequences for the decision-maker’s wellbeing.\n\n\n\nRisk-reward dilemma\n\n\n\n\nStates and actions\nThe environment consists of two states, \\(\\mathcal S = \\{\\textsf{p}, \\textsf{d}\\}\\), representing a prosperous and a degraded state of the environment.\n\nstate_set = ['prosperous', 'degraded']; p=0; d=1\n\nWe also defined two Python variable p=0 and d=1 to serves as readable and memorable indices to represent the environmental contexts.\nThe agent can choose between two actions, \\(\\mathcal A = \\{\\textsf{f}, \\textsf{r}\\}\\), representing a safe and a risky decision.\n\naction_set = ['safe', 'risky']; f=0; r=1\n\nLikewise, we define two Python variables, f=0 and r=1, to serve as readable and memorable indices to represent the agent’s actions. We represent the safe action with the f instead of the s to avoid confusion with the state s.\n\n\nTransitions | Environmental dynamics\nThe environmental dynamics, i.e., the transitions between environmental state contexts are modeled by two parameters, a collapse probability, \\(p_c\\), and a recovery probability, \\(p_r\\).\nLet’s assume the following default values,\n\npc = 0.05\npr = 0.025\n\nWe implement the transitions as a three-dimensional array or tensors, with dimensions \\(Z \\times M \\times Z\\), where \\(Z\\) is the number of states and \\(M\\) is the number of actions.\n\nT = np.zeros((2,2,2))\n\nThe cautious action guarantees to remain in the prosperous state, \\(T(\\mathsf{p,f,p})=1\\). Thus, the agent can avoid the risk of environmental collapse by choosing the cautious action, \\(T(\\mathsf{p,f,d})=0\\).\n\nT[p,f,d] = 0\nT[p,f,p] = 1   \n\nThe risky action risks the collapse to the degraded state, \\(T(\\mathsf{p,r,d}) = p_c\\), with a collapse probability \\(p_c\\). Thus, with probability \\(1-p_c\\), the environment remains prosperous under the risky action, \\(T(\\mathsf{p,r,p}) = 1-p_c\\).\n\nT[p,r,d] = pc\nT[p,r,p] = 1-pc\n\nAt the degraded state, recovery is only possible through the cautious action, \\(T(\\mathsf{d,f,p})=p_r\\), with recovery probability \\(p_r\\). Thus, with probability \\(1-p_r\\), the environment remains degraded under the cautious action, \\(T(\\mathsf{d,f,d})=1-p_r\\).\n\nT[d,f,p] = pr\nT[d,f,d] = 1-pr\n\nFinally, the risky action at the degraded state guarantees a lock-in in the degraded state, \\(T(\\mathsf{d,r,d})=1\\). Thus, the environment cannot recover from the degraded state under the risky action, \\(T(\\mathsf{d,r,p})=0\\).\n\nT[d,r,p] = 0\nT[d,r,d] = 1\n\nLast, we make sure that our transition tensor is normalized, i.e., the sum of all transition probabilities from a state-action pair to all possible next states equals one, \\(\\sum_{s'} T(s, a, s') = 1\\).\n\nassert np.allclose(T.sum(-1), 1.0)\n\nAll together, the transition tensor looks as follows:\n\nT\n\narray([[[1.   , 0.   ],\n        [0.95 , 0.05 ]],\n\n       [[0.025, 0.975],\n        [0.   , 1.   ]]])\n\n\n\n\nRewards | Short-term welfare\nThe rewards or welfare the agent receives represent the ecosystem services the environment provides. It is modeled by three parameters: a safe reward \\(r_s\\), a risky reward \\(r_r&gt;r_s\\), and a degraded reward \\(r_d&lt;r_s\\). We assume the following default values,\n\nrs = 0.8\nrr = 1.0\nrd = 0.0\n\nAs the transition, we implement the rewards as a three-dimensional array or tensor, with dimensions \\(Z \\times M \\times Z\\), where \\(Z\\) is the number of states and \\(M\\) is the number of actions.\n\nR = np.zeros((2,2,2))\n\nThe cautious action at the prosperous state guarantees the safe reward, \\(R(\\mathsf{p,f,p}) = r_s\\),\n\nR[p,f,p] = rs\n\nThe risky action at the prosperous leads to the risky reward if the environment does not collapse, \\(R(\\mathsf{p,r,p}) = r_r\\),\n\nR[p,r,p] = rr\n\nYet, whenever the environment enters, remains, or leaves the degraded state, it provides only the degraded reward \\(R(\\mathsf{d,:,:}) = R(\\mathsf{:,:,d}) = r_d\\), where \\(:\\) denotes all possible states and actions.\n\nR[d,:,:] = R[:,:,d] = rd\n\nTogether, the reward tensor looks as follows:\n\nR\n\narray([[[0.8, 0. ],\n        [1. , 0. ]],\n\n       [[0. , 0. ],\n        [0. , 0. ]]])\n\n\n\n\nPolicy\nFor now, we have not contemplated how the agent should behave. Therefore, to understand how transitions, rewards, and policies are related, let us simulate the MDP using a random policy.\nWe will implement a policy as a two-dimensional array or tensor, with dimensions \\(Z \\times M\\), where \\(Z\\) is the number of states and \\(M\\) is the number of actions.\n\nX = np.random.rand(2,2)  # random values between 0 and 1\n\nA policy has to be a probability distribution over actions for each state, \\(\\sum_a X(s, a) = 1\\). To ensure this, we normalize the policy tensor,\n\nX = X / X.sum(axis=-1, keepdims=True) \n\nand test, if the policy is a valid probability distribution by asserting that the sum of all probabilities over actions is equal to one,\n\nassert np.allclose(X.sum(-1), 1.0)\n\nTogether, the policy tensor looks as follows:\n\nX\n\narray([[0.32878922, 0.67121078],\n       [0.9813571 , 0.0186429 ]])\n\n\nWe convert this logic into a Python function, that returns a random policy for a given number of states and actions,\n\ndef random_policy(Z=2, M=2):\n    X = np.random.rand(Z,M)  # random values\n    X = X/X.sum(axis=-1, keepdims=True) # normalize values, such that\n    assert np.allclose(X.sum(-1), 1.0) # X is a proper probability distribution\n    return X\n\nFor example, a random policy for our example MDP with two states and two actions looks as follows:\n\nnp.random.seed(42)\nrandom_policy(M=2, Z=2)\n\narray([[0.28261752, 0.71738248],\n       [0.55010153, 0.44989847]])\n\n\nThis completes all definitions required for an MDP. We can now simulate the MDP by iterating over time steps and applying the policy to the current state.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sequential Decisions</span>"
    ]
  },
  {
    "objectID": "03.01-SequentialDecisions.html#simulation",
    "href": "03.01-SequentialDecisions.html#simulation",
    "title": "6  Sequential Decisions",
    "section": "6.3 Simulation",
    "text": "6.3 Simulation\n\nStochastic simulation\nAs in the case of Markov chains, we can simulate the MDP by drawing random numbers. We draw the actions according to the policy and the next state according to the transition probabilities. The reward is then a result of the current state, the current action, and the next state. We implement this as a Python function that takes the transition tensor, the reward tensor, the policy tensor, the initial state, and the number of time steps as input arguments.\n\ndef simulate_markov_decision_process(TransitionTensor, RewardTensor, Policy, InitialState, NrTimeSteps):   \n    state_trajectory = []\n    reward_trajectory = []\n    state = InitialState\n    \n    for t in range(0, NrTimeSteps):\n        # Choose random action according to policy:\n        action = np.random.choice([f, r], p=Policy[state]) \n        # Transition to new state:\n        state_ = np.random.choice([p, d], p=TransitionTensor[state][action]) \n        # Record reward:\n        reward = RewardTensor[state, action, state_] \n        # Update state:\n        state = state_  \n        # Store in trajectories\n        state_trajectory.append(state);\n        reward_trajectory.append(reward) \n\n    return np.array(state_trajectory), np.array(reward_trajectory)\n\nWe execute the simulation and visualize the time-evolution of the MDP’s environmental state and agent’s rewards.\n\nnp.random.seed(1818)\nstate_trajectory, reward_trajectory = simulate_markov_decision_process(T, R, X, 0, 500)\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(1-np.array(state_trajectory), ls='-', marker='.', color='Darkblue')\naxes[1].plot(reward_trajectory, color='Red'); axes[0].set_ylabel('Environment'); axes[1].set_ylabel('Rewards'); axes[1].set_xlabel('Time steps'); plt.tight_layout();\n\n\n\n\n\n\n\n\nWe observe the same stochastic nature of the simulation as with Markov chains. Furthermore, the agent’s rewards fluctuate over time, depending on the environmental state and the agent’s actions. The agent’s rewards are higher in the prosperous state and lower in the degraded state.\nCan we make sense of the stochasticity by computing averages over many simulations?\n\n\nEnsemble simulation\nLet’s repeat the previous simulation to create an ensemble of stochastic simulation runs. Let’s assume we want an ensemble of 250 runs.\n\nstate_ensemble = []\nreward_ensemble = []\nfor _ in range(250):\n    state = 0\n    state_trajectory, reward_trajectory =\\\n        simulate_markov_decision_process(T, R, X, 0, 500)\n    state_ensemble.append(state_trajectory)\n    reward_ensemble.append(reward_trajectory)\nstate_ensemble = np.array(state_ensemble)\nreward_ensemble = np.array(reward_ensemble)\n\nIt is always a good idea to investigate the object one has just created for consistency, for instance, checking the shape of the ensemble.\n\nprint(state_ensemble.shape, reward_ensemble.shape)\n\n(250, 500) (250, 500)\n\n\nFor each ensembel, the first dimension of the ensemble is the number of runs, the second dimension is the number of time steps.\nVisualizing the ensemble by takeing the mean over the first dimension (using ensemble.mean(axis=0)),\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(1-state_ensemble.mean(0), ls='-', marker='.', color='Darkblue')\naxes[1].plot(reward_ensemble.mean(0), color='Red');\naxes[0].set_ylabel('Environment'); axes[1].set_ylabel('Rewards'); \naxes[0].set_ylim(-0.1,1.1); axes[1].set_ylim(-0.1,1.1); axes[1].set_xlabel('Time steps');\nplt.tight_layout();\n\n\n\n\n\n\n\nFigure 6.2\n\n\n\n\n\nFigure 6.2 shows the ensemble average of the environmental state and the agent’s rewards over time. The ensemble average is smoother than the individual runs, indicating that the stochasticity averages out over many runs. This observation suggests that we can work with the MDP in the same way as a Markov chain, simulating the time evolution of the state distribution directly.\n\n\nDistribution trajectory\nWe realize that the MDP’s transition tensor can be reduced to a Markov Chain’s transition matrix when we fix the agent’s policy:\n\\[\nT_\\mathbf{x}(s, s') := \\sum_{a \\in \\mathcal A} x(s, a) T(s, a, s')\n\\]\nIn Python, we use the einsum function for that, since it gives us full control over which indices we want to execute the summation:\n\ns, a, s_ = 0, 1, 2\nTss = np.einsum(X, [s,a],     # first object with indices\n                T, [s,a,s_],  # second object with indices\n                [s,s_])       # indices of the output\nTss\n\narray([[0.96643946, 0.03356054],\n       [0.02453393, 0.97546607]])\n\n\nWith the effective Markov chain transition matrix, we use the matrix update derived in 02.04-StateTransitions to simulate how the state distribution evolves.\n\nps = [1, 0]\np_trajectory = []\nfor i in range(500):\n    ps = ps @ Tss\n    p_trajectory.append(ps)\np_trajectory = np.array(p_trajectory)\n\nThe trajectory of the state distribution has the number of time steps as the first dimension and the number of states as the second dimension.\n\np_trajectory.shape\n\n(500, 2)\n\n\nVisualizing the state distribution evolution together with the ensemble average reveals a close resembles between the two.\n\nplt.plot(1-state_ensemble.mean(0), ls='-', marker='.', color='Darkblue',  label='Ensemble average')\nplt.plot(p_trajectory[:, 0], ls='-', marker='.', color='blue', label='State distribution')\nplt.xlabel('Time steps $t$'); plt.ylabel('Average system state $s$'); plt.legend();\n\n\n\n\n\n\n\n\nTo compute the average reward trajectory over time, we use the same logic as for the state distribution trajectory, \\(p_t(s)\\). We compute the reward distribution by summing over the state dimension, weighted by the state distribution,\n\\[\n\\langle R_t \\rangle_\\mathbf{x}= \\mathbb E_\\mathbf{x}[r_t] = \\sum_{s \\in \\mathcal S} \\sum_{a \\in \\mathcal A} \\sum_{s' \\in \\mathcal{S}}  p_t(s) x(s,a)T(s,a,s')R(s,a,s'),\n\\]\nwhere \\(\\mathbb E_\\mathbf{x}[\\cdot]\\) denotes the expected value of a random variable \\(\\cdot\\) given the agent follows policy \\(\\mathbf x\\).\nYou see how, in this equation on the right hand side, the information flows from the left to the right. The state distribution \\(p_t(s)\\) is multiplied with the policy \\(x(s,a)\\) to get the probability of taking action \\(a\\). This probability is then multiplied with the transition probability \\(T(s,a,s')\\) to get the probability of transitioning to state \\(s'\\). Finally, this probability is multiplied with the reward \\(R(s,a,s')\\) to get the expected reward.\nWe use the einsum function to convert this logic into Python,\n\ns, a, s_, t = 0, 1, 2, 3\nr = np.einsum(p_trajectory, [t, s],\n              X, [s,a],     \n              R, [s,a,s_],\n              T, [s,a,s_],  \n              [t])  # output only in time dimension\n\nWe check that the average-reward trajectory is only a one-dimensional array, with the number of timesteps as the first dimension.\n\nr.shape\n\n(500,)\n\n\nVisualizing the average-reward distribution evolution together with the ensemble average reveals a close resemblance between the two.\n\nplt.plot(reward_ensemble.mean(0), color='Red', label='Ensemble average');\nplt.plot(r, ls='-', marker='.', color='pink', label='Average rewards');\nplt.xlabel('Timesteps $t$'); plt.ylabel('Rewards $R$'); plt.legend();\n\n\n\n\n\n\n\n\nThus, we can also calculate the stationary distribution of an MDP given a policy \\(\\mathbf x\\) in the same way as for a Markov chain.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sequential Decisions</span>"
    ]
  },
  {
    "objectID": "03.01-SequentialDecisions.html#goals-and-values",
    "href": "03.01-SequentialDecisions.html#goals-and-values",
    "title": "6  Sequential Decisions",
    "section": "6.4 Goals and values",
    "text": "6.4 Goals and values\nIn the Markov Decision Process framework, the agent’s purpose or goal is formalized within the reward signal, flowing from the environment to the agent (Sutton & Barto, 2018). At each time step, the agent the reward is represented by a single number \\(R_t \\in \\mathbb R\\). Informally, the agent’s goal is to maximize the total amount of reward it receives over time. This may entail choosing actions that yield less immediate rewards to get more rewards in the future.\nRepresenting the agent’s goal by a series of single numbers might seem limiting. However, in practice, it has proven itself flexible and widely applicable. It also aligns well with the unidimensional concepts of utility in economics (Schultz et al., 2017) and fitness in biological or cultural evolution.\n\n\n\nGoal functions\nHow do we translate our informal definition of the agent’s goal as maximizing the total amount of reward into a formal mathematical equation?\nFinite-horizon goal. The simplest case for a goal function \\(G_t\\) is to sum up all rewards the agent receives from timestep \\(t\\) onwards until the final time step \\(T\\), \\[\nG_t := R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_T = \\sum_{\\tau=t+1}^T R_\\tau.\n\\]\nThis definition makes sense only if we have a clearly defined final state, such as the end of a board game, the completion of an individual project, or the end of an individual’s life. However, in human-environment interaction in the context of sustainability transitions, we are interested in the long-term future without a clear final state. In these cases, we cannot use the goal definition from above as with \\(T=\\infty\\), the sum \\(G_t\\) itself could easily be infinite for multiple reward sequences, which would leave the agent without guidance on which reward sequence yields a higher \\(G_t\\) and, hence, what to do.\nFor example, on average, our ensemble of stochastic simulations yields a total finite-horizon gain of\n\nreward_ensemble.sum(axis=1).mean()\n\n197.87679999999997\n\n\nDiscounted goal. We solve this problem of diverging gains \\(G_t\\) with the concept of temporal discounting. We revise our informal definition of the agent’s goal: The agent tries to select actions to maximize the sum of discounted future rewards (Sutton & Barto, 2018). The goal function then becomes,\n\\[\nG_t := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{\\tau=t}^\\infty \\gamma^\\tau R_{t+\\tau+1},\n\\]\nwhere \\(\\gamma \\in [0, 1)\\) is the discount factor. The discount factor determines how much the agent cares about future rewards. A discount factor of \\(\\gamma=0\\) means that the agent only cares about the immediate reward, while as the discount factor approaches \\(\\gamma \\rightarrow 1\\), the agent takes future rewards into account more strongly and becomes more farsighted.\nFor example, the discounted gain with a discount factor of \\(\\gamma=0.9\\) of the last ensemble run is\n\nnp.sum([0.9**t * reward_ensemble[-1, t] for t in range(500)])\n\n9.254059133343878\n\n\nTemporal discounting is a widely used concept in (environmental) economics, psychology, and neuroscience to model human decision-making. It implies that welfare experienced in the future is worth less to the agent than the same amount of welfare experienced now. This concept is used both as a normative and descriptive model of decision-making.\nOne reason for temporal discounting is the uncertainty about the future. The future is uncertain, and the agent might not be around to experience future rewards. In fact, \\(\\gamma\\) can be interpreted as the probability that the agent will be around to experience future rewards.\nThe primary value of temporal discounting and the discount factor for us in our quest to develop integrated system models of human-environment interactions is its ability to model the trade-off between present and future welfare. This trade-off is at the heart of many sustainability transitions, such as the trade-off between short-term economic gains and long-term environmental degradation.\nFor example, let’s assume the agent receives a constant reward stream of \\(R_t=1\\) for all timesteps \\(t\\). We compare the so-called (net) present value at timestep \\(t\\) for different discount factors \\(\\gamma\\). We also compute the sum of the discounted rewards for the infinite future, \\(G_t\\).\n\nfor discountfactor in [0.1, 0.5, 0.9, 0.99]:\n    summands = [discountfactor**t for t in range(10000)]\n    plt.plot(summands, label=discountfactor)\n\n    total_value = np.sum(summands)\n    print(\"Discount factor {dcf:3.2f}: Total {total:5.1f}\"\\\n        .format(dcf=discountfactor, total=total_value))\n\nplt.legend(); plt.ylabel('Present value'); plt.xlabel('Timestep'); \nplt.xlim(0,100);\n\nDiscount factor 0.10: Total   1.1\nDiscount factor 0.50: Total   2.0\nDiscount factor 0.90: Total  10.0\nDiscount factor 0.99: Total 100.0\n\n\n\n\n\n\n\n\nFigure 6.3\n\n\n\n\n\nHere, we used the Python string method format to print the results in a readable way. It can be used to insert variables into a string. The curly brackets {} are placeholders for the variables, and the variables are passed to the format method as arguments. The colon : inside the curly brackets is used to format the output. For example, :3.2f formats the number as a floating-point number with three digits before and two digits after the decimal point.\nNormalized goal. To account for the fact that the total value depends on the level of discounting, even if the reward stream is constant, we can normalize the goal as follows,\n\\[ G_t = (1-\\gamma) \\sum_{\\tau=t}^\\infty \\gamma^\\tau R_{t+\\tau+1},\\]\nwhere \\(1-\\gamma\\) is a normalizing factor and \\(R_{t+\\tau+1}\\) is the reward received at time step \\(t+\\tau+1\\).\n\nfor dcf in [0.1, 0.5, 0.9, 0.99]:\n    summands = [dcf**t for t in range(10000)]\n    normalizing = 1-dcf\n    total_value = normalizing * np.sum(summands)\n    print(\"Discount factor {dcf:3.2f}: Total {total:5.1f}\".format(dcf=dcf, total=total_value))\n\nDiscount factor 0.10: Total   1.0\nDiscount factor 0.50: Total   1.0\nDiscount factor 0.90: Total   1.0\nDiscount factor 0.99: Total   1.0\n\n\nWith normalization, the discount factor parameter \\(\\gamma\\) expresses how much the agent cares for the future without influencing the scale of the total value. That way, the outcomes of different discount factors can be compared with each other.\nFor example, the normalized discounted gain with a discount factor of \\(\\gamma=0.9\\) of the last ensemble run is\n\ndcf = 0.9\n(1-dcf) * np.sum([dcf**t * reward_ensemble[-1, t] for t in range(500)])\n\n0.9254059133343876\n\n\nBellman equation. Regardless of the goal formulation, the agent’s gains \\(G_t\\) at successive time steps relate to each other in an important way:\n\\[\\begin{align}\nG_t &= (1-\\gamma) \\sum_{\\tau=t}^\\infty \\gamma^\\tau R_{t+\\tau+1}\\\\\n   &= (1-\\gamma) \\left(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} \\cdots \\right)\\\\\n   &= (1-\\gamma) \\left(R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} \\cdots) \\right)\\\\\n   &= (1-\\gamma) R_{t+1} + \\gamma (1-\\gamma) (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} \\cdots) \\\\\n   &= (1-\\gamma) R_{t+1} + \\gamma G_{t+1},\n\\end{align}\\]\nThe gain \\(G_t\\) is composed of the current short-term reward and the (discounted) value of the future gains. This recursive relationship is known as the Bellman equation and is the foundation of many solution methods for MDPs, such as dynamic programming and reinforcement learning.\nFor example, we can test the Bellman equation by comparing the gain at time step \\(t\\) with the short-term reward at time step \\(t\\) and the gain at time step \\(t+1\\).\n\ndcf = 0.9\nG0 = (1-dcf) * np.sum([dcf**t * reward_ensemble[-1, t] for t in range(0, 500)])\nG1 = (1-dcf) * np.sum([dcf**t * reward_ensemble[-1, t+1] for t in range(0, 499)])\n\nnp.allclose((1-dcf) * reward_ensemble[-1, 0] + dcf * G1, G0)\n\nTrue\n\n\nWhy does that work even though we have a finite time horizon of 500 simulation timesteps here? From Figure 6.3 we observe that for a discount factor \\(\\gamma=0.9\\), the contributions of rewards for timesteps above \\(t&gt;100\\) are practically zero. So, with a simulation time of 500 timesteps, we are well above the time horizon the agent cares about. This example illustrates not only the power of the Bellman equation. It also shows how a discount factor induces a timescale the agent cares about.\nGoals or gains are defined over individual reward streams or trajectories. These may be stochastic beyond the agent’s control. Therefore, the agent’s course of action should consider the expected gains, i.e., the average gains over all possible reward streams, given a policy \\(\\mathbf x\\).\n\n\nValue functions\nValue functions are defined to be the expected gain \\(G_t\\) for a policy \\(\\mathbf x\\), given a state or state-action pair. They are helpful in finding a good policy since the best policy will yield the highest value.\nGiven a policy \\(\\mathbf x\\), we define the state value, \\(v_{\\mathbf x}(s)\\), as the expected gain, \\(\\mathbb E_\\mathbf{x}[ G_t | S_t = s]\\), when starting in state \\(s\\) and the following the policy \\(\\mathbf x\\),\n\\[\nv_\\mathbf{x}(s) := \\mathbb E_\\mathbf{x}[ G_t | S_t = s] = (1-\\gamma) \\mathbb E_\\mathbf{x}\\left[\\sum_{\\tau=t}^\\infty \\gamma^\\tau R_{t+\\tau+1} | S_t = s\\right], \\quad \\text{for all } s \\in \\mathcal S,\n\\]\nAnalogously, we define the state-action value, \\(q_\\mathbf{x}(s, a)\\), as the expected gain when starting in state \\(s\\) and executing action \\(a\\), and from then on following policy \\(\\mathbf x\\),\n\\[\nq_\\mathbf{x}(s, a) := \\mathbb E_X [G(t) | s(t) = s, a(t)=a].\n\\]\n\\[\nq_\\mathbf{x}(s, a) := \\mathbb E_\\mathbf{x}[ G_t | S_t = s, A_t = a] = (1-\\gamma) \\mathbb E_\\mathbf{x}\\left[\\sum_{\\tau=t}^\\infty \\gamma^\\tau R_{t+\\tau+1} | S_t = s, A_t = a\\right], \\quad \\text{for all } s \\in \\mathcal S, a \\in \\mathcal A.\n\\]\nHow is that useful?\n\nState values let us compare strategies. A strategy \\(\\mathbf x\\) is better than a strategy \\(\\mathbf y\\) iff for all states \\(s\\): \\(v_\\mathbf{x}(s) &gt; v_\\mathbf{y}(s)\\).\nThe best strategy yields the highest value. At least one strategy is always better than or equal to all other strategies. That is an optimal strategy \\(\\mathbf x_*\\) with the optimal state value \\(v_*(s) := \\max_\\mathbf{x} v_\\mathbf{x}(s), \\forall s\\).\nHighest state-action values indicate the best action. If we knew the optimal state-action value, \\(q_*(s, a) := \\max_\\mathbf{x} q_\\mathbf{x}(s,a), \\forall s,a\\), we can simply assign nonzero probability at each state \\(s\\) only to actions which yield maximum value, \\(\\max_{\\tilde a} q_*(s, \\tilde a)\\).\n\nThe beauty of state(-action) values, in general, and optimal state(-action) values, in particular, is that they encapsulate all relevant information about future environmental dynamics with all inherent stochasticity into short-term actionable numbers. Relevant means relevant to the agent regarding its goal function. State-action values represent the short-term consequences of actions in each state regarding the long-term goal. Optimal state-action values allow for selecting the best actions, irrespective of knowing potential successor states and their values or any details about environmental dynamics. Having such values would save the agent enormous cognitive computational demands every time it must make a decision.\nThe only problem we are left with is, how to compute a policy’s state(-action) values?\n\n\nBellman equation\nWe convert the recursive relationship of the goal function (?eq-bellman1) to state values,\n\\[\\begin{align}\nv_\\mathbf{x}(s) &= \\mathbb E_\\mathbf{x} [G_t | S_t = s] \\\\\n&= \\mathbb E_\\mathbf{x} \\left[ (1-\\gamma) R_{t+1} + \\gamma G_{t+1} | S_t = s \\right] \\\\\n&= (1-\\gamma) \\mathbb E_\\mathbf{x}[ R_{t+1} | S_t = s] + \\gamma \\mathbb E_\\mathbf{x}[G_{t+1} | S_{t+1} = s' ] \\\\\n&= (1-\\gamma) R_\\mathbf{x}(s) + \\gamma \\sum_{s'} T_\\mathbf{x}(s,s') v_\\mathbf{x}(s'),\n\\end{align}\\]\nwhere \\(R_\\mathbf{x}(s)\\) is the expected reward in state \\(s\\) under policy \\(\\mathbf x\\) and \\(T_\\mathbf{x}(s,s')\\) is the expected transition probability from state \\(s\\) to state \\(s'\\) under policy \\(\\mathbf x\\).\nThe expected state reward \\(R_\\mathbf{x}(s)\\) is given by\n\\[R_\\mathbf{x}(s) = \\sum_{a \\in \\mathcal A} \\sum_{s' \\in \\mathcal{S}}  x(s,a)T(s,a,s')R(s,a,s'),\\]\nwhich can be neatly translated into Python using the numpy.einsum method.\n\ns, a, s_ = 0, 1, 2  # defining indices for convenicence\nRs = np.einsum(X, [s, a], T, [s, a, s_], R, [s, a, s_], [s]); Rs\n\narray([0.90068162, 0.        ])\n\n\nThe recursive equation is called the Bellman equation in honor of Richard Bellman and his pioneering work (Bellman 1957). The recursive relationship is exploited in several algorithmic ways to compute the values or even approximate the optimal state values. In recent years, it became possible to approximate optimal state values with deep neural networks, a technique known as deep reinforcement learning (Mnih et al., 2015), allowing for solving high-dimensional MDPs with many - even infinitely many - states and actions. This is a fascinating field of research, which we will not cover in this course. I recommend the interested reader to start from the excellent (introduction to reinforcement learning by Sutton & Barto, 2018).\nOur focus lies on a transparent way of modeling human-environment interactions. We use MDPs as a framework to improve our conceptual understanding of decision-making under uncertainty. Specifically, we exemplify that with the trade-off between short-term and long-term welfare.\nUsing an MDP framework, our models are formulated in a way that - in principle - can scale to high-dimensional systems. The trade-off is, however, the computational cost of solving high-dimensional MDPs. The more complex and “realistic” a model, the less we can understand how the outcome depends on the model’s specifications.\nAs these model specifications are often highly uncertain in the context of sustainability and global change (Polasky et al., 2011), it is very likely that we end up with an optimal policy for a wrong model that is not useful for decision-making. It might even be harmful, conveying a false sense of optimality. This problem gets worse with the complexity of the model. The more model parameters we have to specify as the input to the model, the more sources of possible but unconscious uncertainty there is.\nTherefore, we will focus on minimalistic models but take a radical stance to account for parameter uncertainty to keep the analysis and interpretation transparent. Thus, in the following, we derive an analytical expression how to compute the state values for a given policy.\nWe write the Bellman equation in matrix form,\n\\[\n\\mathbf v_\\mathbf{x} = (1-\\gamma) \\mathbf R_\\mathbf{x} + \\gamma \\underline{\\mathbf T}_\\mathbf{x} \\mathbf v_\\mathbf{x}\n\\]\nwhere \\(\\mathbf R_\\mathbf{x}\\) is the vector of expected state rewards \\(R_\\mathbf{x}(s)\\), \\(\\underline{\\mathbf T}_\\mathbf{x}\\) is the transition matrix, and \\(\\mathbf v_\\mathbf{x}\\) is the vector of state values under policy \\(\\mathbf x\\). Thus, \\(\\mathbf v_\\mathbf{x}\\) and \\(\\mathbf R_\\mathbf{x}\\) are vectors of dimension \\(Z\\), i.e., the number of states, and \\(\\underline{\\mathbf T}_\\mathbf{x}\\) is the transition matrix of dimension \\(Z \\times Z\\).\nWe can solve this equation for \\(\\mathbf v_\\mathbf{x}\\),\n\\[\\begin{align}\n\\mathbf v_\\mathbf{x} &= (1-\\gamma) \\mathbf R_\\mathbf{x} + \\gamma \\underline{\\mathbf T}_\\mathbf{x} \\mathbf v_\\mathbf{x} \\\\\n\\mathbf v_\\mathbf{x} - \\gamma \\underline{\\mathbf T}_\\mathbf{x} \\mathbf v_\\mathbf{x} &= (1-\\gamma) \\mathbf R_\\mathbf{x} \\\\\n(\\mathbb 1_Z - \\gamma\\underline{\\mathbf T}_\\mathbf{x}) \\mathbf v_\\mathbf{x} &= (1-\\gamma) \\mathbf R_\\mathbf{x} \\\\\n(\\mathbb 1_Z - \\gamma\\underline{\\mathbf T}_\\mathbf{x})^{-1} (\\mathbb 1_Z - \\gamma\\underline{\\mathbf T}_\\mathbf{x}) \\mathbf v_\\mathbf{x} &= (1-\\gamma) (\\mathbb 1_Z - \\gamma\\underline{\\mathbf T}_\\mathbf{x})^{-1} \\mathbf R_\\mathbf{x} \\\\\n\\mathbf v_\\mathbf{x} &= (1-\\gamma) (\\mathbb 1_Z - \\gamma\\underline{\\mathbf T}_\\mathbf{x})^{-1} \\mathbf R_\\mathbf{x}, \\\\\n\\end{align}\\]\nwhere \\(\\mathbb 1_Z\\) is the identity matrix of dimension \\(Z\\).\nThus, to compute state value, we must invert a \\(Z\\times Z\\)-matrix, which is computationly infeasable for large MDPs. For low-dimensional models, however, it works perfectly fine and can even be executed analytically.\nIn Python, an identity matrix can be created with the eye function from the numpy package.\n\nnp.eye(2)\n\narray([[1., 0.],\n       [0., 1.]])\n\n\nWe define a function to compute the state values given a policy, a transition tensor, a reward tensor, and a discount factor. The function returns a vector of state values. We use the inv function from the numpy.linalg package to invert the matrix.\n\ndef compute_statevalues(\n    policy_Xsa, transitions_Tsas, rewards_Rsas, discountfactor):\n    s, a, s_ = 0, 1, 2  # defining indices for convenicence\n    Tss = np.einsum(policy_Xsa, [s, a], transitions_Tsas, [s, a, s_], [s,s_])\n    Rs = np.einsum(policy_Xsa, [s, a], transitions_Tsas, [s, a, s_], \n                   rewards_Rsas, [s, a, s_], [s])\n    inv = np.linalg.inv((np.eye(2) - discountfactor*Tss))\n    Vs = (1-discountfactor) * np.einsum(inv, [s,s_], Rs, [s_], [s])\n    return Vs\n\n\nVs = compute_statevalues(X, T, R, 0.9); Vs\n\narray([0.7220388 , 0.13059414])\n\n\nThus, in contrast to the expected state rewards, the long-term value of the degraded state is above the immediate reward of the degraded state, \\(r_d=0\\).\n\nRs\n\narray([0.90068162, 0.        ])\n\n\nIn the state value \\(v_{\\mathbf x}(\\mathsf{d})\\) of the degraded state, the agent anticipates the recovery of the environment and the return to the prosperous state. Likewise, the agent anticipates the collapse of the environment and the loss of the prosperous state in the state value of the prosperous state. Hence, \\(v_{\\mathbf x}(\\mathsf{p})\\) is smaller than the expected reward of the prosperous state, \\(R_{\\mathbf x}(\\mathsf{p})\\). The expected state rewards only consider the immediate possible transitions, while the state values also account for the long-term consequences of these transitions.\nHow do these values depend on the discount factor \\(\\gamma\\)?\nWe define an array of linearly spaced values of different discount factors,\n\ndiscountfactors = np.linspace(0.001, 0.9999, 301)\n\nWe then compute the state value for each discount-factor value using a list comprehension,\n\nvalues = np.array([compute_statevalues(X, T, R, dcf) for dcf in discountfactors])\nvalues.shape\n\n(301, 2)\n\n\nWe plot the state values along the discount factors on the x-axis. We also include the expected state rewards, which are independent of the discount factor.\n\nplt.plot(discountfactors, Rs[0]*np.ones_like(discountfactors), label='$R(p)$', c='green', ls='--');\nplt.plot(discountfactors, values[:, 0], label='$v(p)$', c='green');\nplt.plot(discountfactors, values[:, 1], label='$v(d)$', c='brown');\nplt.plot(discountfactors, Rs[1]*np.ones_like(discountfactors), label='$R(d)$', c='brown', ls='--');\nplt.legend(); plt.xlabel('Discount factor'); plt.ylabel('Value')\n\nText(0, 0.5, 'Value')\n\n\n\n\n\n\n\n\n\nWhen the discount factor is close to zero, \\(\\gamma=0\\), the values equal the average immediate rewards.\nWhen the discount factor \\(\\gamma\\rightarrow 1\\), the state values for the prosperous and the degraded state approach each other.\nLast, the state values change more for large \\(\\gamma&gt;0.85\\) than for lower \\(\\gamma\\).\nSo far, we investigated how to compute the state values for a given policy and use a random policy as an example. To eventually answer what the agent should do, we must compare multiple policies and find the best one.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sequential Decisions</span>"
    ]
  },
  {
    "objectID": "03.01-SequentialDecisions.html#optimal-policies",
    "href": "03.01-SequentialDecisions.html#optimal-policies",
    "title": "6  Sequential Decisions",
    "section": "6.5 Optimal policies",
    "text": "6.5 Optimal policies\nThe key question of our example model is, when is it better to play safe, and when is it better to be risky? From our model definition, we can easily see that, in the degraded state, it is always better to play safe as this is the only way to recover to the more rewarding, prosperous state. But what about the prosperous state?\n\nNumerical computation\nWe define two policies, a safe policy, \\(\\mathbf x_{\\text{safe}}\\), where the agent always chooses the safe action and a risky policy, \\(\\mathbf x_{\\text{risky}}\\), where the agent always chooses the risky action in the prosperous state.\n\nXsafe = np.array([[1,0],[1,0]])\nXrisk = np.array([[0,1],[1,0]])\n\nFor each of these policies, we compute the state values with our compute_statevalues function,\n\nV_safe = np.array([compute_statevalues(Xsafe, T, R, dcf) for dcf in discountfactors])\nV_risk = np.array([compute_statevalues(Xrisk, T, R, dcf) for dcf in discountfactors])\n\nand plot these values for each policy and each state as\n\nplt.plot(discountfactors, V_safe[:, 0], label='$v_{safe}(p)$', color='blue'); \nplt.plot(discountfactors, V_safe[:, 1], label='$v_{safe}(d)$', color='blue', lw=0.4); \nplt.plot(discountfactors, V_risk[:, 0], label='$v_{risk}(p)$', color='red'); \nplt.plot(discountfactors, V_risk[:, 1], label='$v_{risk}(d)$', color='red', lw=0.4); \nplt.legend(); plt.xlabel('Discount factor'); plt.ylabel('Value');\n\n\n\n\n\n\n\n\nWe find a critical discount factor \\(\\hat \\gamma\\), where the optimal policy changes. Below \\(\\hat\\gamma\\), the agent acts optimally by choosing the risky policy. Above the critical discount factor, \\(\\hat\\gamma\\), the agent acts optimally by choosing the safe policy.\nHence, when the agent cares enough about the future, it is better to be safe than sorry, even if this means giving up immediate, short-term welfare (\\(r_s &lt; r_r\\)).\nBut how does this result depend on the other parameters, \\(p_c, p_r, r_s, r_r, r_d\\)?\nThis investigates how the optimal policy depends on all parameters of the model; we first define general transition and reward functions that return a transition and reward tensor, given our model parameters. We make these functions general by passing the most general datatype to the respective numpy.arrays, i.e., dtype=object. This allows us to store arbitrary Python objects in the arrays, such as float numbers or symbolic expressions.\n\ndef get_transitions(pc, pr):\n    c=0; r=1; p=0; d=1  # for reference we define these as function-local variables\n    T = np.zeros((2,2,2), dtype=object)\n    T[p,c,d] = 0            # Cautious action guarantees prosperous state\n    T[p,c,p] = 1   # \n    T[p,r,d] = pc;          # Risky action risks collapse\n    T[p,r,p] = 1-T[p,r,d]   # ... but collapse may not happen\n    T[d,c,p] = pr           # Recovery only possible with cautious action \n    T[d,c,d] = 1-T[d,c,p]   # ... but recovery might not happen\n    T[d,r,p] = 0            # Risky action remains at degraded state\n    T[d,r,d] = 1\n    return T\n\n\ndef get_rewards(rs, rr=1, rd=0):\n    c=0; r=1; p=0; d=1  # for reference we define these as function-local variables\n    R = np.zeros((2,2,2), dtype=object)\n    R[p,c,p] = rs            # The cautious action at the prosperous state guarantees the safe reward \n    R[p,r,p] = rr            # The risky action can yield the risky reward if the environment remains at p\n    R[d,:,:] = R[:,:,d] = rd # Otherwise, the agent receives rd\n    return R\n\nNow, we can create transition and reward tensors flexibly. As we want to perform numerical computations, we specify the data type of the arrays to be float numbers.\n\nT = get_transitions(0.04, 0.1).astype(float)\nT\n\narray([[[1.  , 0.  ],\n        [0.96, 0.04]],\n\n       [[0.1 , 0.9 ],\n        [0.  , 1.  ]]])\n\n\n\nR = get_rewards(0.7).astype(float)\nR\n\narray([[[0.7, 0. ],\n        [1. , 0. ]],\n\n       [[0. , 0. ],\n        [0. , 0. ]]])\n\n\nLet’s assume we want to know how the critical discount factor \\(\\hat \\gamma\\) depends on the collapse probability \\(p_c\\) for a given recovery probability \\(p_r=0.01\\) and safe reward \\(r_s=0.8\\), a risky reward \\(r_r = 1.0\\) and a degraded reward \\(r_d=0.0\\). We define these quantities as\n\npr = 0.01\nrs = 0.5\nrr = 1.0\nrd = 0.0\n\nand let the discount factor and collapse probabilities run from almost zero to almost one with a resolution of 301 elements,\n\ndiscountfactors = np.linspace(0.0001, 0.9999, 301)\ncollapseprobabilities = np.linspace(0.0001, 0.9999, 301)\n\nWe will go through each combination of discount factors and collapse probabilities, compute the state values for both policies, compare them, and store the result in a data container. We prepare this data container by\n\nrisky_optimal_data_container = np.zeros((discountfactors.size, collapseprobabilities.size, 2))\n\nNow, we are ready to execute our simulation. We loop through each discount factor and for each discount factor through each collapse probability, obtain our new transition matrix, compute the state values, and store them in our data container. The Jupyter cell magic %%time shows us how long it took to execute that cell.\n\n%%time\nfor i, dcf in enumerate(discountfactors):\n    for j, pc in enumerate(collapseprobabilities):\n        T = get_transitions(pc, pr).astype(float)\n        R = get_rewards(rs, rr, rd).astype(float)\n        Vs_risk = compute_statevalues(Xrisk, T, R, dcf)\n        Vs_safe = compute_statevalues(Xsafe, T, R, dcf)\n        risky_optimal_data_container[i, j, :] = Vs_risk &gt; Vs_safe\n\nCPU times: user 10.9 s, sys: 1.94 s, total: 12.8 s\nWall time: 8.7 s\n\n\nWe noticeably have to wait for the result!\n\nplt.subplot(131); plt.xticks([]); plt.yticks([]); \nplt.subplot(133); plt.xticks([]); plt.yticks([]); \nplt.subplot(132) # just to center the plot in the middle\n\nplt.pcolormesh(collapseprobabilities, discountfactors, \n               risky_optimal_data_container[:,:,0], cmap='bwr')\nplt.ylabel('Discount factor'); plt.xlabel('Collapse probabiliy');\n\n\n\n\n\n\n\n\nThe higher the collapse probability, the lower the critical discount factor. When the collapse is more likely, less future care is required to evaluate the safe policy as optimal. When the collapse probability is zero (\\(p_c=0\\)), the critical discount factor is one (\\(\\hat\\gamma=1\\)), and the agent should always choose the risky policy, as the environment cannot be destroyed.\nWhen the discount factor is zero (\\(\\gamma=0\\)), the critical collapse probability is a half \\(\\hat p_c=0.5\\). If an environmental collapse under the risky action is more likely \\(p_c &gt; \\hat p_c\\), the agent should always choose the safe policy and vice versa. But where does the value \\(0.5\\) come from? Intuitively, it is the ratio between the safe and the risky reward, \\(r_s/r_r\\).\nBut how can we be sure? And wouldn’t it be great, if we could speed up the computation time somehow?\nThe solution to both questions lies in a symbolic computation of the critical parameter values \\(\\hat \\gamma, \\hat p_c, \\hat p_r, \\hat r_s, \\hat r_r, \\hat r_d\\).\n\n\nSymbolic computation\nWe define symbolic expressions for our model parameters and obtain the corresponding transition and reward tensors,\n\npc, pr = sp.symbols(\"p_c, p_r\")\nT = sp.Array(get_transitions(pc, pr))\nT\n\n\\(\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}1 & 0\\\\1 - p_{c} & p_{c}\\end{matrix}\\right] & \\left[\\begin{matrix}p_{r} & 1 - p_{r}\\\\0 & 1\\end{matrix}\\right]\\end{matrix}\\right]\\)\n\n\n\nrs, rr, rd = sp.symbols(\"r_s r_r r_d\")\nR = sp.Array(get_rewards(rs, rr, rd))\nR\n\n\\(\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}r_{s} & r_{d}\\\\r_{r} & r_{d}\\end{matrix}\\right] & \\left[\\begin{matrix}r_{d} & r_{d}\\\\r_{d} & r_{d}\\end{matrix}\\right]\\end{matrix}\\right]\\)\n\n\nAs before, we also define a risky and a safe policy, now as symbolic variables,\n\nXsafe = sp.Array([[1,0],[1,0]])\nXrisk = sp.Array([[0,1],[1,0]])\n\nand also the discount factor as a symbolic variable\n\ndcf = sp.symbols(\"gamma\")\ndcf\n\n\\(\\displaystyle \\gamma\\)\n\n\nLuckily, we only have to change our compute_statevalues slightly, (since the np.einsum function also works with Sympy expressions)\n\ndef symbolic_statevalues(policy_Xsa, transitions_Tsas, rewards_Rsas, discountfactor=dcf):\n    s, a, s_ = 0, 1, 2  # defining indices for convenicence\n    Tss = sp.Matrix(np.einsum(policy_Xsa, [s, a], transitions_Tsas, [s, a, s_], [s,s_]))   \n    Rs = sp.Array(np.einsum(policy_Xsa, [s, a], transitions_Tsas, [s, a, s_], rewards_Rsas, [s, a, s_], [s]))\n    inv = (sp.eye(2) - discountfactor*Tss).inv(); inv.simplify()  # sp.simplify() often helps \n    Vs = (1-discountfactor) * sp.Matrix(np.einsum(inv, [s,s_], Rs, [s_], [s])); Vs.simplify()\n    return Vs\n\nThe symbolic expressions of the state values for the risky policy are\n\nsymbolic_statevalues(Xrisk, T, R)\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{\\gamma p_{c} p_{r} r_{d} - \\gamma p_{c} p_{r} r_{r} + \\gamma p_{c} r_{r} + \\gamma p_{r} r_{r} - \\gamma r_{r} + p_{c} r_{d} - p_{c} r_{r} + r_{r}}{\\gamma p_{c} + \\gamma p_{r} - \\gamma + 1}\\\\\\frac{\\gamma p_{c} p_{r} r_{d} - \\gamma p_{c} p_{r} r_{r} + \\gamma p_{c} r_{d} + \\gamma p_{r} r_{r} - \\gamma r_{d} + r_{d}}{\\gamma p_{c} + \\gamma p_{r} - \\gamma + 1}\\end{matrix}\\right]\\)\n\n\nand for the safe policy, are\n\nsymbolic_statevalues(Xsafe, T, R)\n\n\\(\\displaystyle \\left[\\begin{matrix}r_{s}\\\\\\frac{\\gamma p_{r} r_{s} - \\gamma r_{d} + r_{d}}{\\gamma p_{r} - \\gamma + 1}\\end{matrix}\\right]\\)\n\n\nTo check whether the risky policy is optimal, we subtract the value of the safe policy from the risky policy’s value at the prosperous state 0.\n\nrisky_optimal = sp.simplify(symbolic_statevalues(Xrisk, T, R)[0])\\\n    - sp.simplify(symbolic_statevalues(Xsafe, T, R)[0]) \nsp.simplify(risky_optimal)\n\n\\(\\displaystyle \\frac{\\gamma p_{c} p_{r} r_{d} - \\gamma p_{c} p_{r} r_{r} + \\gamma p_{c} r_{r} + \\gamma p_{r} r_{r} - \\gamma r_{r} + p_{c} r_{d} - p_{c} r_{r} + r_{r} - r_{s} \\left(\\gamma p_{c} + \\gamma p_{r} - \\gamma + 1\\right)}{\\gamma p_{c} + \\gamma p_{r} - \\gamma + 1}\\)\n\n\nWe can solve this equation for any variable. For example, to check the critical collapse probability for an entirely myopic agent with zero care for the future, we solve the equation for the collapse probability \\(p_c\\) and substitute the discount factor \\(\\gamma=0\\).\n\nsp.solve(risky_optimal, pc)[0].subs(dcf, 0)\n\n\\(\\displaystyle \\frac{- r_{r} + r_{s}}{r_{d} - r_{r}}\\)\n\n\nThus, our initution about the ratio between \\(r_s\\) and \\(r_r\\) was not entirely correct. In fact, we can simplify the three reward parameters \\(r_r\\), \\(r_s\\), and \\(r_d\\). As it is irrelevant to the agent’s decision whether all rewards are multiplied by a factor or all rewards are added by a constant, we can set \\(r_d=0\\) and \\(r_s=1\\) without loss of generality.\nSetting the degraded reward to zero, \\(r_d=0\\), and the risky reward to one, \\(r_r=1\\), improves the transparency and interpretability of the model.\n\nsp.solve(risky_optimal.subs(rr, 1).subs(rd, 0), pc)[0].subs(dcf, 0)\n\n\\(\\displaystyle 1 - r_{s}\\)\n\n\nThus, the critical collapse probability \\(\\hat p_c\\) for \\(\\gamma =0\\) is given by the \\(\\hat p_c = 1-r_s\\).\nBy using symbolic calculations, we improve the transparency and interpretability of our model.\nHow can we speed up the computation time with sympy?\n\n\nEfficient computation\nTo create a plot as above, it is an excellent strategy to convert this symbolic expression into a numeric function. In sympy, this is done with the sympy.lambdify function, (called as sp.lambdify((&lt;symbolic parameters&gt;), &lt;symbolic expression to be turned into a numeric function&gt;)\n\nrisky_optimal_func = sp.lambdify((pc,pr,dcf,rs,rr,rd), risky_optimal)\n\nFor example, we can now execute risky_optimal_func for \\(p_c=0.2\\), \\(p_r=0.01\\), \\(\\gamma=0.9\\), \\(r_s=0.5\\), \\(r_r=1.0\\), and \\(r_d=0.0\\) as\n\nrisky_optimal_func(0.2, 0.01, 0.9, 0.5, 1.0, 0.0)\n\n-0.19826989619377183\n\n\nand learn that the risky policy is not optimal in this case.\nHowever, the big advantage of a lambdified function is that we can apply it in vectorized form. This means the parameters don’t have to be single numbers. They can be vectors or even larger tensors. See, for example,\n\ngams = np.linspace(0.0001, 0.9999, 9)\nrisky_optimal_func(0.2, 0.01, gams, 0.5, 1.0, 0.0)\n\narray([ 0.299984  ,  0.27779382,  0.25014334,  0.21473437,  0.1677686 ,\n        0.10248474,  0.00556964, -0.15331544, -0.46154209])\n\n\nThus, to recreate our example from above, where we wanted to know how the critical discount factor \\(\\hat\\gamma\\) depends on the collapse probability \\(p_c\\) for given other parameters, we can now use the risky_optimal_func directly in vectorized form.\nHowever, if we simply put two vectors (of the same dimension) inside the function, we only get\n\ndiscountfactors = np.linspace(0.0001, 0.9999, 9)\ncollapseprobabilities = np.linspace(0.0001, 0.9999, 9)\nrisky_optimal_func(collapseprobabilities, 0.01, discountfactors, 0.5, 1.0, 0.0)\n\narray([ 0.49989999,  0.3595776 ,  0.19241376,  0.01072705, -0.16556291,\n       -0.31475139, -0.42146066, -0.48138804, -0.499999  ])\n\n\nwe only get one vector (of the same dimension) out. This is, beacuse vectorization groups changes along a dimension together.\nThis means we need to separate the variation in the discount factors (in discountfactors) and the variation in the collapse probabilities (in collapseprobabilities) into different dimensions. Luckily, we don’t have to do that manually. The numpy method numpy.meshrid exactly fits this purpose. For example,\n\ndiscountfactors = [0.8, 0.9]\ncollapseprobabilities = [0.1, 0.2, 0.3]\nnp.meshgrid(discountfactors, collapseprobabilities)\n\n[array([[0.8, 0.9],\n        [0.8, 0.9],\n        [0.8, 0.9]]),\n array([[0.1, 0.1],\n        [0.2, 0.2],\n        [0.3, 0.3]])]\n\n\nIn practise, we can use a meshgrid as follows,\n\npr_ = 0.01\nrs_ = 0.5\nrr_ = 1.0\nrd_ = 0.0\ndiscountfactors = np.linspace(0.0001, 0.9999, 301)\ncollapseprobabilities = np.linspace(0.0001, 0.9999, 301)\nDCFs, PCs = np.meshgrid(discountfactors, collapseprobabilities)\n\n\nrisky_optimal_func(PCs, pr_, DCFs, rs_, rr_, rd_)\n\narray([[ 0.49989999,  0.49989966,  0.49989932, ...,  0.49398743,\n         0.49251766,  0.49009707],\n       [ 0.49656699,  0.49655555,  0.49654403, ...,  0.32758543,\n         0.29387422,  0.24378043],\n       [ 0.49323399,  0.49321152,  0.4931889 , ...,  0.20822659,\n         0.1607447 ,  0.09481031],\n       ...,\n       [-0.49323534, -0.49325773, -0.49328013, ..., -0.4998874 ,\n        -0.49990965, -0.4999319 ],\n       [-0.49656768, -0.49657908, -0.49659048, ..., -0.49994305,\n        -0.49995431, -0.49996556],\n       [-0.49990001, -0.49990034, -0.49990068, ..., -0.49999835,\n        -0.49999867, -0.499999  ]])\n\n\nNotice how quickly that was compared to our previous calculations!\nTo time how long the cell execution takes more precisely, we can use the %%timeit cell magic command:\n\n%%timeit\nrisky_optimal_func(PCs, pr_, DCFs, rs_, rr_, rd_)\n\n1.92 ms ± 396 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nIt executes the cell multiple times and presents us with a short summary statistic. Compare the average runtime of the cell with the numerical computation. It is around 5000 times faster!\nThus, we can summarize the lambdified sympy expression into a plot_parameter_space function:\n\ndef plot_parameter_space(safe_reward=0.5, risky_reward=1.0, degraded_reward=0.0, recov_prop=0.05):\n    plt.subplot(131); plt.xticks([]); plt.yticks([]); \n    plt.subplot(133); plt.xticks([]); plt.yticks([]); \n    plt.subplot(132) # just to center the plot in the middle\n    \n    resolution=251\n    \n    X = np.linspace(0.0001, 0.9999, resolution)\n    Y = np.linspace(0.0001, 0.9999, resolution)\n    XX, YY = np.meshgrid(X, Y)\n\n    ro = risky_optimal_func(XX, recov_prop, YY, safe_reward, risky_reward, degraded_reward)\n    plt.pcolormesh(XX, YY, ro, cmap='bwr', vmin=-0.1, vmax=0.1)\n    plt.ylabel('Discount factor'); plt.xlabel('Collapse leverage');\n\n\nplot_parameter_space()\n\n\n\n\n\n\n\n\nWhen working with this Jupyter Notebook directly, we can interactively explore the parameter space of the model.\nFor an agent that does not discount the future at all, i.e., with \\(\\gamma \\rightarrow 1\\), the critical collapse leverage yields,\n\nsp.simplify(sp.solve(risky_optimal.subs(rr,1).subs(rd, 0), pc)[0].subs(dcf, 1))\n\n\\(\\displaystyle \\frac{p_{r} \\left(1 - r_{s}\\right)}{p_{r} + r_{s}}\\)\n\n\nThus, if there is zero recovery probability \\(p_r=0\\), the safe policy is optimal regardless of the relative reward \\(0&lt;r_s&lt;1\\).\n\nplot_parameter_space(recov_prop=0.0)\n\n\n\n\n\n\n\n\nIf \\(p_r &gt; 0\\), then it depends on the relative reward \\(r_s\\) whether the safe or the risky policy is optimal for a fully farsighted agent.\nTaken together, by using symbolic computation from the sympy package, we improve interpretability, transparancey and computational efficiency of our model.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sequential Decisions</span>"
    ]
  },
  {
    "objectID": "03.01-SequentialDecisions.html#learning-goals-revisited",
    "href": "03.01-SequentialDecisions.html#learning-goals-revisited",
    "title": "6  Sequential Decisions",
    "section": "6.6 Learning goals revisited",
    "text": "6.6 Learning goals revisited\n\nWe introduced the elements of a Markov Decision Process (MDP) and discussed how they relate to applications in human-environment interactions\nWe simulateed and visualized the time-evolution of an MDP.\nWe covered what value functions are, why they are usful and how to realte to the agent’s goal and Bellman equation.\nWe computed value functions in serveral ways and visualized how the best policy depends on other model parameters.\n\n\n\n\n\nBarfuss, W., Donges, J. F., Lade, S. J., & Kurths, J. (2018). When optimization for governing human-environment tipping elements is neither sustainable nor safe. Nature Communications, 9(1), 2354. https://doi.org/10.1038/s41467-018-04738-z\n\n\nMarescot, L., Chapron, G., Chadès, I., Fackler, P. L., Duchamp, C., Marboutin, E., & Gimenez, O. (2013). Complex decisions made simple: A primer on stochastic dynamic programming. Methods in Ecology and Evolution, 4(9), 872–884. https://doi.org/10.1111/2041-210X.12082\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236\n\n\nPolasky, S., Carpenter, S. R., Folke, C., & Keeler, B. (2011). Decision-making under great uncertainty: Environmental management in an era of global change. Trends in Ecology & Evolution, 26(8), 398–404. https://doi.org/10.1016/j.tree.2011.04.007\n\n\nSchultz, W., Stauffer, W. R., & Lak, A. (2017). The phasic dopamine signal maturing: From reward via behavioural activation to formal economic utility. Current Opinion in Neurobiology, 43, 139–148. https://doi.org/10.1016/j.conb.2017.03.013\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (Second edition). The MIT Press.\n\n\nWilliams, B. K. (2009). Markov decision processes in natural resources management: Observability and uncertainty. Ecological Modelling, 220(6), 830–840. https://doi.org/10.1016/j.ecolmodel.2008.12.023",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sequential Decisions</span>"
    ]
  },
  {
    "objectID": "03.02-StrategicInteractions.html",
    "href": "03.02-StrategicInteractions.html",
    "title": "7  Strategic Interactions",
    "section": "",
    "text": "7.1 Motivation | Collective action for sustainability\nConsider the following questions:\nPlease enter your views below.\nScott Barrett asked these questions at the beginning of a talk, which I can highly recommend watching. The talk is called Climate Change Diplomacy: a Most Dangerous Game and is given at the London School of Economics.\nThe typical answers to these questions raise the point of why it is so difficult to succeed in stopping climate change despite recognizing the problem and trying to solve it.\nThe outcome depends on all! Carbon dioxide (CO2) is the most prevalent greenhouse gas driving global climate change. CO2 from different sources (fossil fuels, burned biomass, land ecosystems, oceans) is being added to Earth’s atmosphere from various locations over the globe. Then, it mixes relatively fast in the atmosphere, i.e., the consequences for a region do not depend on how much that region emits but on the overall emissions.\nTo stabilize the climate, (net) emissions have to go to zero.\nWhile the climate is the most discussed example, the collective action problem extends to the whole planetary commons.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strategic Interactions</span>"
    ]
  },
  {
    "objectID": "03.02-StrategicInteractions.html#motivation-collective-action-for-sustainability",
    "href": "03.02-StrategicInteractions.html#motivation-collective-action-for-sustainability",
    "title": "7  Strategic Interactions",
    "section": "",
    "text": "Do you think climate change is a significant problem the world needs to address?\nDo you think the world has been trying?\nDo you think the world has succeeded?\n\n\n\n\n\n\n\n\n\nThis visualization from NASA shows the CO2 added to Earth’s atmosphere over 2021, split into four major contributors: fossil fuels in orange, burning biomass in red, land ecosystems in green, and the ocean in blue. The dots on the surface also show how green land ecosystems and the ocean in blue absorb atmospheric carbon dioxide. Though the land and oceans are each carbon sinks in a global sense, individual locations can be sources at different times.\n\n\n\n\n\n\n\nLinear damages of climate change\n\n\n\n\nAdvantages of game theory\nIn this lecture, we introduce the basics of mathematical game theory to uncover the underlying mechanisms of strategic interactions. We will see how the behavior of individuals can lead to collective outcomes that are not in the interest of any individual. We will also discuss possible mechanisms and variations of the situation that help to overcome these challenges and will acknowledge the limitations of these variations.\nA mathematical game describes an action situation where an outcome relevant to an individual depends on at least one other actor. This is why we speak of interactions instead of only actions of a single-agent action situation. The strategic aspect comes into play when the actors are aware of the interdependence and can anticipate the actions of others.\n\n\nLearning goals\nAfter this lecture, students will be able to:\n\nApply game theory to model multi-agent action situations\nResolve games by finding Nash equilibria\nDescribe the dimensions of a social dilemma\nExplain two special kinds of games: agreement games and threshold public goods, and how they relate to the dimensions of a social dilemma.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strategic Interactions</span>"
    ]
  },
  {
    "objectID": "03.02-StrategicInteractions.html#game-theory",
    "href": "03.02-StrategicInteractions.html#game-theory",
    "title": "7  Strategic Interactions",
    "section": "7.2 Game theory",
    "text": "7.2 Game theory\n\nLife is a game. At least in theory.\n\n\nimport numpy as np  \nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import BoundaryNorm\nfrom ipywidgets import interact, fixed\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; plt.rcParams['grid.linewidth'] = 0.25; \n\nGame theory in itself is diverse. Here, we focus on normal-form games with the following elements.\n\nA finate set of \\(N\\) agents \\(\\mathcal I = \\{2,\\dots, N\\}\\) participating in an interaction.\nFor each agent \\(i\\in\\mathcal I\\), a discrete set of options or actions \\(\\mathcal A^i = \\{A^i_1, \\dots, A^i_M\\}\\).\n\nLet’s denote the joint action set by \\(\\boldsymbol{\\mathcal A} = \\mathcal A^1 \\times \\dots \\times A^N\\).\nAn action profile \\(\\boldsymbol a = (a^1,\\dots,a^N) \\in \\boldsymbol{\\mathcal A}\\) is a joint action of all agents.\n\nFor each agent \\(i\\in\\mathcal I\\), a welfare, reward or payoff function \\(R^i: \\boldsymbol{\\mathcal A} \\rightarrow \\mathbb R\\).\n\n\\(R^i(\\boldsymbol a)\\) is the welfare agent \\(i\\) receives when all agents chose \\(\\boldsymbol a = (a^1,\\dots,a^N) \\in \\boldsymbol{\\mathcal A}\\)\n\nThe agent’s policy or strategy \\(x^i: \\mathcal A^i \\rightarrow [0,1]\\).\n\n\\(x^i(a^i)\\) is the probability agent \\(i\\) chooses action \\(a^i\\).\nA strategy is called pure if it chooses actions deterministically. If it is not pure, it is called mixed instead.\n\n\n\nLet’s play\n\nYou are given two choices: abate climate change or continue to pollute the atmosphere.\nYou gain 100 Euros (of averated damages) for each person that chooses abate.\nIf you choose abate, you must pay 250 Euros.\n\nWhat would you choose?\n\n\n\n        \n        \n\n\nWould you have chosen differently if the action had been labeled red and blue?\nThe agents or actors in this game are all participants in this questionnaire. The actions are abate and pollute. The payoff is a given in (hypothetical) money. The strategies are the deterministic choice of either abate or pollute.\n\n\nMathematical model\nLet us model the reward functions of this normal-form game in general terms. We have \\(N\\) agents, the players of the game, each with two actions \\(\\mathsf{A}\\) or \\(\\mathsf{P}\\), the choices they can make.\nEach abating actor brings a benefit \\(b\\) (of averted damages) to all actors at an individual cost \\(c\\).\nFrom the perspective of a focal agent, let \\(N_\\mathsf{A}\\) be the number of all other actors abating. The rewards are then\n\nfor a polluting actor: \\(R_\\mathsf{P}(\\mathbf a) = N_\\mathsf{A} b\\)\nfor an abating actor: \\(R_\\mathsf{A}(\\mathbf a) = (N_\\mathsf{A} + 1) b - c\\)\n\nWe visualize these payoffs as a function of the number of other abating actors.\n\ndef plot_payoffs(N, b, c, ax=None):\n    Na_other = np.arange(0, N)\n    bA = (Na_other+1)*b - c\n    bP = Na_other*b\n\n    _, a = plt.subplots() if ax is None else (None, ax)\n    a.plot(Na_other, bA, '.-', label='Abate')\n    a.plot(Na_other, bP, '.-', label='Pollute')\n    a.legend(); a.set_xlabel('Number of other actors abating'); a.set_ylabel('Payoff')\n\n\nplot_payoffs(N=100, b=1, c=50)\n\n\n\n\n\n\n\n\nWe observe that the reward of polluting is always higher than that of abating, regardless of the number of other actors abating.\nThus, regardless of what the others do, every individual is incentivized to choose pollute. Hence, for every individual \\(i\\), pollute is a dominant strategy.\nDefinition | Dominant strategy\nLet \\(\\boldsymbol x = (x^i, \\boldsymbol x^{-i})\\) be a joint strategy, where \\(x^i\\) is actor \\(i\\)’s strategy, and \\(\\boldsymbol x^{-i}\\) is the joint strategy of all other actors.\nActor \\(i\\) has a dominant strategy, \\(x^i_D\\), iff\n\\[R^i(x^i_D,  {\\boldsymbol x}^{-i}) \\geq R^i(\\tilde x^i, \\tilde {\\boldsymbol x}^{-i})\\]\nfor all possible other strategies \\(\\tilde x^i, \\tilde {\\boldsymbol x}^{-i}\\).\nThus, when all actors choose pollute, no actor has an incentive to deviate from this strategy. This is called a Nash equilibrium.\n\n\nNash equilibirum\n\nDefinition\nLet \\(\\boldsymbol x = (x^i, \\boldsymbol x^{-i})\\) be a joint strategy, where \\(x^i\\) is agent \\(i\\)’s strategy and \\(\\boldsymbol x^{-i}\\) is the joint strategy of all other agents.\nA joint strategy \\(\\boldsymbol x_*\\) is a Nash-equilibrium when no agent can benefit from changing its strategy unilaterally,\n\\[ R^i(x^i_*, \\boldsymbol x_*^{-i}) \\geq R^i(\\tilde x^i, \\boldsymbol x_*^{-i})\\]\nfor all agents \\(i\\) and all other strategies \\(\\tilde X^i\\).\nIn 1950, John Nash showed (in a one-pager) that such an equilibrium always exists for games with any number of (finite) actors with a finite number of actions and any type of payoffs (beyond zero-sum games).\nDeeDive | Nash’s equilibrium produces the same solution as von Neumann and Morgenstern’s minimax in the two-player zero-sum game. But while von Neumann and Morgenstern had struggled to extend the minimax solution beyond two-player zero-sum games in their 600-page book, Nash’s solution could be extended to any other case! I recommend the following blog post Time for Some Game Theory - by Lionel Page for an intuitive introduction to game theory.\n\n\nInterpretation\nThere is much confusion about how to interpret a Nash equilibrium, considering the question of how actors would be able to play a Nash equilibrium in a one-shot interaction. We will briefly discuss two interpretations: the rationalistic and the learning interpretation.\nIn the rationalistic interpretation, rational players would mentally simulate the various ways the game could unfold and choose a Nash equilibrium. However, when faced with non-trivial strategic situations for the first time, people typically fail to play a Nash equilibrium of the game. When there is more than one equilibrium, this interpretation cannot say which one an actor would choose (This is also known as the equilibrium selection problem).\nIn the learning interpretation, actors learn to play a Nash equilibrium through experience from repeated interactions over time. Learning here is understood in the broadest sense possible. It could be through imitation, trial and error, or even (cultural or genetic) evolution (Hoffman & Yoeli, 2022). However, we will not explicitly model the learning process in this lecture and the whole part on target equilibria. This will be the main topic of the last part of this course.\n\n\nMovie time\nThe movie A Beautiful Mind portrays John Nash, the inventor of the Nash equilibrium. The Bar Scene is the moment in the film where Nash experiences the revelation of his equilibrium concept.\n\n\n\n        \n        \n\n\nIs the solution of the game Nash advocates for in the clip a Nash equilibrium?\n\n\n\nSocial dilemma\nIn fact, the bar scene from A Beautiful Mind is a good example of a social dilemma.\nA social dilemma is a situation where all actors have an incentive to behave selfishly.\nHowever, everyone would be better off if everyone would behave cooperatively.\nThe game of abating and polluting is also a social dilemma for some values of the parameters \\(b\\) and \\(c\\).\n\nplot_payoffs(N=3, b=0.55, c=1.5)\n\n\n\n\n\n\n\n\nAs long as the cost of abating is greater than the benefit, \\(b&lt;c\\), the unique Nash equilibrium is that all actors pollute.\nHowever, when the benefit of abating times the number of actors is higher than the cost \\(c&lt;bN\\), all actors would be better off if all actors abate.\nWhen both conditions are met, \\(b&lt;c&lt;bN\\), the game is a social dilemma.\nNote, when \\(c&gt;bN\\), all actors polluting is the unique Nash equilibrium and the social optimum. Everyone is better off when all actors pollute.\nThis simple model helps to explain why situations with many actors \\(N\\) can be more prone to be social dilemmas. When abating benefits everyone, independent of how many actors are involved, having many actors makes the situation likely to be a social dilemma. Independent of how large the cost \\(c\\) may be, we simply have to increase \\(N\\), such that \\(c&lt;bN\\). The condition that the benefits \\(b\\) are independent of \\(N\\) refers to the public good nature of the benefits. The benefits of having an intact and healthy planet with maintenance and regulating ecosystem services serve everyone, regardless of how many. They are so-called non-rivalrous. Other services of Nature are rivalrous, e.g., the fish in the ocean.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strategic Interactions</span>"
    ]
  },
  {
    "objectID": "03.02-StrategicInteractions.html#dimensions-of-a-social-dilemma",
    "href": "03.02-StrategicInteractions.html#dimensions-of-a-social-dilemma",
    "title": "7  Strategic Interactions",
    "section": "7.3 Dimensions of a social dilemma",
    "text": "7.3 Dimensions of a social dilemma\nLet us dissect a social dilemma along two dimensions:\n\nthe greed \\(G\\) to exploit others, and\nthe fear \\(F\\) of being exploited by others.\n\nFor two actors that must face the decision between abate or pollute, we can summarize the payoffs in a matrix, \\[\n\\begin{array}{c|cc}\n\\text{} & \\color{blue}{\\mathsf{Abate}} & \\color{blue}{\\mathsf{Pollute}} \\\\\n\\hline\n\\color{red}{\\mathsf{Abate}} & {\\color{red}{1}} \\ | \\ {\\color{blue}{1}} & {\\color{red}{-1-F}} \\ | \\ {\\color{blue}{+1+G}} \\\\\n\\color{red}{\\mathsf{Pollute}} & {\\color{red}{+1+G}} \\ | \\ {\\color{blue}{-1-F}} & {\\color{red}{-1}} \\ | \\ {\\color{blue}{-1}} \\\\\n\\end{array}\n\\].\nDepending on whether the greed \\(G\\) and fear \\(F\\) are positive or negative, we can distinguish four types of games Figure 10.3.\n\n\n\n\n\n\nFigure 7.1: Dimensions of a social dilemma with ordinal payoffs and Nash equilibira shown in boxes.\n\n\n\nIn Figure 10.3, the payoff values are ordinal, meaning that only their order, \\(3&gt;2&gt;1&gt;0\\), is considered of relevance.\n\nCase 1 | Tragedy (\\(G&gt;0, F&gt;0\\)).\nWhen actors are greedy to exploit others and fear being exploited by others, the game is a tragedy. Regardless of what others do, each agent has an incentive to pollute. Thus, the Nash equilibrium is that all actors pollute. The tragedy is that all actors would be better off if all actors abate. Another common name for this situation is the Prisoner’s dilemma.\n\n\nCase 2 | Divergence (\\(G&gt;0, F&lt;0\\)).\nWhen actors are greedy to exploit others but do not fear being exploited by others, the actors are in a situation of divergence. When enough other actors pollute, individual incentives regard abate better than pollute. Thus, in the two-actor case, both (abate, pollute) and (pollute, abate) are Nash equilibria, where the polluting actor receives more than the abating one (as long as \\(F&gt;-(G+2)\\)). This is a situation of inequality emerging despite both actors being identical. It also induces a first-mover advantage, as the first actor to choose pollute will receive more reward than the abating actor. We call this situation divergence since the collective remains divided and only partial sustainability is achieved. Other popular names are chicken, hawk-dove, snow-drift describing different stories around the situation.\n\n\nCase 3 | Coordination (\\(G&lt;0, F&gt;0\\)).\nWhen actors are not greedy to exploit others but fear being exploited by others, they are in a situation of coordination. What is better for an individual mirrors what the others do. Thus, both (abate, abate) and (pollute, pollute) are Nash equilibria. In both equilibria, both agents are equally well off, but it depends on which of the two equilibria the actors coordinate. The agents are better off in (abate, abate) than in (pollute, pollute). However, coordination may still be difficult to achieve, e.g., because of anonymity, a lack of communication, or false beliefs. Nevertheless, turning a tragedy into a coordination game is a common mechanism to resolve the social dilemma. Another popular name for this situation is the stag-hunt game.\n\n\nCase 4 | Comedy (\\(G&lt;0, F&lt;0\\)).\nWhen there is neither greed to exploit others nor fear to be exploited by others, the actors are in a situation of comedy. Regardless of what others do, each agent has an incentive to abate. Thus, the joint strategy (abate, abate) is the only Nash equilibrium. Since individual and collective interests point to the same solution, we call this the comedy of the commons (Ostrom et al., 2002). The Harmony game is another common name for this situation.\n\n\nLimitations\nWe assumed that the actors were anonymous. However, actors are often not anonymous, especially in the governance of local commons. They know each other and can communicate and reciprocate (Anderies & Janssen, 2016; Nowak, 2006; Ostrom et al., 2002). This can help overcome the social dilemma. We will discuss and model this in the last part of the course.\nWe also did not discuss any mechanisms that let one or more of these games (or incentive regimes) emerge. We will discuss two such broad mechanisms in the remainder of this chapter: international agreements and threshold public goods.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strategic Interactions</span>"
    ]
  },
  {
    "objectID": "03.02-StrategicInteractions.html#international-agreements",
    "href": "03.02-StrategicInteractions.html#international-agreements",
    "title": "7  Strategic Interactions",
    "section": "7.4 International Agreements",
    "text": "7.4 International Agreements\nLet us revisit our climate commons dilemma from above. Each abating actor brings a benefit \\(b=100\\) EUR (of averted damages) to all actors at an individual cost \\(c=250\\) EUR. One way to resolve this social dilemma could be through an agreement. The actors could agree to abate. In the following, we will model this additional game layer, highlighting its potential and limitations.\n\nLet’s play\nWe assume that an agreement has already been negotiated. If you sign the agreement, you must choose abate. However, the agreement comes only into force if there are at least three signatories.\nYou gain 100 Euros for each person who chooses abate. If you choose abate, you have to pay 250 Euros\nYou have to make three choices:\n\nwhether you sign or not sign the agreement\nwhat you choose if there are not enough signatories: abate or pollute\nwhat you choose if you did not sign the agreement: abate or pollute\n\n\n\n\n        \n        \n\n\n\n\nAgreement participation game\nGenerally, in this model, the agreement mandates that all signatories abate if at least \\(k^*\\) actors sign the agreement. Then, at\n\nStage 1: Every actor chooses whether or not to sign the agreement. At\nStage 2: The signatories choose jointly whether to abate or pollute. Finally, at\nStage 3: The non-signatories choose independently whether to abate or pollute.\n\n\n\nSelf-enforcing agreements\nAn international environmental agreement (IEA) must be self-enforcing, i.e., a Nash equilibrium in the game-theoretic sense, as there are no global enforcement mechanisms.\n\nNo signatory can gain by withdrawing unilaterally\nNo non-signatory can gain by joining\nThus, there is no incentive to re-negotiate\n\n\n\nCritical participation level \\(k^*\\)\nThe crucial question is, what is the critical participation level \\(k^*\\), such that the agreement is self-enforcing?\nSuppose there are \\(k\\) signatories.\n\nIn a tragedy dilemma, non-signatories will defect in Stage 3.\nSignatories have an incentive to abate if \\(kb-c \\geq 0\\).\nRearranging yields: if \\(k \\geq c/b\\), signatories abate.\nLet \\(k^0\\) be the smallest integer greater than or equal to \\(c/b\\).\nSuppose there are \\(k^0\\) signatories.\nNo non-signatory would want to join the agreement.\nThus, the critical participation level is\n\n\\[\\frac{c}{b}+1 \\geq k^* \\geq \\frac{c}{b}\\]\n\ndef plot_agreement_payoffs(N, b, c, ax=None):\n    kstar = int(np.ceil(c/b)); print(kstar)\n\n    Ns_other = np.arange(0, N)\n    bS = ((Ns_other+1)*b - c &lt; 0)*0 + ((Ns_other+1)*b - c &gt;= 0)*((np.arange(N)+1)*b - c)\n    bN = ((Ns_other)*b - c &lt; 0)*0 + ((Ns_other)*b - c &gt;= 0)*np.arange(N)*b\n\n    _, a = plt.subplots() if ax is None else (None, ax)\n    a.plot(Ns_other, bS, '.-', label='Signatories')\n    a.plot(Ns_other, bN, '.-', label='Non-signatories')\n    a.legend(); a.set_xlabel('Number of other signatories'); a.set_ylabel('Payoff')\n\n\n\nAgreements turn tragedy into divergence\nFull participation in an agreement is difficult (only possible if costs are astronomical). This holds for many generalizations (e.g., non-linear payoff functions/cost functions).\n\nfig, axs = plt.subplots(1,2, figsize=(11,3))\nplot_agreement_payoffs(8, 1, 3.5, ax=axs[0]); axs[0].set_ylim(-3,8)\nplot_payoffs(8, 1, 3.5, ax=axs[1]); axs[1].set_ylim(-3,8);\n\n4",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strategic Interactions</span>"
    ]
  },
  {
    "objectID": "03.02-StrategicInteractions.html#threshold-public-goods",
    "href": "03.02-StrategicInteractions.html#threshold-public-goods",
    "title": "7  Strategic Interactions",
    "section": "7.5 Threshold Public Goods",
    "text": "7.5 Threshold Public Goods\nIs the tragedy dilemma the game we are playing?\n\n\n\n\n\n\nFigure 7.2: Climate tipping risks\n\n\n\n\nLet’s play again\n\nYou are given two choices: abate climate change or continue to pollute the atmosphere.\nYou gain 100 Euros (of averated damages) for each person that chooses abate.\nIf you choose abate, you must pay 250 Euros.\nIf not all choose to abate, the climate will tip, and everybody will lose 350 Euros.\n\nWhat would you choose?\n\n\n\n        \n        \n\n\n\n\nThreshold dilemma game\nIn general, we can model a threshold public goods game as follows:\n\nThere are \\(N\\)-actors.\nEach actor can contribute an amount \\(c\\) (by abating) to the public good, or they contribute nothing and pollute.\nEach contributed unit brings a benefit \\(b_u\\) (of averted linear damages) to all actors.\nIf the collective does not contribute at least a critical threshold amount \\(T_\\text{crit}\\), all actors experience a catastrophic impact \\(m\\) of non-linear tipping damages.\n\nHow does the threshold dilemma map onto the tragedy dilemma? For simplicity, let’s assume actors have two actions:\n\nContributing a fair amount to avert the collapse, \\(c=T_\\text{crit}/N\\)\nContributing nothing.\n\nThe unit benefit \\(b_u\\) relates to a benefit \\(b\\) from abating from the tragedy dilemma by \\[b=b_u c = b_u T_\\text{crit}/N\\]\n\ndef plot_threshold_payoff(N, bu, Tc, m, ax=None):\n    c=Tc/N; b=bu*c\n    \n    Na_other = np.arange(0, N)\n    bA = (Na_other+1)*b - c - m*(Na_other+1&lt;N)\n    bP = Na_other*b - m*(Na_other&lt;N)\n\n    _, a = plt.subplots() if ax is None else (None, ax)\n    a.plot(Na_other, bA, '.-', label='Abate')\n    a.plot(Na_other, bP, '.-', label='Pollute')\n    a.legend(); a.set_xlabel('Number of other actors abating'); a.set_ylabel('Payoff')\n\nContribution thresholds turn tragedy into coordination challenge (Figure 7.3).\n\nplot_threshold_payoff(5, 0.5, 50, 6);\n\n\n\n\n\n\n\nFigure 7.3: Payoffs from the thresholds game.\n\n\n\n\n\nNations are good at solving coordination challenges via international conferences and agreements. The Montreal Protocol on Substances that Deplete the Ozone Layer is the landmark multilateral environmental agreement that regulates the production and consumption of nearly 100 artificial chemicals referred to as ozone-depleting substances (ODS). When released into the atmosphere, those chemicals damage the stratospheric ozone layer, Earth’s protective shield that protects humans and the environment from harmful levels of ultraviolet radiation from the sun. Adopted on 16 September 1987, the Protocol is, to date, one of the rare treaties to achieve universal ratification. [Source | UNEP]\n\nConditions for coordination\nUnder what conditions on the parameters does a threshold turn a tragedy dilemma into a coordination challenge?\nThe benefit of a cooperating actor choosing to abate when all others choose to abate is \\[R^i(\\mathsf{A, A}) = N b_u \\frac{T_c}{N} - \\frac{T_c}{N}  \\]\n\ndef bAA(N, bu, Tc, m): return N*bu*Tc/N - Tc/N\n\nThe benefit of a polluter, when all others choose abate is \\[R^i(\\mathsf{P, A}) = (N-1)b_u \\frac{T_c}{N} - m\\]\n\ndef bPA(N, bu, Tc, m): return (N-1)*bu*Tc/N - m\n\nWhen all other actors pollute, there is no incentive to abate (because the collapse happens anyway). But when all other actors abate, there is an incentive to abate if\n\\[R^i(\\mathsf{A,A}) &gt; R^i(\\mathsf{P, A}).\\]\nRearranging yields, \\[m^* = \\frac{1-b_u}{N} T^*_c.\\]\n\n# Parameters\nbu=0.5; N = 5\n\n# Varying parameters  \nms = np.linspace(0, 10, 201); \nTs = np.linspace(0, 100, 251);\nTT, MM = np.meshgrid(Ts, ms)\n\n# Get the plot nice\ncmap = plt.colormaps['PiYG'];\nnorm = BoundaryNorm([0,0.5,1], ncolors=cmap.N, clip=True)\n\n# do the plot\ncb = plt.pcolormesh(TT, MM, bAA(N, bu, TT, MM)&gt;bPA(N, bu, TT, MM), cmap=cmap, norm=norm)\n\n# make the plot nice (again)\ncbar = plt.colorbar(cb, ticks=[0.25, 0.75]); \ncbar.ax.set_yticklabels(['Tragedy', 'Coordination'])\n\nplt.xlabel('Threshold'); plt.ylabel('Impact');\nplt.plot(Ts, Ts/N*(1-bu));\n\n\n\n\n\n\n\n\n\n\n\nUncertainty\nWhat if there is uncertainty? We will consider two kinds of uncertainty:\n\nImpact uncertainty: We don’t know exactly how bad it is going to be\nThreshold uncertainty: We don’t know exactly where the threshold lies\n\n\nImpact uncertainty\nAssume the impact \\(m\\) is uncertain.\nThe impact \\(m\\) is distributed according to a probability distribution.\nExpected value theory underlying game theory suggests that decision-makers will only care about the expected value.\nThus, as long as \\(m_\\text{certain} = \\mathbb E[m_\\text{uncertain}]\\), the model remains unchanged.\nHere, we consider a very simple distribution of \\(Pr(m)\\). There is a \\(1/3\\) chance of a small impact \\(m_\\text{certain} - \\Delta m\\), a \\(1/3\\) chance of a medium impact \\(m=m_\\text{certain}\\), and a \\(1/3\\) chance of a large impact \\(m=m_\\text{certain} + \\Delta m\\). Obviously, the expected value is \\(m_\\text{certain}\\).\n\ndef plot_uncertainimpacts_payoffs(N, bu, Tc, m, delta_m=2, ax=None):\n    c=Tc/N; b=bu*c;\n    \n    Na_other_fair = np.arange(0, N)\n\n    bPl = Na_other_fair*b - (m-delta_m)*(Na_other_fair&lt;N)\n    bPc = Na_other_fair*b - m*(Na_other_fair&lt;N)\n    bPh = Na_other_fair*b - (m+delta_m)*(Na_other_fair&lt;N)\n    \n    bAl = (Na_other_fair+1)*b - c - (m-2)*(Na_other_fair+1&lt;N)\n    bAc = (Na_other_fair+1)*b - c - m*(Na_other_fair+1&lt;N)\n    bAh = (Na_other_fair+1)*b - c - (m+2)*(Na_other_fair+1&lt;N)\n    \n    bP = np.mean([bPl, bPc, bPh], axis=0)\n    bA = np.mean([bAl, bAc, bAh], axis=0)\n        \n    _, a = plt.subplots() if ax is None else (None, ax)\n\n    col1 = plt.rcParams['axes.prop_cycle'].by_key()['color'][0] \n    col2 = plt.rcParams['axes.prop_cycle'].by_key()['color'][1]\n     \n    a.plot(Na_other_fair, bAc, '.-', color=col1, alpha=0.5)\n    a.plot(Na_other_fair, bAl, '.--', color=col1, alpha=0.5)\n    a.plot(Na_other_fair, bAh, '.-.', color=col1, alpha=0.5)\n    \n    a.plot(Na_other_fair, bPc, '.-', color=col2, alpha=0.5)\n    a.plot(Na_other_fair, bPl, '.--', color=col2, alpha=0.5)\n    a.plot(Na_other_fair, bPh, '.-.', color=col2, alpha=0.5)\n\n    a.plot(Na_other_fair, bA, '.-', label='Average abate', color=col1)\n    a.plot(Na_other_fair, bP, '.-', label='Average pollute', color=col2)\n\n    a.set_xlabel('Number of other actors abating'); a.set_ylabel('Payoffs')\n    a.legend()\n\n\nplot_uncertainimpacts_payoffs(N=5, bu=0.5, Tc=50, m=6, delta_m=4)\n\n\n\n\n\n\n\n\nThus, impact uncertainty does not change the outcome of our model.\n\n\nThreshold uncertainty\nEmpirically, there is considerable uncertainty about the critical threshold \\(T_\\text{crit}\\) (Figure 7.2).\nSuppose, the threshold \\(T_\\text{crit}\\) is uncertain:\n\nwith \\(p=1/3\\) it is at \\(T_\\text{crit,low} = T_\\text{crit,certain} - 10\\)\nwith \\(p=1/3\\) it is at \\(T_\\text{crit,mid} = T_\\text{crit,certain}\\)\nwith \\(p=1/3\\) it is at \\(T_\\text{crit,high} = T_\\text{crit,certain} + 10\\)\n\nThus, expected value of the threshold remains the same.\n\ndef plot_uncertainthresholds_payoff(N, bu, Tc, m, ax=None):\n    c=Tc/N; b=bu*c\n    \n    Na_other_fair = np.arange(0, N)\n\n    bD = Na_other_fair*b - m*(Na_other_fair&lt;N)\n    \n    bC = (Na_other_fair+1)*b - c - m*(Na_other_fair+1&lt;N)\n    bL = (Na_other_fair+1)*b - c - m*(Na_other_fair+1&lt;N)    \n    bH = (Na_other_fair+1)*b - c - m*(Na_other_fair&lt;N)    \n    \n    bA = np.mean([bC, bL, bH], axis=0)\n\n    _, a = plt.subplots() if ax is None else (None, ax)\n    a.plot(Na_other_fair, bA, '.-', label='Average abate')\n    a.plot(Na_other_fair, bD, '.-', label='Pollute')\n    a.plot(Na_other_fair, bC, '.-', label='Abate - Center threshold', color='grey')\n    a.plot(Na_other_fair, bL, '.--', label='Abate - Low threshold', color='black')\n    a.plot(Na_other_fair, bH, '.-.', label='Abate - High threshold', color='lightgrey')\n    \n    a.legend(); a.set_xlabel('Number of other actors abating'); a.set_ylabel('Payoff')\n\nThreshold uncertainty reverts the coordination challenge back to a tragedy\n\nplot_uncertainthresholds_payoff(N=5, bu=0.5, Tc=50, m=6)\n\n\n\n\n\n\n\n\nThreshold uncertainty raises the cost of averting collapse. Essentially, this is because low thresholds cannot compensate for high thresholds. High thresholds cause collapse, making actors worse off than those with medium thresholds. However, low thresholds cannot make actors better off than medium thresholds.\n\n\nThreshold dilemmas summary\nContribution thresholds can turn tragedy into a coordination challenge - given a sufficiently severe collapse impact and sufficiently low threshold.\nUncertainty is important: Impact uncertainty has no effect - But threshold uncertainty can revert the coordination challenge back to tragedy.\n\n\nExperimental evidence\nOur model allows us to make predictions about how actors behave in threshold public goods games across the four (two by two) treatments of uncertainty:\n\nImpact uncertainty has no effect. Certainty and Impact uncertainty should yield the same behavior (abating), just as Threshold uncertainty and Threshold+Impact uncertainty should yield the same behavior (polluting).\nThreshold uncertainty has an effect. The certainty and impact uncertainty treatment should yield a different behavior than the treatments with threshold uncertainty.\n\nBoth hypotheses are confirmed by the experimental evidence (Figure 7.4) (Barrett & Dannenberg, 2012).\n\n\n\n\n\n\nFigure 7.4: Experimental evidence on threshold public goods games",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strategic Interactions</span>"
    ]
  },
  {
    "objectID": "03.02-StrategicInteractions.html#learning-goals-revisited",
    "href": "03.02-StrategicInteractions.html#learning-goals-revisited",
    "title": "7  Strategic Interactions",
    "section": "7.6 Learning goals revisited",
    "text": "7.6 Learning goals revisited\nIn this chapter,\nwe applied game theory to model multi-agent action situations.\nWe resolved games by defining and finding Nash equilibria.\nWe described and analyzed the dimensions of a social dilemma.\nWe introduced and analyzed two special kinds of games: agreement games and threshold public goods.\n\nAgreement games can turn tragedies into divergence dilemmas.\nThreshold public goods can turn tragedies into coordination challenges - given enough certainty about where the threshold lies.\n\n\nOverall limitations\nOur equilibrium game-theoretic model left it unclear where the strategies come from or from which process they arise.\nThe models in this chapter did not explicitly consider environmental dynamics.\nThe consequences for the actors were assumed to be experienced immediately.\n\n\nBibliographic remarks\nThe fear and greed dimensions of a social dilemma are inspired by (Macy & Flache, 2002).\nThe name of the four social dilemma types is inspired by (Ostrom et al., 2002), who talks about the drama of the commons. Commons may sometimes be a tragedy, sometimes a comedy, often something in between.\nInternational environmental agreement games are coined by (Barrett, 1994, 2005).\n\n\n\n\nAnderies, J. M., & Janssen, M. A. (2016). Sustaining the commons. Independent. https://dlc.dlib.indiana.edu/dlc/handle/10535/8839\n\n\nBarrett, S. (1994). Self-Enforcing International Environmental Agreements. Oxford Economic Papers, 46(Supplement_1), 878–894. https://doi.org/10.1093/oep/46.Supplement_1.878\n\n\nBarrett, S. (2005). Environment and Statecraft: The Strategy of Environmental Treaty-Making. Oxford University Press. https://doi.org/10.1093/0199286094.001.0001\n\n\nBarrett, S., & Dannenberg, A. (2012). Climate negotiations under scientific uncertainty. Proceedings of the National Academy of Sciences, 109(43), 17372–17376. https://doi.org/10.1073/pnas.1208417109\n\n\nHoffman, M., & Yoeli, E. (2022). Hidden Games: The Surprising Power of Game Theory to Explain Irrational Human Behaviour. Hachette UK.\n\n\nMacy, M. W., & Flache, A. (2002). Learning dynamics in social dilemmas. Proceedings of the National Academy of Sciences, 99(suppl_3), 7229–7236. https://doi.org/10.1073/pnas.092080099\n\n\nNowak, M. A. (2006). Five Rules for the Evolution of Cooperation. Science. https://doi.org/10.1126/science.1133755\n\n\nOstrom, E., Dietz, T., Dolšak, N., Stern, P. C., Stonich, S., & Weber, E. U. (Eds.). (2002). The drama of the commons. National Academies Press. http://www.nap.edu/catalog/10287",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Strategic Interactions</span>"
    ]
  },
  {
    "objectID": "03.03-DynamicInteractions.html",
    "href": "03.03-DynamicInteractions.html",
    "title": "8  Dynamic Interactions",
    "section": "",
    "text": "8.1 Motivation | Futures and environments\nPrototypical models did not explicitly consider future environmental consequences of strategic interactions (Figure 8.1).\nHowever, many real-world scenarios involve strategic interactions with environmental consequences. For example, the tragedy of the commons, agreements, and threshold public goods can be extended to include ecological consequences. In this lecture, we will introduce dynamic games, particularly stochastic or Markov games, to model strategic interactions with environmental consequences.\nStochastic games integrate Markov chains, Markov decision processes, and game theory to model strategic interactions in dynamic environments. They are particularly useful for modeling human-environment interactions, where the environment is affected by human actions and, in turn, affects human behavior.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic Interactions</span>"
    ]
  },
  {
    "objectID": "03.03-DynamicInteractions.html#motivation-futures-and-environments",
    "href": "03.03-DynamicInteractions.html#motivation-futures-and-environments",
    "title": "8  Dynamic Interactions",
    "section": "",
    "text": "Figure 8.1: Environmental dynamics are relevant to consider\n\n\n\n\n\n\nAdvantages of dynamic games\nUsing dynamic games, particularly stochastic games to model strategic interactions with environmental consequences has several advantages:\n\ninherently stochastic - to account for uncertainty\nnonlinear - to account for structural changes\nagency - to account for human behavior\ninteractions - to account for strategic interactions\nfuture-looking - to account for the trade-off between short-term and long-term\nfeedback - between multiple agents and the environment\n\nStochastic games also have structural benefits that make them compatible with numerical computer modeling due to their discrete action and state sets, as well as their advancement in discrete time intervals.\n\n\nLearning goals\nAfter this lecture, students will be able to:\n\nDescribe the elements of a stochastic game\nApply stochastic games to model human-environment interactions\nAnalyze the strategic interactions in stochastic games\n\n\nimport sympy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; plt.rcParams['grid.linewidth'] = 0.25;",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic Interactions</span>"
    ]
  },
  {
    "objectID": "03.03-DynamicInteractions.html#dynamic-games-strategic-interactions-with-environmental-consequences",
    "href": "03.03-DynamicInteractions.html#dynamic-games-strategic-interactions-with-environmental-consequences",
    "title": "8  Dynamic Interactions",
    "section": "8.2 Dynamic games | Strategic interactions with environmental consequences",
    "text": "8.2 Dynamic games | Strategic interactions with environmental consequences\n\n\n\n\n\n\nFigure 8.2: Multiagent-Environment Interface\n\n\n\nHere, we focus on environments with a discrete state set in discrete time. These specifications are commonly called stochastic or Markov games. They consist of the following elements:\n\nA discrete set of environmental contexts or states \\(\\mathcal S = \\{S_1, \\dots, S_Z\\}\\).\n\nWe denote an environmental state by \\(s \\in \\mathcal S\\).\n\nA finate set of \\(N\\) agents \\(\\mathcal I = \\{2,\\dots, N\\}\\) participating in an interaction.\nFor each agent \\(i\\in\\mathcal I\\), a discrete set of options or actions \\(\\mathcal A^i = \\{A^i_1, \\dots, A^i_M\\}\\).\n\nLet’s denote the joint action set by \\(\\mathbf{\\mathcal A} = \\mathcal A^1 \\times \\dots \\times A^N\\).\nAn action profile \\(\\mathbf a = (a^1,\\dots,a^N) \\in \\mathbf{\\mathcal A}\\) is a joint action of all agents.\n\n\nTime \\(t\\) advances in discrete steps \\(t=0,1,2,\\dots\\), and agents choose their actions simultaneously.\n\nWe denote the state at time \\(t\\) by \\(s_t\\) and the joint action by \\(\\mathbf a_t\\).\n\nThe transitions tensor \\(T: \\mathcal S \\times \\mathbf{\\mathcal A} \\times \\mathcal S \\to [0,1]\\) defines the environmental dynamics.\n\n\\(T(s, \\mathbf a, s')\\) is the probability of transitioning from state \\(s\\in\\mathcal S\\) to \\(s'\\in\\mathcal S\\) given the joint action \\(\\mathbf a\\).\nThus, \\(\\sum_{s'} T(s, \\mathbf a, s') = 1\\) most hold for all \\(s\\in\\mathcal S\\) and \\(\\mathbf a\\in\\mathbf{\\mathcal A}\\).\n\nThe reward tensor \\(\\mathbf R: \\mathcal I \\times \\mathcal S \\times \\mathbf{\\mathcal A} \\times \\mathcal S \\to \\mathbb R\\) defines the agents’ short-term welfare, utility, rewards, or payoffs.\n\n\\(R^i(s, \\mathbf a, s')\\) is the reward agent \\(i\\) receives for transitioning from state \\(s\\) to \\(s'\\) given the joint action \\(\\mathbf a\\).\nWe assume that it is each agent \\(i\\)’s goal to maximize their expected discounted sum of future rewards, \\(G^i = \\sum_{t=0}^\\infty (\\gamma^i)^t R^i(s_t, \\mathbf a_t, s_{t+1})\\), where \\(\\gamma^i\\in[0,1)\\) is the discount factor.\n\nWe assume that agents can condition their probabilities of choosing actions on the current state \\(s_t\\), yielding so-called Markov policies or strategies, \\(\\mathbf x: \\mathcal I \\times \\mathcal S  \\times \\mathcal A^i \\to [0, 1]\\).\n\n\\(x^i(s, a)\\) is the probability agent \\(i\\) chooses action \\(a\\) in state \\(s\\).",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic Interactions</span>"
    ]
  },
  {
    "objectID": "03.03-DynamicInteractions.html#application-ecological-public-good",
    "href": "03.03-DynamicInteractions.html#application-ecological-public-good",
    "title": "8  Dynamic Interactions",
    "section": "8.3 Application | Ecological public good",
    "text": "8.3 Application | Ecological public good\nWe apply the stochastic game framework to integrate the fact that we are embedded in a shared environment and care about the future to some extent. We do so by considering a public good game with ecological consequences (Figure 8.3), which allows us to answer how the strategic incentives depend on their level of care for future rewards.\n\n\n\n\n\n\nFigure 8.3: Ecological public good collective decision-making environment\n\n\n\n\nStates, agents and actions\nThe environment consists of two states, \\(\\mathcal S = \\{\\textsf{p}, \\textsf{d}\\}\\), representing a prosperous and a degraded state of the environment.\n\nstate_set = ['prosperous,' 'degraded']; p=0; g=1; Z=2\n\nWe also defined two Python variable p=0 and g=1 to serves as readable and memorable indices to represent the environmental contexts.\nThere are \\(2\\) identical agents. In each state \\(s \\in \\mathcal S\\), each agent \\(i \\in \\{1, 2\\}\\) can choose within their action set between either cooperation or defection, \\(\\mathcal A^i = \\{\\mathsf{c,d}\\}\\).\n\naction_sets = ['cooperate', 'defect']; c=0; d=1; M=2\n\nLikewise, we define two Python variables, c=0 and d=1, to serve as readable and memorable indices to represent the agents’ actions.\nWe denote the number of cooperating agents by \\(N_c\\). The number of defecting agents is \\(N_d = N - N_c\\).\n\n\nTransitions | Environmental dynamics\nWe represent the environmental dynamics, i.e., the transitions between environmental state contexts, in a \\(Z \\times M \\times M \\times Z\\) tensor, where \\(Z\\) is the number of states and \\(M\\) is the number of actions. In this representation,\n\nthe first dimension corresponds to the current state,\nthe second to the action profile of the first agent,\nthe third to the action profile of the other agent, and\nthe fourth and last dimension corresponds to the next state.\n\n\nTransitionTensor = np.zeros((Z, M, M, Z), dtype=object)\n\nThe environmental dynamics are then governed by two parameters: a collapse leverage, \\(q_c\\), and a recovery probability, \\(p_r\\).\n\nqc, pr = sp.symbols('q_c p_r')\n\nA collapse from the prosperous to the degraded state occurs with a transition probability\n\\[T(\\mathsf p, \\mathbf a, \\mathsf g) = \\frac{N_d}{N}  q_c. \\]\n\nTransitionTensor[p, c, c, g] = 0  # no agent defects\nTransitionTensor[p, d, c, g] = qc/2  # one agent defects\nTransitionTensor[p, c, d, g] = qc/2  # other agent defects\nTransitionTensor[p, d, d, g] = qc  # all agents defect\n\nThus, if all agents defect, the environment collapses with probability \\(q_c\\). The collapse leverage indicates how much impact a defecting agent exerts on the environment. The environment remains within the prosperous state with probability, \\(T(\\mathsf p, \\mathbf a, \\mathsf p) = 1 - \\frac{N_d}{N}  q_c\\).\n\nTransitionTensor[p, : , :, p] = 1 - TransitionTensor[p, : , :, g]\n\nIn the degraded state, we set the recovery to occur with probability,\n\\[T(\\mathsf g, \\mathbf a, \\mathsf p) = p_r,\\]\nindependent of the agents’ actions.\n\nTransitionTensor[g, :, :, p] = pr\n\nThe probability that the environment remains in the degraded state is then, \\(T(\\mathsf g, \\mathbf a, \\mathsf g) = 1 - p_r\\).\n\nTransitionTensor[g, :, :, g] = 1-pr\n\nTogether, our transition tensor is then given by\n\nsp.Array(TransitionTensor)\n\n\\(\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}1 & 0\\\\1 - \\frac{q_{c}}{2} & \\frac{q_{c}}{2}\\end{matrix}\\right] & \\left[\\begin{matrix}1 - \\frac{q_{c}}{2} & \\frac{q_{c}}{2}\\\\1 - q_{c} & q_{c}\\end{matrix}\\right]\\\\\\left[\\begin{matrix}p_{r} & 1 - p_{r}\\\\p_{r} & 1 - p_{r}\\end{matrix}\\right] & \\left[\\begin{matrix}p_{r} & 1 - p_{r}\\\\p_{r} & 1 - p_{r}\\end{matrix}\\right]\\end{matrix}\\right]\\)\n\n\nLast, we make sure that our transition tensor is normalized, i.e., the sum of all transition probabilities from a state-joint-action pair to all possible next states equals one, \\(\\sum_{s'} T(s, \\mathbf a, s') = 1\\).\n\nTransitionTensor.sum(-1)\n\narray([[[1, 1],\n        [1, 1]],\n\n       [[1, 1],\n        [1, 1]]], dtype=object)\n\n\n\n\nRewards | Short-term welfare\n\nRewardTensor = np.zeros((2, Z, M, M, Z), dtype=object)\n\nRewards in the prosperous state follow the standard public goods game with a synergy factor \\(r\\) and a cost of cooperation \\(c\\).\n\nr, cost = sp.symbols('r c')\n\nThe rewards for cooperating and defecting agents are then given by\n\\[ R^i(\\mathsf p, a^i, a^{-i}, \\mathsf p) = \\begin{cases} \\frac{r c (N_c +1)}{N}  - c, & \\text{if } a^i = \\mathsf c, \\\\ \\frac{r c N_c}{N}, & \\text{if } a^i = \\mathsf d. \\end{cases} \\]\n\nRewardTensor[:, p, c, c, p] = r*cost - cost\nRewardTensor[:, p, d, d, p] = 0\nRewardTensor[0, p, c, d, p] = RewardTensor[1, p, d, c, p] = r*cost/2 - cost\nRewardTensor[0, p, d, c, p] = RewardTensor[1, p, c, d, p] = r*cost/2\n\nWhen a state transition involves the degraded state, \\(\\mathsf{g}\\), the agents only receive an environmental collapse impact \\(m\\):\n\nm = sp.symbols('m')\n\n\\[R^i(\\mathsf p, \\mathbf a, \\mathsf g) = R^i(\\mathsf g, \\mathbf a, \\mathsf g) = R^i(\\mathsf g, \\mathbf a, \\mathsf p) = m, \\quad \\text{for all} \\ \\mathbf a, i.\\]\n\nRewardTensor[:, p, :, :, g] = RewardTensor[:, g, :, :, g] = RewardTensor[:, g, :, :, p] = m \n\nTogether, our reward tensor is then given by\n\nsp.Array(RewardTensor)\n\n\\(\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}\\left[\\begin{matrix}c r - c & m\\\\\\frac{c r}{2} - c & m\\end{matrix}\\right] & \\left[\\begin{matrix}\\frac{c r}{2} & m\\\\0 & m\\end{matrix}\\right]\\\\\\left[\\begin{matrix}m & m\\\\m & m\\end{matrix}\\right] & \\left[\\begin{matrix}m & m\\\\m & m\\end{matrix}\\right]\\end{matrix}\\right] & \\left[\\begin{matrix}\\left[\\begin{matrix}c r - c & m\\\\\\frac{c r}{2} & m\\end{matrix}\\right] & \\left[\\begin{matrix}\\frac{c r}{2} - c & m\\\\0 & m\\end{matrix}\\right]\\\\\\left[\\begin{matrix}m & m\\\\m & m\\end{matrix}\\right] & \\left[\\begin{matrix}m & m\\\\m & m\\end{matrix}\\right]\\end{matrix}\\right]\\end{matrix}\\right]\\)\n\n\n\n\nDeepDive | Subsituting parameter values\nIn this chapter, we defined the transition and reward tensors as general numpy arrays with data types object, which we filled with symbolic expressions from sympy. To manipulate and substitute these expressions, we can use the sympy.subs method, however, not directly on the numpy array. Instead, we define a helper function substitute_in_array that takes a numpy array and a dictionary of substitutions and returns a new array with the substitutions applied.\n\ndef substitute_in_array(array, subs_dict):\n    result = array.copy()\n    for index,_ in np.ndenumerate(array):\n        if isinstance(array[index], sp.Basic):\n            result[index] = array[index].subs(subs_dict)\n    return result\n\nTo make this work, it seems to be of critical importance that the subsitution dictionary is given as a dictionary in the form of {&lt;symbol_variable&gt;: &lt;subsitution&gt;, ...} and not as dict(&lt;symbol_variable&gt;=&lt;subsitution&gt;, ...). For example,\n\nsubstitute_in_array(TransitionTensor, {pr:0.01, qc:0.2}).astype(float)\n\narray([[[[1.  , 0.  ],\n         [0.9 , 0.1 ]],\n\n        [[0.9 , 0.1 ],\n         [0.8 , 0.2 ]]],\n\n\n       [[[0.01, 0.99],\n         [0.01, 0.99]],\n\n        [[0.01, 0.99],\n         [0.01, 0.99]]]])\n\n\n\n\nPolicies | Strategic choices\nThe crucial question is whether or not the agents should cooperate or defect in the prosperous state, \\(\\mathsf p\\), under the assumption that agents are anonymous - and how to answer depends on the parameter conditions \\(q_c\\), \\(p_r\\), \\(\\gamma\\), \\(r\\), \\(c\\), and \\(m\\).\nAnonymity means that agents do not consider the game’s history, i.e., behave according to a Markov policy. We analyze the two extreme cases: An agent can either cooperate or defect in the prosperous state, \\(\\mathsf p\\).\nGenerally, a single agent’s policy is represented by a \\(Z \\times M\\) tensor. The cooperative policy is then given by\n\nXsa_coo =  0.5 * np.ones((Z, M))\nXsa_coo[p, c] = 1\nXsa_coo[p, d] = 0\n\nThe defective policy is given by\n\nXsa_def =  0.5 * np.ones((Z, M))\nXsa_def[p, c] = 0\nXsa_def[p, d] = 1\n\nTo obtain the incentive regimes, we need to calculate the long-term values of the four policy combinations:\n\nmutual cooperation \\((\\mathsf c \\mathsf c)\\), i.e. both agents cooperate,\nunilateral defection \\((\\mathsf d \\mathsf c)\\), i.e. one agent defects, and the other cooperates,\nunilateral cooperation \\((\\mathsf c \\mathsf d)\\), i.e. one agent cooperates, and the other defects, and\nmutual defection \\((\\mathsf d \\mathsf d)\\), i.e. both agents defect.\n\nThey can also be represented by a meta-game payoff matrix, where the rows represent the focal agent’s policies, and the columns represent the opponent’s policies,\n\n\n\n\n\\(\\mathsf c\\)\n\\(\\mathsf d\\)\n\n\n\n\n\\(\\mathsf c\\)\n\\(v_{\\mathsf c \\mathsf c}\\)\n\\(v_{\\mathsf c \\mathsf d}\\)\n\n\n\\(\\mathsf d\\)\n\\(v_{\\mathsf d \\mathsf c}\\)\n\\(v_{\\mathsf d \\mathsf d}\\)\n\n\n\nWe summarize these respecitve joint policies in four \\(N \\times Z \\times M\\) tensors,\n\nXisa_cc = np.array([Xsa_coo, Xsa_coo])\nXisa_cd = np.array([Xsa_coo, Xsa_def])\nXisa_dc = np.array([Xsa_def, Xsa_coo])\nXisa_dd = np.array([Xsa_def, Xsa_def])\n\n\n\nState values | Long-term welbeing\nLong-term values are defined precisely like in the single-agent case (03.01-SequentialDecisions) except that they now depend on the joint policy \\(\\mathbf x\\) and each agent \\(i\\) holds their own values.\nGiven a joint policy \\(\\mathbf x\\), we define the state value for agent \\(i\\), \\(v^i_{\\mathbf x}(s)\\), as the expected gain, \\(\\mathbb E_\\mathbf{x}[ G^i_t | S_t = s]\\), when starting in state \\(s\\) and the following the policy \\(\\mathbf x\\),\n\\[\nv^i_\\mathbf{x}(s) := \\mathbb E_\\mathbf{x}[ G^i_t | S_t = s] = (1-\\gamma^i) \\mathbb E_\\mathbf{x}\\left[\\sum_{\\tau=t}^\\infty (\\gamma^i)^\\tau R^i_{t+\\tau+1} | S_t = s\\right], \\quad \\text{for all } s \\in \\mathcal S,\n\\]\nThey are also computable like in the single-agent case (03.01-SequentialDecisions) by solving the Bellman equations in matrix form,\n\\[\\mathbf v^i_\\mathbf{x} = (1-\\gamma^i) (\\mathbf 1_Z - \\gamma^i\\underline{\\mathbf T}_\\mathbf{x})^{-1} \\mathbf R^i_\\mathbf{x}.\\]\nBefore we solve this equation, we focus on computing the effective transition matrix \\(\\underline{\\mathbf T}_\\mathbf{x}\\) and the average reward \\(\\mathbf R^i_\\mathbf{x}\\), given a joint policy \\(\\mathbf x\\).\nThe transition matrix \\(\\underline{\\mathbf T}_\\mathbf{x}\\) is a \\(Z \\times Z\\) matrix, where the element \\(T_\\mathbf{x}(s,s')\\) is the probability of transitioning from state \\(s\\) to \\(s'\\) under the joint policy \\(\\mathbf x\\). It is computed as\n\\[T_\\mathbf{x}(s,s') = \\sum_{a^1 \\in \\mathcal A^1} \\dots \\sum_{a^N \\in \\mathcal A^N} x^1(s, a^1) \\dots x^N(s, a^N) T(s, a^1, \\dots, a^N, s').\\]\nFor \\(N=2\\), we implement in Python as follows:\n\ndef compute_symbolic_TransitionMatrix_Tss(policy_Xisa, \n                                          transitions_Tsas):\n    s, a, b, s_ = 0, 1, 2, 3  # defining indices for convenience\n    \n    Tss = sp.Matrix(np.einsum(policy_Xisa[0], [s, a], \n                              policy_Xisa[1], [s, b],\n                              transitions_Tsas, [s, a, b, s_], \n                              [s,s_]))   \n    return sp.simplify(Tss)\n\nFor example, the transition matrix for the joint policy \\(\\mathbf x = (\\mathsf d, \\mathsf c)\\) is then given by\n\ncompute_symbolic_TransitionMatrix_Tss(Xisa_dc, TransitionTensor)\n\n\\(\\displaystyle \\left[\\begin{matrix}1.0 - 0.5 q_{c} & 0.5 q_{c}\\\\1.0 p_{r} & 1.0 - 1.0 p_{r}\\end{matrix}\\right]\\)\n\n\nThe average reward \\(\\mathbf R^i_\\mathbf{x}\\) is a \\(N \\times Z\\)-matrix, where the element \\(R_\\mathbf{x}^i(s)\\) is the expected reward agent \\(i\\) receives in state \\(s\\) under the joint policy \\(\\mathbf x\\). It is computed as\n\\[ R_\\mathbf{x}^i(s) = \\sum_{a^1 \\in \\mathcal A^1} \\dots \\sum_{a^N \\in \\mathcal A^N} x^1(s, a^1) \\dots x^N(s, a^N) T(s, a^1, \\dots, a^N, s') R^i(s, a^1, \\dots, a^N, s').\\]\nFor \\(N=2\\), we implement in Python as follows:\n\ndef compute_symbolic_AverageReward_Ris(policy_Xisa, \n                                       transitions_Tsas, \n                                       rewards_Risas):\n    i, s, a, b, s_ = 0, 1, 2, 3, 4 # defining indices for convenience\n    Ris = sp.Array(np.einsum(policy_Xisa[0], [s, a], \n                             policy_Xisa[1], [s, b],\n                             transitions_Tsas, [s, a, b, s_], \n                             rewards_Risas, [i, s, a, b, s_],\n                             [i, s]))\n    return sp.simplify(Ris)\n\nFor example, the average reward under the joint policy \\(\\mathbf x = (\\mathsf d, \\mathsf c)\\) is then given by\n\ncompute_symbolic_AverageReward_Ris(Xisa_dc, TransitionTensor, RewardTensor)\n\n\\(\\displaystyle \\left[\\begin{matrix}- \\frac{c r \\left(0.5 q_{c} - 1.0\\right)}{2} + 0.5 m q_{c} & 1.0 m\\\\- \\frac{c \\left(0.5 q_{c} - 1.0\\right) \\left(r - 2\\right)}{2} + 0.5 m q_{c} & 1.0 m\\end{matrix}\\right]\\)\n\n\nWith the transition matrix \\(\\underline{\\mathbf T}_\\mathbf{x}\\) and the average reward \\(\\mathbf R^i_\\mathbf{x}\\), we can now solve the Bellman equation for the state values \\(\\mathbf v^i_\\mathbf{x}\\). For convenience, we assume a homogeneous discount factor \\(\\gamma^i = \\gamma\\) for all agents \\(i\\).\n\ndcf = sp.symbols('gamma')\ndef compute_symbolic_statevalues(policy_Xisa, \n                                 transitions_Tsas, \n                                 rewards_Risas, \n                                 discountfactor=dcf):\n    i, s, a, s_ = 0, 1, 2, 3  # defining indices for convenience\n    \n    Tss = compute_symbolic_TransitionMatrix_Tss(\n        policy_Xisa, transitions_Tsas)\n    Ris = compute_symbolic_AverageReward_Ris(\n        policy_Xisa, transitions_Tsas, rewards_Risas)\n      \n    inv = (sp.eye(2) - discountfactor*Tss).inv();\n    inv.simplify()  # sp.simplify() often helps \n    \n    Vis = (1-discountfactor) *\\\n        sp.Matrix(np.einsum(inv, [s,s_], Ris, [i, s_], [i, s]));\n    \n    return sp.simplify(Vis)\n\nWith the help of the function compute_symbolic_statevalues, we can now compute the state values for all four joint policies.\n\nMutual cooperation\nThe state value of the prosperous state, \\(\\mathsf p\\), for the joint policy \\((\\mathsf c, \\mathsf c)\\) is given by\n\nstatevalues_Vis_cc = compute_symbolic_statevalues(\n    Xisa_cc, TransitionTensor, RewardTensor)\n\n\nVcc_p = statevalues_Vis_cc[0, p]\nVcc_g = statevalues_Vis_cc[0, g]\nVcc_p\n\n\\(\\displaystyle 1.0 c \\left(r - 1\\right)\\)\n\n\n\n\nUnilateral defection\nThe state value of the prosperous state, \\(\\mathsf p\\), for the joint policy \\((\\mathsf d, \\mathsf c)\\) is given by\n\nstatevalues_Vis_dc = compute_symbolic_statevalues(\n    Xisa_dc, TransitionTensor, RewardTensor)\n\n\nVdc_p = statevalues_Vis_dc[0, p]\nVdc_g = statevalues_Vis_dc[0, g]\nVdc_p\n\n\\(\\displaystyle \\frac{- 0.5 c \\gamma p_{r} q_{c} r + 1.0 c \\gamma p_{r} r + 0.5 c \\gamma q_{c} r - 1.0 c \\gamma r - 0.5 c q_{c} r + 1.0 c r + 1.0 \\gamma m p_{r} q_{c} + 1.0 m q_{c}}{2.0 \\gamma p_{r} + 1.0 \\gamma q_{c} - 2.0 \\gamma + 2.0}\\)\n\n\n\n\nUnilateral cooperation\nThe state value of the prosperous state, \\(\\mathsf p\\), for the joint policy \\((\\mathsf c, \\mathsf d)\\) is given by\n\nstatevalues_Vis_cd = compute_symbolic_statevalues(\n    Xisa_cd, TransitionTensor, RewardTensor)\n\n\nVcd_p = statevalues_Vis_cd[0, p]\nVcd_g = statevalues_Vis_cd[0, g]\nVcd_p\n\n\\(\\displaystyle \\frac{- 0.5 c \\gamma p_{r} q_{c} r + 1.0 c \\gamma p_{r} q_{c} + 1.0 c \\gamma p_{r} r - 2.0 c \\gamma p_{r} + 0.5 c \\gamma q_{c} r - 1.0 c \\gamma q_{c} - 1.0 c \\gamma r + 2.0 c \\gamma - 0.5 c q_{c} r + 1.0 c q_{c} + 1.0 c r - 2.0 c + 1.0 \\gamma m p_{r} q_{c} + 1.0 m q_{c}}{2.0 \\gamma p_{r} + 1.0 \\gamma q_{c} - 2.0 \\gamma + 2.0}\\)\n\n\n\n\nMutual defection\nThe state value of the prosperous state, \\(\\mathsf p\\), for the joint policy \\((\\mathsf d, \\mathsf d)\\) is given by\n\nstatevalues_Vis_dd = compute_symbolic_statevalues(\n    Xisa_dd, TransitionTensor, RewardTensor)\n\n\nVdd_p = statevalues_Vis_dd[0, p]\nVdd_g = statevalues_Vis_dd[0, g]\nVdd_p\n\n\\(\\displaystyle \\frac{1.0 m q_{c} \\left(\\gamma p_{r} + 1\\right)}{\\gamma p_{r} + \\gamma q_{c} - \\gamma + 1}\\)\n\n\n\n\n\nCritical curves\nFinally, we can compute the critical conditions on the model parameters where the agents’ incentives change. The three conditions are:\n\nDilemma: The agents are indifferent between all cooperating and all defecting, \\(v_{cc} = v_{dd}\\),\nGreed: Each individual agent is indifferent between cooperating and defecting, given all others cooperate, \\(v_{cc} = v_{dc}\\), and\nFear: Each individual agent is indifferent between cooperating and defecting, given all agents defect, \\(v_{cd} = v_{dd}\\).\n\nWithout greed the action situation becomes a coordination challenge between two pure equilibria of mutual cooperation and mutual defection. Without greed and fear the only Nash equilibrium left is mutual cooperation.\n\n\n\nDimensions of a social dilemma with ordinal payoffs and Nash equilibira shown in boxes from 03.02-StrategicInteractions.\n\n\n\nDilemma\nThe critical curve at which collapse avoidance becomes collectively optimal is obtained by setting \\(v_{cc} = v_{dd}\\). Solving for the collapse impact \\(m\\) yields,\n\ndilemma_m = sp.solve(Vdd_p - Vcc_p, m)[0]\ndilemma_m\n\n\\(\\displaystyle \\frac{c \\left(\\gamma p_{r} r - \\gamma p_{r} + \\gamma q_{c} r - \\gamma q_{c} - \\gamma r + \\gamma + r - 1.0\\right)}{q_{c} \\left(\\gamma p_{r} + 1.0\\right)}\\)\n\n\nWe verify that the dilemma condition does not depend on environmental state by computing it also for the degraded state.\n\nsp.solve(Vdd_g - Vcc_g, m)[0] - dilemma_m\n\n\\(\\displaystyle 0\\)\n\n\n\n\nGreed\nThe critical curve at which agents become indifferent to greed, i.e., exactly where cooperators are indifferent to cooperation and defection, given all other actors cooperate, is obtained by setting \\(v_{cc} = v_{dc}\\). Solving for the collapse impact \\(m\\) yields,\n\ngreed_m = sp.solve(Vdc_p - Vcc_p, m)[0] \ngreed_m\n\n\\(\\displaystyle \\frac{0.5 c \\left(\\gamma p_{r} q_{c} r + 2.0 \\gamma p_{r} r - 4.0 \\gamma p_{r} + \\gamma q_{c} r - 2.0 \\gamma q_{c} - 2.0 \\gamma r + 4.0 \\gamma + q_{c} r + 2.0 r - 4.0\\right)}{q_{c} \\left(\\gamma p_{r} + 1.0\\right)}\\)\n\n\nWe verify that the greed condition does not depend on the environmental state by computing it also for the degraded state.\n\nsp.solve(Vdc_g - Vcc_g, m)[0] - greed_m\n\n\\(\\displaystyle 0\\)\n\n\n\n\nFear\nThe critical curve at which actors are indifferent to fear, i.e., exactly where defectors are indifferent to cooperation and defection, given all other actors defect, is obtained by setting \\(v_{cd} = v_{dd}\\). Solving for the collapse impact \\(m\\) yields,\n\nfear_m = sp.solve(Vcd_g - Vdd_g, m)[0]\nfear_m\n\n\\(\\displaystyle \\frac{0.5 c \\left(- \\gamma p_{r} q_{c} r + 2.0 \\gamma p_{r} q_{c} + 2.0 \\gamma p_{r} r - 4.0 \\gamma p_{r} - \\gamma q_{c}^{2} r + 2.0 \\gamma q_{c}^{2} + 3.0 \\gamma q_{c} r - 6.0 \\gamma q_{c} - 2.0 \\gamma r + 4.0 \\gamma - q_{c} r + 2.0 q_{c} + 2.0 r - 4.0\\right)}{q_{c} \\left(\\gamma p_{r} + 1.0\\right)}\\)\n\n\nWe verify that also the fear condition does not depend on the environmental state by computing it also for the degraded state.\n\nsp.solve(Vcd_g - Vdd_g, m)[0] - fear_m\n\n\\(\\displaystyle 0\\)\n\n\n\n\n\nVisualization\nHaving obtained symbolic expressions for the critical curves, we can now visualize them as a function of the discount factor \\(\\gamma\\), indicating how much the agents value future rewards.\n\nParameter values\nLet us apply the model to the case of global sustainability. We set public goods synergy factors \\(r=1.2\\) and the cost of cooperation to \\(5\\).\n\nvr = 1.2\nvc = 5 \n\nRegarding the transition probabilities, we apply the conversion rule developed in 02.04-StateTransitions to set the collapse leverage, \\(q_c\\) and the recovery probability \\(p_r\\), in terms of typical timescales. Under current business-as-usual policies, there are about 50 years left to avert the collapse (Rockström et al., 2017). After a collapse, there is a potential lock-in into an unfavorable Earth system state for a timescale up to millennia (Steffen et al., 2018). Interpreting a time step as one year, we set the collapse leverage to \\(q_r = 1/50 = 0.02\\) and the recovery probability to \\(p_r = 1/10000 = 0.0001\\).\n\nvqc = 0.02\nvqr = 0.0001\n\n\n\nFinal graphic\nWe convert the symbolic expressions to numerical functions using lambdify.\n\nFdilemma_m = sp.lambdify((r, cost, qc, pr, dcf), dilemma_m, 'numpy')\nFgreed_m = sp.lambdify((r, cost, qc, pr, dcf), greed_m, 'numpy')\nFfear_m = sp.lambdify((r, cost, qc, pr, dcf), fear_m, 'numpy')\n\n\ndcf_values = np.linspace(0.95, 1.0, 100)\n\nplt.plot(dcf_values, Fdilemma_m(vr, vc, vqc, vqr, dcf_values), \n         c='k', label='Dilemma')\nplt.plot(dcf_values, Fgreed_m(vr, vc, vqc, vqr, dcf_values),\n         c='b', label='Greed')\nplt.plot(dcf_values, Ffear_m(vr, vc, vqc, vqr, dcf_values),\n         c='g', label='Fear')\n\nplt.fill_between(dcf_values, 0, 4, color='gray', alpha=0.4)\n\nplt.annotate(\"Tragedy\", (0.975, -1.6), ha='center', va='center')\nplt.annotate(\"Coordination\", (0.987, -4), ha='center', va='center')\nplt.annotate(\"Comedy\", (0.996, -6), ha='center', va='center')\nplt.annotate(\"Collapse avoidance suboptimal\", \n             (0.9999, 2.5), ha='right', va='center')\n\nplt.legend(loc='center left'); plt.ylim(-7, 3); plt.xlim(0.955, 1.0);\nplt.xlabel('Discount factor $\\gamma$'); plt.ylabel('Collapse impact $m$');\n\n\n\n\n\n\n\n\nThis plot is a precise reproduction of the result by (Barfuss et al., 2020). It highlights that\n\nthe same care for the future that makes individual decision-making apt for the long-term also positively impacts collective decision-making.\nThis care for the future alone can turn a tragedy into a comedy, where individual incentives are fully aligned with the collective interest - completely resolving the social dilemma.\nThis is true, given the anticipated collapse impact is sufficiently severe. Thus, agents need to consider the catastrophic impact and, at the same time, be immensely future-oriented.\nNo other mechanism is required: Agents are anonymous, cannot reciprocate, and cannot make any agreements.",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic Interactions</span>"
    ]
  },
  {
    "objectID": "03.03-DynamicInteractions.html#learning-goals-revisited",
    "href": "03.03-DynamicInteractions.html#learning-goals-revisited",
    "title": "8  Dynamic Interactions",
    "section": "8.4 Learning goals revisited",
    "text": "8.4 Learning goals revisited\nIn this chapter,\n\nwe introduced the concept of dynamic games and described the elements of a stochastic game.\nWe applied the stochastic games framework to model human-environment interactions on the question, how the individual care for future consequences and the fact that we all are embedded into a shared environment impacts collective decision-making.\nWe analyze the strategic interactions in this stochastic game model and found that caring for the future can turn a tragedy into a commedy of the commons.\n\n\n\n\n\nBarfuss, W., Donges, J. F., Vasconcelos, V. V., Kurths, J., & Levin, S. A. (2020). Caring for the future can turn tragedy into comedy for long-term collective action under risk of collapse. Proceedings of the National Academy of Sciences, 117(23), 12915–12922. https://doi.org/10.1073/pnas.1916545117\n\n\nRockström, J., Gaffney, O., Rogelj, J., Meinshausen, M., Nakicenovic, N., & Schellnhuber, H. J. (2017). A roadmap for rapid decarbonization. Science. https://doi.org/10.1126/science.aah3443\n\n\nSteffen, W., Rockström, J., Richardson, K., Lenton, T. M., Folke, C., Liverman, D., Summerhayes, C. P., Barnosky, A. D., Cornell, S. E., Crucifix, M., Donges, J. F., Fetzer, I., Lade, S. J., Scheffer, M., Winkelmann, R., & Schellnhuber, H. J. (2018). Trajectories of the Earth System in the Anthropocene. Proceedings of the National Academy of Sciences, 115(33), 8252–8259. https://doi.org/10.1073/pnas.1810141115",
    "crumbs": [
      "Target Equilibria",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic Interactions</span>"
    ]
  },
  {
    "objectID": "04-TransformationAgency.html",
    "href": "04-TransformationAgency.html",
    "title": "Transformation Agency",
    "section": "",
    "text": "In this last part, we cover transformation-agency or agent-based models. They operationalize transformation knowledge, which is knowledge about how to move from the existing system to the desired future. This knowledge includes concrete strategies and steps to take. In sustainability transitions, producing transformation knowledge could involve developing policy instruments, designing new institutions, or implementing new technologies. Transformation knowledge is strongly associated with agency and asks how to?.\nTransformation-agency models (or agent-based models applied to sustainability transitions). Agent-based modeling (ABM) can be viewed as a merger between dynamic systems and target equilibrium models, in that typical agent-based models are about the dynamics of agent behavior. However, agent-based modeling in itself is diverse. Therefore, we can only provide a brief overview and specific aspects of ABM in this part.\n\n\n\nThree types of models based on three types of knowledge for transdisciplinary reserach\n\n\nSpecifically, we will cover\n\nRule-based behavioral agency in agent-based models in Chapter 04.01\nIndividual reinforcement learning in Chapter 04.02, and\nThe non-linear dynamics of reinforcement learning in Chapter 04.03.",
    "crumbs": [
      "Transformation Agency"
    ]
  },
  {
    "objectID": "04.01-BehavioralAgency.html",
    "href": "04.01-BehavioralAgency.html",
    "title": "9  Behavioral agency",
    "section": "",
    "text": "9.1 Motivation | Agent-based modeling of complex systems\nThe distinctive feature of the science of complex systems is the fascination that arises when the whole becomes greater than the sum of its parts (Figure 9.1) and properties on the macro-level emerge that do not exist on the micro-level.\nComplex systems thinking is instrumental in understanding the interactions between society and nature (Figure 9.2).\nAgent-based models capture the features of a complex system in the most direct way, in that they model the behavior of individual agents and their interactions.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Behavioral agency</span>"
    ]
  },
  {
    "objectID": "04.01-BehavioralAgency.html#motivation-agent-based-modeling-of-complex-systems",
    "href": "04.01-BehavioralAgency.html#motivation-agent-based-modeling-of-complex-systems",
    "title": "9  Behavioral agency",
    "section": "",
    "text": "Figure 9.1: Properties of a complex system\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: Complex Society-Nature Systems\n\n\n\n\n\nLearning goals\nAfter this lecture, students will be able to:\n\nExplain the history and rationale of agent-based modeling and generative social science\nExplain the advantages and challenges of agent-based modeling\nSimulate two famous agent-based models in Python\nImplement animations in Python\nWrite object-oriented Python programs\n\n\nimport numpy as np  \nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, interactive\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfrom functools import partial\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; \nplt.rcParams['grid.linewidth'] = 0.25;",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Behavioral agency</span>"
    ]
  },
  {
    "objectID": "04.01-BehavioralAgency.html#overview-generative-social-science",
    "href": "04.01-BehavioralAgency.html#overview-generative-social-science",
    "title": "9  Behavioral agency",
    "section": "9.2 Overview | Generative social science",
    "text": "9.2 Overview | Generative social science\n\nEssence of agent-based modeling\nAn agent-based model explicitly represents the individual units of the system and their repeated interactions. Beyond this, no further assumptions are made in agent-based modeling (Izquierdo et al., 2024).\nHence, agent-based modeling is a very flexible modeling framework.\n\nYou can model anything with an agent-based model, but you are not guaranteed to understand it.\n\nAdding complexity to your model allows you to study different phenomena. However, it can make the analysis and understanding of the model more difficult, even with advanced mathematics. As a result, agent-based models are usually created using a programming language and analyzed through computer simulation. This method is so common that agent-based modeling and simulation are often seen as synonymous (Izquierdo et al., 2024).\n\n\nGenerative Social Science\nThe flexibility of ABM offers a generative approach to social science (Epstein, 1999). Explaining the emergence of macroscopic societal regularities requires that one answers the following question:\n\nHow could the decentralized local interactions of heterogeneous autonomous agents generate the given regularity?\n\nGenerative social science is a research approach that uses computational models to generate and explain complex social behaviors. It is characterized by a\n\nfocus on explanation over prediction and\nits use of agent-based models to bridge the gap between micro and macro phenomena.\n\n\n\nFeatures of agent-based modeling\nAgent-based modeling allows us to cover some features that traditionally have been difficult to analyze mathematically (Epstein, 1999; Izquierdo et al., 2024):\n\nBounded Rationality, which has two components: bounded information and bounded computing power. Typically, agents use more or less sophisticated rules or heuristics based on local information. They do not have global information or infinite computational power.\nAutonomy and the micro-macro link. ABM is particularly well suited to studying how global phenomena emerge from individual interactions and how these emergent global phenomena may constrain and shape individuals’ actions. In agent-based models, there is no central, or “top-down,” control over individual behavior. Micro and macro will typically co-evolve.\nOut-of-equilibrium dynamics. Dynamics are inherent to ABM. Running a simulation consists of repeatedly applying the rules that define the model. Equilibria are never imposed a priori; they may emerge as an outcome of the simulation, or they may not.\nAgents’ heterogeneity. Since agents are explicitly modeled, their diversity can vary according to the modeler’s preferences. Agents may differ in myriad ways — genetically, culturally, by social network, by preferences — all of which may change or adapt endogenously over time. Representative or aggregated agent methods - common in dynamic systems or target equilibria models - or not used.\nLocal interactions and explicit space. The fact that agents and their environment are represented explicitly in ABM makes it particularly straightforward and natural to model local interactions.\nInterdependencies between processes (e.g., demographic, economic, biological, geographical, technological) that have traditionally been studied in different disciplines and are not often analyzed together. An agent-based model does not restrict the type of rules that can be implemented, so models can include rules linking disparate aspects of the world that are often studied in different disciplines. This feature makes ABM particularly well suited to studying complex Nature-Society systems.\n\n\n\nThe generativist’s experiment\nA given macroscopic regularity is to be explained by the canonical agent-based experiment as follows (Epstein, 1999):\n\nSituate an initial population of autonomous, heterogeneous agents in a relevant spatial environment; - allow them to interact according to simple local rules, - and thereby generate - or “grow” - the macroscopic regularity from the bottom up.\n\nIf you didn’t grow it, you didn’t explain its emergence. This generativist approach to social science has been successfully applied to explain the following phenomena:\n\neconomic classes (Axtell etl al. 1999)\ncooperation in spatial games (Lindgren and Nordahl, 1994; Epstein, 1998; Huberman and Glance, 1993; Nowak and May, 1992; Miller, 1996)\nvoting behaviors (Kollman, Miller, and Page, 1992),\ndemographic histories (Dean et al. 1999)\ntrade networks (Tesfatsion, 1995; Epstein and Axtell, 1996),\nright-skewed wealth distributions (Epstein and Axtell, 1996)\n\n… and many more!\n\n\nVariants of agent-based modeling\nAs agent-based modeling is so flexible, various subcategories of models can be distinguished:\n\nCellular automata\nNetwork models\nLearning and evolution models in games\n…",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Behavioral agency</span>"
    ]
  },
  {
    "objectID": "04.01-BehavioralAgency.html#example-conways-game-of-life",
    "href": "04.01-BehavioralAgency.html#example-conways-game-of-life",
    "title": "9  Behavioral agency",
    "section": "9.3 Example | Conway’s Game of Life",
    "text": "9.3 Example | Conway’s Game of Life\nThe Game of Life is a cellular automaton devised by the British mathematician John Horton Conway in 1970.\nA cellular automaton is a discrete model of computation consisting of a regular grid of cells, each in one of a finite number of states (e.g., on and off).\nThe game of life is a very influential model in the field of complex systems (although Conway wasn’t particularly proud of it)\nSee Inventing Game of Life (John Conway) - Numberphile for a brief backstory on the game of life.\n\n\n\n        \n        \n\n\n\nQuestions\nThe game of life is a comparably simple model to answer two very fundamental questions:\n\nHow can something reproduce itself?\nHow can a complex structure (like the mind) emerge from a basic set of rules?\n\n\n\nStates\nThe cells of the cellular automaton can be in one of two states: dead or alive.\nWe can represent the state of a cell with a binary variable: 1 (black) for alive and 0 (white) for dead. The state of the whole system can the be represented as follows:\n\nROWS, COLS = 40, 100  # define the size of the grid\ngrid = np.random.choice([0, 1], size=(ROWS, COLS), p=[0.7, 0.3]) #generate random states\nplt.imshow(grid, cmap='binary', interpolation='none') # plot the grid\nplt.gca().set_xticks([]); plt.gca().set_yticks([]);  # remove x and y ticks\n\n\n\n\nA Game-of-Life grid\n\n\n\n\n\n\nDynamics\nThe dynamics of the game of life are governed by the following rules:\n\nLiving cells with fewer than two living neighbors die\nLiving cells with more than three living neighbors die\nDead cells with exactly three neighbors become alive\n\nWe will use the matplotlib.animation.FuncAnimation function to animate the game of life in Python. To do so, we need to implement the game’s rules in a function that updates the grid.\nIt must receive the number of the current frame of the animation plus possible further function arguments. We supply it with an image argument variable representing the grid’s image. It must return an iterable of artists, which the matplotlib.animation.FuncAnimation will use to update the plot.\n\n# Function to update the grid based on the Game of Life rules\ndef update_grid(frame, image):\n    global grid  # required to access the variable inside the function\n    new_grid = grid.copy()\n\n    for i in range(0, ROWS):\n        for j in range(0, COLS):\n            neighbors_sum = (  # the % sign is a modulo division, i.e., 13 % 13 = 0 \n                grid[(i - 1) % ROWS, (j - 1) % COLS] + grid[(i - 1) % ROWS, j] +\n                grid[(i - 1) % ROWS, (j + 1) % COLS] + grid[i, (j - 1) % COLS] +\n                grid[i, (j + 1) % COLS] + grid[(i + 1) % ROWS, (j - 1) % COLS] +\n                grid[(i + 1) % ROWS, j] + grid[(i + 1) % ROWS, (j + 1) % COLS])\n\n            # the rules of the game\n            if grid[i, j] == 1 and (neighbors_sum &lt; 2 or neighbors_sum &gt; 3):\n                new_grid[i, j] = 0\n            elif grid[i, j] == 0 and neighbors_sum == 3:\n                new_grid[i, j] = 1\n\n    grid = new_grid\n    image.set_array(grid)\n    return image,  # must return an iterable of artists \n\nWith the update_grid function in place, we can now create the animation using the FuncAnimation function. The %%caputure magic command is used to suppress the output of the cell below, as we will call the animation separately.\n\n%%capture\n# Set up the Matplotlib figure and axis\nfig, ax = plt.subplots(figsize=(16,9))\nim = ax.imshow(grid, cmap='binary', interpolation='none')\nax.set_xticks([]); ax.set_yticks([])\n\n# Create animation\nani = animation.FuncAnimation(fig, partial(update_grid, image=im),\n                              frames=150, interval=150)\n\nFinally, we can display the animation using the HTML function from the IPython.display module.\n\n# Display the animation using HTML\n# HTML(ani.to_jshtml())\n\n\n\nEmerging structures\nDespite the simplicity of the rules, complex structures of species moving and reproducing can emerge from these rules, despite them not having any concept of movement or reproduction.\nSee, for example, the Epic Conway’s Game of Life or Life in life videos.\n\n\n\n        \n        \n\n\n\n\n\n        \n        \n\n\n\n\nImpact\nAlthough the rules are incredibly simple, it is impossible to say whether a given configuration persists or eventually dies out. There are fundamental limits to prediction.\nIt was shown that you can do any form of computation (that you can do on a regular computer) with the game of life.\nComplex behavior does not require complicated rules. Complex behavior can emerge from simple rules. This realization has been a key insight of complexity sciences and has shaped the way complexity science is done today.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Behavioral agency</span>"
    ]
  },
  {
    "objectID": "04.01-BehavioralAgency.html#example-schellings-segregation-model",
    "href": "04.01-BehavioralAgency.html#example-schellings-segregation-model",
    "title": "9  Behavioral agency",
    "section": "9.4 Example | Schelling’s segregation model",
    "text": "9.4 Example | Schelling’s segregation model\nThe second example model studies the phenomenon of racially segregated neighborhoods. The content here is heavily inspired by QuantEcon’s Quantitative Economics with Python.\n\nQuestions\nWe observe racially segregated neighborhoods.\nDoes that mean that all residents are racists?\n\n\nContext\nIn 1969, Thomas C. Schelling developed a simple but striking model of racial segregation.\nHis model studies the dynamics of racially mixed neighborhoods.\nLike much of Schelling’s work, the model shows how local interactions can lead to surprising aggregate structure.\nIn particular, it shows that relatively mild preference for neighbors of similar race can lead in aggregate to the collapse of mixed neighborhoods, and high levels of segregation.\nIn recognition of this and other research, Schelling was awarded the 2005 Nobel Prize in Economic Sciences (joint with Robert Aumann).\n\n\nThe Model\nWe will cover a variation of Schelling’s model that is easy to program and captures the main idea.\n\nSet-Up\nSuppose we have two types of people: orange people and green people.\nFor the purpose of this lecture, we will assume there are 250 of each type.\nThese agents all live on a single-unit square.\nThe location of an agent is just a point \\((x, y)\\), where \\(0 &lt; x, y &lt; 1\\).\n\n\nPreferences\nWe will say that an agent is happy if half or more of her 10 nearest neighbors are of the same type.\nHere ‘nearest’ is in terms of Euclidean distance.\nAn agent who is not happy is called unhappy.\nAn important point here is that agents are not averse to living in mixed areas.\nThey are perfectly happy if half their neighbors are of the other color.\n\n\nBehavior\nInitially, agents are mixed together (integrated).\nIn particular, the initial location of each agent is an independent draw from a bivariate uniform distribution on \\(S = (0, 1)^2\\).\nNow, cycling through the set of all agents, each agent is now given the chance to stay or move.\nWe assume that each agent will stay put if they are happy and move if unhappy.\nThe algorithm for moving is as follows\n\nDraw a random location in \\(S\\)\n\nIf happy at the new location, move there\n\nElse, go to step 1\n\nIn this way, we cycle continuously through the agents, moving as required.\nWe continue to cycle until no one wishes to move.\n\n\n\nImplementation\nWe use object-oriented programming (OOP) to model agents as objects.\nOOP is a programming paradigm based on the concept of objects, which can contain data and code: - data in the form of fields (often known as attributes or properties), and - code in the form of procedures (often known as methods).\n\nAgent class\nA class defines how an object will work. Typically, the class will define several methods that operate on instances of the class. A key method is the __init__ method, which is called when an object is created.\n\nclass Agent: \n\n    # The init method is called when the object is created.\n    def __init__(self, type, num_neighbors, require_same_type): \n        self.type = type\n        self.draw_location()\n        self.num_neighbors = num_neighbors\n        self.require_same_type = require_same_type\n\n    def draw_location(self):\n        self.location = np.random.uniform(0, 1), np.random.uniform(0, 1)\n\n    def get_distance(self, other):\n        \"Computes the Euclidean distance between self and another agent.\"\n        a = (self.location[0] - other.location[0])**2\n        b = (self.location[1] - other.location[1])**2\n        return np.sqrt(a + b)\n\n    def number_same_type(self, agents):\n        \"Number of neighbors of same type.\"\n        distances = []\n        # distances is a list of pairs (d, agent), where d is the distance from\n        # agent to self\n        for agent in agents:\n            if self != agent:\n                distance = self.get_distance(agent)\n                distances.append((distance, agent))\n        # == Sort from smallest to largest, according to distance == #\n        distances.sort()\n        # == Extract the neighboring agents == #\n        neighbors = [agent for d, agent in distances[:self.num_neighbors]]\n        # == Count how many neighbors have the same type as self == #\n        return sum(self.type == agent.type for agent in neighbors)\n\n    def happy(self, agents):\n        \"True if a sufficient number of nearest neighbors are of the same type.\"\n        num_same_type = self.number_same_type(agents)\n        return num_same_type &gt;= self.require_same_type\n\n    def update(self, agents):\n        \"If not happy, then randomly choose new locations until happy.\"\n        while not self.happy(agents):\n            self.draw_location()\n\nTesting the agent class:\n\nA = Agent(0, num_neighbors=4, require_same_type=2)\ntype(A)\n\n__main__.Agent\n\n\n\nA.location\n\n(0.9530720480796, 0.566243724536969)\n\n\nCreating a list of agents:\n\nnp.random.seed(4)\nagents = [Agent(0, 4, 2) for i in range(100)]\nagents.extend(Agent(1, 4, 2) for i in range(100))\nlen(agents)\n\n200\n\n\nIs agent three happy?\n\na3 = agents[3]\na3.happy(agents), a3.location\n\n(False, (0.9762744547762418, 0.006230255204589863))\n\n\nLet’s let agent three update its position:\n\na3.update(agents)\n\n\nagents[3].happy(agents), agents[3].location\n\n(True, (0.06780815958339637, 0.961674586087924))\n\n\n\n\nObservation function\nWe implement a function to plot the distribution of agents.\n\ndef plot_distribution(agents, cycle_num, ax=None):\n    \"Plot the distribution of agents after cycle_num rounds of the loop.\"\n    x_values_0, y_values_0 = [], []\n    x_values_1, y_values_1 = [], []\n    # == Obtain locations of each type == #\n    for agent in agents:\n        x, y = agent.location\n        if agent.type == 0:\n            x_values_0.append(x)\n            y_values_0.append(y)\n        else:\n            x_values_1.append(x)\n            y_values_1.append(y)\n    if ax is None: fig, ax = plt.subplots(figsize=(4, 4))\n    plot_args = {'markersize': 4, 'alpha': 0.6}\n    # ax.set_facecolor('azure')\n    ax.plot(x_values_0, y_values_0, 'o', markerfacecolor='orange', **plot_args)\n    ax.plot(x_values_1, y_values_1, 'o', markerfacecolor='green', **plot_args)\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.set_title(f'Cycle {cycle_num}')\n\nTesting the observation function,\n\nnum_of_type_0 = 250\nnum_of_type_1 = 250\nnum_neighbors = 10      # Number of agents regarded as neighbors\nrequire_same_type = 5   # Want at least this many neighbors to be same type\n\n# == Create a list of agents == #\nagents = [Agent(0, num_neighbors, require_same_type) for i in range(num_of_type_0)]\nagents.extend(Agent(1, num_neighbors, require_same_type) for i in range(num_of_type_1))\n\nplot_distribution(agents, 0)\n\n\n\n\nInitial distribution of agents.\n\n\n\n\n\n\nSimulation run\n\nnp.random.seed(10)  # For reproducible random numbers\n\n# == Main == #\nnum_of_type_0 = 250\nnum_of_type_1 = 250\nnum_neighbors = 10      # Number of agents regarded as neighbors\nrequire_same_type = 5   # Want at least this many neighbors to be same type\n\n# == Create a list of agents == #\nagents = [Agent(0, num_neighbors, require_same_type) for i in range(num_of_type_0)]\nagents.extend(Agent(1, num_neighbors, require_same_type) for i in range(num_of_type_1))\n\ncount = 1\n# ==  Loop until none wishes to move == #\nfig, axs = plt.subplots(2,3, figsize=(13, 6))\naxs.flatten()\n\nwhile True:\n    print('Entering loop ', count)\n    plot_distribution(agents, count, axs.flatten()[count-1])\n    count += 1\n\n    # Update and check whether everyone is happy\n    no_one_moved = True\n    for agent in agents:\n        old_location = agent.location\n        agent.update(agents)\n        if agent.location != old_location:\n            no_one_moved = False\n    if no_one_moved:\n        break\n    \nprint('Converged, terminating.')\nplt.tight_layout()\n\nEntering loop  1\nEntering loop  2\nEntering loop  3\nEntering loop  4\nEntering loop  5\nEntering loop  6\nConverged, terminating.\n\n\n\n\n\nA simulation run of Schelling’s model.\n\n\n\n\nIn this instance, the program terminated after 6 cycles through the set of agents, indicating that all agents had reached a state of happiness.\n\n\n\nInterpretation\nWhat is striking about the pictures is how rapidly racial integration breaks down.\nThis is despite the fact that people in the model don’t actually mind living mixed with the other type.\nEven with these preferences, the outcome is a high degree of segregation.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Behavioral agency</span>"
    ]
  },
  {
    "objectID": "04.01-BehavioralAgency.html#challenges-of-agent-based-modeling",
    "href": "04.01-BehavioralAgency.html#challenges-of-agent-based-modeling",
    "title": "9  Behavioral agency",
    "section": "9.5 Challenges of agent-based modeling",
    "text": "9.5 Challenges of agent-based modeling\n\nPerformance Limitations. The execution speed of ABMs can be slow, which can be a limitation for extensive simulations.\nTransparency and Reproducibility. Providing a clear and accessible description is challenging due to model complexity.\nData Parameters and Validation. Getting empirical data and validating models that may simulate unobservable associations is challenging.\nArbitrariness and Parameterization. The many parameters that need to be set can lead to a high degree of arbitrariness.\nBehavior modeling. There are endless possibilities to design plausible behavioral rules. A sensitivity analysis is difficult.\n\nUp next\n\nReinforcement learning as a principled model for behavior to counter some arbitrariness in parameterization behavioral rules.\nSynthesis: Collective reinforcement learning dynamics to counter performance limitations and a lack of transparency and reproducibility.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Behavioral agency</span>"
    ]
  },
  {
    "objectID": "04.01-BehavioralAgency.html#learning-goals-revisited",
    "href": "04.01-BehavioralAgency.html#learning-goals-revisited",
    "title": "9  Behavioral agency",
    "section": "9.6 Learning goals revisited",
    "text": "9.6 Learning goals revisited\nIn this chapter,\nwe covered the history and rationale of agent-based modeling: generative social science.\nWe covered the advantages (flexibility and expressiveness) and challenges (transparency, arbitrariness, performance) of agent-based modeling\nWe implemented and simulated two famous agent-based models in Python: Conway’s Game of Life and Schelling’s segregation model.\nWe implement animations in Python using the matplotlib.animations.FuncAnimation function.\nWe wrote Schelling’s segregation model as an object-oriented program.\n\n\n\n\nEpstein, J. M. (1999). Agent-based computational models and generative social science. Complexity, 4(5), 41–60. https://doi.org/10.1002/(SICI)1099-0526(199905/06)4:5&lt;41::AID-CPLX9&gt;3.0.CO;2-F\n\n\nIzquierdo, L. R., Izquierdo, S. S., & Sandholm, W. H. (2024). Agent-Based Evolutionary Game Dynamics. https://doi.org/10.5281/zenodo.13938500",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Behavioral agency</span>"
    ]
  },
  {
    "objectID": "04.02-IndividualLearning.html",
    "href": "04.02-IndividualLearning.html",
    "title": "10  Individual learning",
    "section": "",
    "text": "10.1 Motivation\nIn this chapter, we will introduce the basics of temporal-difference reward-prediction reinforcement learning.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Individual learning</span>"
    ]
  },
  {
    "objectID": "04.02-IndividualLearning.html#motivation",
    "href": "04.02-IndividualLearning.html#motivation",
    "title": "10  Individual learning",
    "section": "",
    "text": "Give a man a fish, and he’ll eat for a day\n\n\nTeach a man to fish, and he’ll eat for a lifetime\n\n\nGive a man a taste for fish, and he’ll eat even if conditions change. [source]\n\n\n\nUsing behavioral theories in ABMs is challenging\nGeneral agent-based modeling is a flexible tool for studying different theories of human behavior. However, the social and behavioral sciences are not known for their tendency to integrate. Knowledge about human behavior is fragmented into many different, context-specific, and often not formalized theories. For example, in an attempt to order and use this knowledge for sustainability science, Constantino and colleagues presented a selection of 32 behavioral theories (Constantino et al., 2021).\n\nThe many behavioral theories pose a significant challenge for general agent-based modeling when it comes to incorporating human decision-making into models of Nature-society systems (Schlüter et al., 2017):\n\nFragmentation of theories: A vast array of theories on human decision-making is scattered across different disciplines, making it difficult to navigate and select relevant theories. Each theory often focuses on specific aspects of decision-making, leading to fragmented knowledge.\nIncomplete theories: The degree of formalization varies across theories. Many decision-making theories are incomplete or not fully formalized, requiring modelers to fill logical gaps with assumptions to make simulations work. This step introduces more degrees of freedom and possibly arbitrariness into the modeling process.\nCorrelation-based theories: Many theories focus on correlations rather than causal mechanisms, essential for dynamic modeling. This requires modelers to make explicit assumptions about causal relationships - introducing more degrees of freedom and possibly arbitrariness into the modeling process.\nContext-dependent theories: The applicability of theories can vary greatly depending on the context, which adds complexity to their integration into models.\n\n\n\nReinforcement learning offers a principled take\nReinforcement learning (RL) offers a general prototype model for intelligent & adaptive decision-making in agent-based models.\nPrinciple\n\n“Do more of what makes you happy.”\n\nWe do not need to specify the behavior of an agent directly.\nInstead, we specify what an agent wants and how it learns. Crucially, how it learns does not depend on the details of the environment model. It is a more general process, applicable across different environments.\nThen, it learns for itself what to do and we study the learning process and the learned behaviors.\nThis approach is particularly valuable for studying human-environment systems when the decision environment changes through a policy intervention or a global change process, like climate change or biodiversity loss. If we had specified the agent’s behavior directly, the agent’s behavior could not change when the environment changes. In contrast, if we specify the underlying agent’s goal, we can study how the agent’s behavior changes when the environment changes. Reinforcement learning agents can learn while interacting with the environment.\n\n\nAn integrating platform for cognitive mechanisms\nRL, broadly understood, offers an interdisciplinary platform for integrating cognitive mechanisms into ABMs. It offers a comprehensive framework for studying the interplay among learning (adaptive behavior), representation (beliefs), and decision-making (actions) (Botvinick et al., 2020).\n\n\n\nRL-based frameworks with cognitive mechanisms\n\n\nCollective or multi-agent reinforcement learning is a natural extension of RL to study the emerging collective behavior of multiple agents in dynamic environments. It allows for formulating hypotheses on how different cognitive mechanisms affect collective behavior in dynamic environments (Barfuss et al., 2024).\nRL is also an interdisciplinary endeavor, studied in Psychology, Neuroscience, Behavioral economics, Complexity science, and Machine learning.\n\n\n\n\n\n\nFigure 10.1: Reinforcement learning in the brain\n\n\n\nFigure 10.1 shows the remarkable analogy between the firing patterns of dopamine neurons in the brain and the prediction errors in a reinforcement learning simulation.\nFigure 10.1 (a-c) shows prediction errors in a Pavlovian RL conditioning task simulation. A conditional stimulus (CS) is presented randomly, followed 2 seconds later by a reward (Unconditional Stimulus - US). (a) In the early training phase, the reward is not anticipated, leading to prediction errors when the reward is presented. As learning occurs, these prediction errors begin to affect prior events in the trial (examples from trials 5 and 10) because predictive values are learned. (b) After learning, the previously unexpected reward no longer creates a prediction error. Instead, the conditional stimulus now causes a prediction error when it occurs unexpectedly. (c) When the reward is omitted when expected, it results in a negative prediction error, signaling that what happened was worse than anticipated.\nFigure 10.1 (d–f) Firing patterns of dopamine neurons in monkeys engaged in a similar instrumental conditioning task [SchultzEtAl1997]. Each raster plot shows action potentials (dots) with different rows for different trials aligned with the cue (or reward) timing. Histograms show combined activity across the trials below. (d) When a reward is unexpectedly received, dopamine neurons fire rapidly. (e) After conditioning with a visual cue (which predicted a food reward if the animal performed correctly), the reward no longer triggers a burst of activity; now, the burst happens at the cue’s presentation. (f) If the food reward is omitted unexpectedly, dopamine neurons exhibit a distinct pause in firing, falling below their typical rate.\nSource of confusion. Because of its broad scope and interdisciplinary nature, simply the phrase “reinforcement learning” can mean different things to different people. To mitigate this possible source of confusion, it is good to acknowledge that RL can refer to a model of human learning, an optimization method, a problem description, and a field of research.\n\n\nLearning goals\nAfter this chapter, students will be able to:\n\nExplain why reinforcement learning is valuable in models of human-environment interactions\nImplement and apply the different elements of the multi-agent environment framework, including a temporal-different learning agent.\nExplain and manage the trade-off between exploration and exploitation.\nVisualize the learning process\nUse the Python library pandas to manage data\nRefine their skills in object-oriented programming\n\n\nimport numpy as np  \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, interactive\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport sympy as sp\nfrom copy import deepcopy\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (15, 4)\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; \nplt.rcParams['grid.linewidth'] = 0.25; \nplt.rcParams['figure.dpi'] = 140",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Individual learning</span>"
    ]
  },
  {
    "objectID": "04.02-IndividualLearning.html#elements-of-the-multi-agent-environment-interface",
    "href": "04.02-IndividualLearning.html#elements-of-the-multi-agent-environment-interface",
    "title": "10  Individual learning",
    "section": "10.2 Elements of the multi-agent environment interface",
    "text": "10.2 Elements of the multi-agent environment interface\nGenerally, making sense of an agent without its environment is difficult, and vice versa.\n\n\n\n\n\n\nFigure 10.2: Reinforcement learning in the multi-agent environment interface\n\n\n\n\nInterface\nAt the interface between agents and the environment are\n\neach agent’s set of (conceivable) actions - from agents to environment,\nextrinsic reward signals - a single number from environment to each agent,\npossibly observation signals - from environment to agents.\n\nNote: In general, the environment is composed of the natural and the social environment.\n\ndef interface_run(agents, env, NrOfTimesteps):\n    \"\"\"Run the multi-agent environment for several time steps.\"\"\"\n\n    observations = env.observe()\n    \n    for t in range(NrOfTimesteps):\n                \n        actions = [agent.act(observations[i])\n            for i, agent in enumerate(agents)]\n        \n        next_observations, rewards, info = env.step(actions)\n\n        for i, agent in enumerate(agents):\n            agent.update(observations[i], actions[i], rewards[i], next_observations[i])\n\n        observations = next_observations                \n\n\n\nEnvironment\nThe environment delivers extrinsic rewards (motivations) to the agents based on the agents’ chosen actions (choices). It may contain environmental states, which may not be fully observable to the agents\nThe most common environment classes:\n\n\n\n\n\n\n\n\n\n\nAgents\nEnvironment\nObservation\n\n\n\n\nMulti-armed bandit\none\nno states\n-\n\n\nNormal-form game\nmultiple\nno states\n-\n\n\nMarkov decision process\none\nmultiple states\nfull\n\n\nStochastic/Markov games\nmultiple\nmultiple states\nfull\n\n\nPartially observable Markov decision process\none\nmultiple states\npartial\n\n\nPartially observable stochastic games\nmultiple\nmultiple states\npartial\n\n\n\nIn all cases, reward signals may be stochastic and or multi-dimensional.\n\nclass Environment:\n    \"\"\"Abstract environment class.\"\"\"\n    \n    def obtain_StateSet(self):\n        \"\"\"Default state set representation `state_s`.\"\"\"\n        return [str(s) for s in range(self.Z)]\n\n    def obtain_ActionSets(self):\n        \"\"\"Default action set representation `action_a`.\"\"\"\n        return [str(a) for a in range(self.M)]\n        \n    def step(self, \n             jA # joint actions\n            ) -&gt; tuple:  # (observations_Oi, rewards_Ri, info)\n        \"\"\"\n        Iterate the environment one step forward.\n        \"\"\"\n        # choose a next state according to transition tensor T\n        tps = self.TransitionTensor[tuple([self.state]+list(jA))].astype(float)\n        next_state = np.random.choice(range(len(tps)), p=tps)\n    \n        # obtain the current rewards\n        rewards = self.RewardTensor[tuple([slice(self.N),self.state]\n                                          +list(jA)+[next_state])]\n    \n        # advance the state and collect info\n        self.state = next_state\n        obs = self.observe()     \n    \n        # report the true state in the info dict\n        info = {'state': self.state}\n    \n        return obs, rewards.astype(float), info\n\n    def observe(self):\n        \"\"\"Observe the environment.\"\"\"\n        return [self.state for _ in range(self.N)]\n        \n\n\n\nAgents\nAgents act (oftentimes to reach a goal). We need to specify their\n\nActions, describing which choices are available to the agent.\nGoal, describing what an agent wants (in the long run). They may contain intrinsic motivations.\nRepresentation, e.g., defining upon which conditions agents select actions (e.g., history of past actions in multi-agent situations).\nValue beliefs (value functions), capturing what is good for the agent regarding its goal in the long run.\nBehavioral rule (policy, strategy), defining how to select actions.\nLearning rule, describing how value beliefs are updated in light of new information.\n(optionally), a model of the environment and rewards. Models are used for planning, i.e., deciding on a behavioral rule by considering possible future situations before they are actually experienced.\n\n\nclass BehaviorAgent:\n    \n    def __init__(self, policy):\n        self.policy_Xoa = policy / policy.sum(-1, keepdims=True)\n        self.ActionIxs = range(self.policy_Xoa.shape[1])\n        \n    def act(self, obs):\n        return np.random.choice(self.ActionIxs, p=self.policy_Xoa[obs])\n\n    def update(self, *args, **kwargs):\n        pass",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Individual learning</span>"
    ]
  },
  {
    "objectID": "04.02-IndividualLearning.html#example-risk-reward-dilemma",
    "href": "04.02-IndividualLearning.html#example-risk-reward-dilemma",
    "title": "10  Individual learning",
    "section": "10.3 Example | Risk Reward Dilemma",
    "text": "10.3 Example | Risk Reward Dilemma\n\n\n\nRisk Reward Dilemma\n\n\n\nclass RiskRewardDilemma(Environment):\n    \"\"\"A simple risk-reward dilemma environment.\"\"\"\n\n    def obtain_StateSet(self):\n        return ['p', 'd']  # prosperous, degraded\n\n    def obtain_ActionSets(self):\n        return [['c', 'r']]  # cautious, risky\n\n\nTransitions | Environmental dynamics\nThe environmental dynamics, i.e., the transitions between environmental state contexts are modeled by two parameters: a collapse probability, \\(p_c\\), and a recovery probability, \\(p_r\\).\n\npc, pr = sp.symbols('p_c p_r')\npc\n\n\\(\\displaystyle p_{c}\\)\n\n\n\np = RiskRewardDilemma().obtain_StateSet().index('p')\nd = RiskRewardDilemma().obtain_StateSet().index('d')\nc = RiskRewardDilemma().obtain_ActionSets()[0].index('c')\nr = RiskRewardDilemma().obtain_ActionSets()[0].index('r')\np,d,c,r    \n\n(0, 1, 0, 1)\n\n\nWe implement the transitions as a three-dimensional array or tensors, with dimensions \\(Z \\times M \\times Z\\), where \\(Z\\) is the number of states and \\(M\\) is the number of actions.\n\nT = np.zeros((2,2,2), dtype=object)\n\nThe cautious action guarantees to remain in the prosperous state, \\(T(\\mathsf{p,c,p})=1\\). Thus, the agent can avoid the risk of environmental collapse by choosing the cautious action, \\(T(\\mathsf{p,c,d})=0\\).\n\nT[p,c,d] = 0\nT[p,c,p] = 1   \n\nThe risky action risks the collapse to the degraded state, \\(T(\\mathsf{p,r,d}) = p_c\\), with a collapse probability \\(p_c\\). Thus, with probability \\(1-p_c\\), the environment remains prosperous under the risky action, \\(T(\\mathsf{p,r,p}) = 1-p_c\\).\n\nT[p,r,d] = pc\nT[p,r,p] = 1-pc\n\nAt the degraded state, recovery is only possible through the cautious action, \\(T(\\mathsf{d,c,p})=p_r\\), with recovery probability \\(p_r\\). Thus, with probability \\(1-p_r\\), the environment remains degraded under the cautious action, \\(T(\\mathsf{d,c,d})=1-p_r\\).\n\nT[d,c,p] = pr\nT[d,c,d] = 1-pr\n\nFinally, the risky action at the degraded state guarantees a lock-in in the degraded state, \\(T(\\mathsf{d,r,d})=1\\). Thus, the environment cannot recover from the degraded state under the risky action, \\(T(\\mathsf{d,r,p})=0\\).\n\nT[d,r,p] = 0\nT[d,r,d] = 1\n\nLast, we make sure that our transition tensor is normalized, i.e., the sum of all transition probabilities from a state-action pair to all possible next states equals one, \\(\\sum_{s'} T(s, a, s') = 1\\).\n\nT.sum(-1)\n\narray([[1, 1],\n       [1, 1]], dtype=object)\n\n\nAll together, the transition tensor looks as follows:\n\nsp.Array(T)\n\n\\(\\displaystyle \\left[\\begin{matrix}\\left[\\begin{matrix}1 & 0\\\\1 - p_{c} & p_{c}\\end{matrix}\\right] & \\left[\\begin{matrix}p_{r} & 1 - p_{r}\\\\0 & 1\\end{matrix}\\right]\\end{matrix}\\right]\\)\n\n\nRecap | Substituting parameter values. In this chapter, we defined the transition and reward tensors as general numpy arrays with data types object, which we filled with symbolic expressions from sympy. To manipulate and substitute these expressions, we can use the sympy.subs method, however, not directly on the numpy array. Instead, we define a helper function substitute_in_array that takes a numpy array and a dictionary of substitutions and returns a new array with the substitutions applied.\n\ndef substitute_in_array(array, subs_dict):\n    result = array.copy()\n    for index,_ in np.ndenumerate(array):\n        if isinstance(array[index], sp.Basic):\n            result[index] = array[index].subs(subs_dict)\n    return result\n\nTo make this work, it seems to be of critical importance that the subsitution dictionary is given as a dictionary in the form of {&lt;symbol_variable&gt;: &lt;subsitution&gt;, ...} and not as dict(&lt;symbol_variable&gt;=&lt;subsitution&gt;, ...). For example,\n\nsubstitute_in_array(T, {pc: 0.1, pr: 0.05}).astype(float)\n\narray([[[1.  , 0.  ],\n        [0.9 , 0.1 ]],\n\n       [[0.05, 0.95],\n        [0.  , 1.  ]]])\n\n\nWith the help of the substitue_in_array function we give the risk-reward dilemma class its environmental dynamics:\n\ndef create_TransitionTensor(self):\n    \"\"\"Create the transition tensor.\"\"\"\n    return substitute_in_array(T, {pc: self.pc, pr: self.pr}).astype(float)\n    \nRiskRewardDilemma.create_TransitionTensor = create_TransitionTensor\n\n\n\nRewards | Short-term welfare\nThe rewards or welfare the agent receives represent the ecosystem services the environment provides. It is modeled by three parameters: a safe reward \\(r_s\\), a risky reward \\(r_r&gt;r_s\\), and a degraded reward \\(r_d&lt;r_s\\). We assume the following default values,\n\nrs, rr, rd = sp.symbols('r_s r_r r_d')\n\nWe implement the rewards as a four-dimensional array or tensor, with dimensions \\(N \\times Z \\times M \\times Z\\), where \\(N=1\\) is the number of agents, \\(Z\\) is the number of states and \\(M\\) is the number of actions. The additional agent dimension is necessary to accommodate multi-agent environments.\n\nR = np.zeros((1,2,2,2), dtype=object)\n\nThe cautious action at the prosperous state guarantees the safe reward, \\(R(\\mathsf{p,c,p}) = r_s\\),\n\nR[0,p,c,p] = rs\n\nThe risky action at the prosperous leads to the risky reward if the environment does not collapse, \\(R(\\mathsf{p,r,p}) = r_r\\),\n\nR[0,p,r,p] = rr\n\nYet, whenever the environment enters, remains, or leaves the degraded state, it provides only the degraded reward \\(R(\\mathsf{d,:,:}) = R(\\mathsf{:,:,d}) = r_d\\), where \\(:\\) denotes all possible states and actions.\n\nR[0,d,:,:] = R[0,:,:,d] = rd\n\nTogether, the reward tensor looks as follows:\n\nsp.Array(R)\n\n\\(\\displaystyle \\left[\\left[\\begin{matrix}\\left[\\begin{matrix}r_{s} & r_{d}\\\\r_{r} & r_{d}\\end{matrix}\\right] & \\left[\\begin{matrix}r_{d} & r_{d}\\\\r_{d} & r_{d}\\end{matrix}\\right]\\end{matrix}\\right]\\right]\\)\n\n\nAgain, we use the substitute_in_array function to give the risk-reward dilemma class its reward function:\n\ndef create_RewardTensor(self):\n    \"\"\"Create the reward tensor.\"\"\"\n    return substitute_in_array(\n        R, {rr: self.rr, rs: self.rs, rd: self.rd}).astype(float)\n    \nRiskRewardDilemma.create_RewardTensor = create_RewardTensor\n\n\n\nInit method\n\ndef __init__(self, CollapseProbability, RecoveryProbability, \n             RiskyReward, SafeReward, DegradedReward, state=0):\n    self.N = 1; self.M = 2; self.Z = 2\n    \n    self.pc = CollapseProbability\n    self.pr = RecoveryProbability\n    self.rr = RiskyReward\n    self.rs = SafeReward\n    self.rd = DegradedReward\n\n    self.StateSet = self.obtain_StateSet()\n    self.ActionSets = self.obtain_ActionSets()\n    self.TransitionTensor = self.create_TransitionTensor()\n    self.RewardTensor = self.create_RewardTensor()\n    \n    self.state = state\nRiskRewardDilemma.__init__ = __init__\n\nBasic testing\n\nenv = RiskRewardDilemma(0.11, 0.4, 1.0, 0.8, 0.0)\nenv.TransitionTensor\n\narray([[[1.  , 0.  ],\n        [0.89, 0.11]],\n\n       [[0.4 , 0.6 ],\n        [0.  , 1.  ]]])\n\n\n\nenv.RewardTensor\n\narray([[[[0.8, 0. ],\n         [1. , 0. ]],\n\n        [[0. , 0. ],\n         [0. , 0. ]]]])\n\n\n\nenv.step([1])\n\n([0], array([1.]), {'state': 0})\n\n\n\n\nTesting the interface\n\nagent = BehaviorAgent(policy=np.ones((2,2)))\nenv = RiskRewardDilemma(0.2, 0.1, 1.0, 0.8, 0.0)\ninterface_run([agent], env, 10)\n\nObviously, this is not very insightful. We need to track the learning process.\nWe need to track the learning process. We can do this by storing the actions, observations, and rewards in a pandas DataFrame. Pandas is a powerful data manipulation library in Python that provides data structures and functions to work with structured data. We will store the data of each time step into a row and its attributes into a set of respective columns of the DataFrame.\n\ndef interface_run(agent, env, NrOfTimesteps):\n    \"\"\"Run the multi-agent environment for several time steps.\"\"\"\n\n    columns = [\"action\", \"observation\", \"reward\"]\n    df = pd.DataFrame(index=range(NrOfTimesteps), columns=columns)\n\n    observations = env.observe()\n    \n    for t in range(NrOfTimesteps):\n                \n        action = agent.act(observations[0])\n        \n        next_observations, rewards, info = env.step([action])\n\n        agent.update(observations[0], action, \n                     rewards[0], next_observations[0])\n\n        df.loc[t] = (action, observations[0], rewards[0])\n        \n        observations = next_observations\n\n    return df\n\n\ndf = interface_run(agent, env, 25)\ndf.tail()\n\n\n\n\n\n\n\n\naction\nobservation\nreward\n\n\n\n\n20\n0\n0\n0.8\n\n\n21\n0\n0\n0.8\n\n\n22\n0\n0\n0.8\n\n\n23\n1\n0\n1.0\n\n\n24\n1\n0\n0.0\n\n\n\n\n\n\n\n\ndef plot_ActionsRewardsObservations(df):\n    fig, axes = plt.subplots(3,1, figsize=(10,5))\n    \n    axes[0].plot(df.action, 'o', label='Agent 0')\n    axes[0].set_ylabel('Action')\n    axes[0].set_yticks([0, 1])\n    axes[0].set_yticklabels([env.ActionSets[0][0], env.ActionSets[0][1]])\n    \n    axes[1].plot(df.reward, 'o', label='Agent 0');\n    axes[1].set_ylabel('Reward')\n    \n    axes[2].plot(df.observation, 'o', label='Agent 0');\n    axes[2].set_ylabel('Observation')\n    axes[2].set_yticks([0, 1])\n    axes[2].set_yticklabels([env.StateSet[0], env.StateSet[1]]);\n\n\ndf = interface_run(agent, env, 25); plot_ActionsRewardsObservations(df)\n\n\n\n\nAction-Reward-Observation Dynamics",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Individual learning</span>"
    ]
  },
  {
    "objectID": "04.02-IndividualLearning.html#reinforcement-learning-agent",
    "href": "04.02-IndividualLearning.html#reinforcement-learning-agent",
    "title": "10  Individual learning",
    "section": "10.4 Reinforcement learning agent",
    "text": "10.4 Reinforcement learning agent\nAgents act (oftentimes to reach a goal). We need to specify their\n\nActions, describing which choices are available to the agent.\nGoal, describing what an agent wants (in the long run). They may contain intrinsic motivations.\nRepresentation, e.g., defining upon which conditions agents select actions (e.g., history of past actions in multi-agent situations).\nValue beliefs (value functions), capturing what is good for the agent regarding its goal in the long run.\nBehavioral rule (policy, strategy), defining how to select actions.\nLearning rule, describing how value beliefs are updated in light of new information.\n(optionally), a model of the environment and rewards. Models are used for planning, i.e., deciding on a behavioral rule by considering possible future situations before they are actually experienced.\n\nAgents act (oftentimes to reach a goal). As in Lecture 03.01-SequentialDecision, the agent aims to maximize the discounted sum of future rewards,\n\\[ G_t = (1-\\gamma) \\sum_{\\tau=0}^\\infty \\gamma^\\tau R_{t+\\tau},\\]\nwhere \\(1-\\gamma\\) is a normalizing factor and \\(R_{t+\\tau+1}\\) is the reward received at time step \\(t+\\tau+1\\).\nHowever, in contrast to Lecture 03.01-SequentialDecision, we assume that the agent does not know the environment’s dynamics and rewards. Instead, the agent learns about the environment while interacting with it.\nThe challenge is that actions may have delayed and uncertain consequences.\nDelayed consequences mean that an action may influence the environmental state, which, in turn, influences the reward the agent receives at a later time step. For example, in our risk-reward dilemma, opting for a sustainable policy may initially reduce the agent’s immediate reward but ensures a comparably higher long-term welfare. Uncertain consequences refer to the stochasticity in the environmental transitions (and possibly the reward signals themselves). For example, in our risk-reward dilemma, the risky action in the prosperous state may lead to a high reward but may also cause a transition to the degraded state. Moreover, uncertainty may also refer to the fact that the environmental transition dynamics may change over time.\nThus, the agent can’t just try each action in each state once and then immediately know which course of action is best. It must learn the best course of action over successive trials, each of which gives possible noisy data.\nWe start implement the learning class by defining __init__ method.\n\nclass Learner():\n    \"\"\"A simple reinforcement learning agent.\"\"\"   \n\n    def __init__(self, ValueBeliefs_Qoa,\n                 DiscountFactor, LearningRate, ChoiceIntensity):\n\n        self.DiscountFactor = self.df = DiscountFactor\n        self.LearningRate = self.lr = LearningRate\n        self.ChoiceIntensity = self.ci = ChoiceIntensity\n\n        self.ValueBeliefs_Qoa = ValueBeliefs_Qoa\n        \n        self.ActionIxs = range(ValueBeliefs_Qoa.shape[1])\n\nThe agent receives the following parameters: the initial value beliefs ValueBeliefs_Qoa, the discount factor DiscountFactor, the learning rate LearningRate, and the choice intensity ChoiceIntensity. Furthermore, we give the agent an attribute ActionIxs that stores the indices of the possible actions. This will be helpful when selecting actions.\n\nValue beliefs\nThe general strategy we focus on to solve the challenges of delayed and uncertain consequences is to let the agent learn value beliefs. Value beliefs are the agent’s estimates of the long-term value of each action \\(a\\) in each state \\(s\\). The agent then uses these value beliefs to select actions. These estimates are also often called Q values. You may think of the quality of an action \\(a\\) in state \\(s\\) which tells the agents Which action to select in which state.\nFor example, in our risk-reward dilemma, we can represent the agent’s value beliefs by\n\nValueBeliefs_Qoa = 10 * np.random.rand(2,2)\nValueBeliefs_Qoa\n\narray([[8.83140215, 5.58254363],\n       [8.59330674, 1.20765508]])\n\n\nThe challenge of uncertain consequences in then solved by an appropriate behavioral rule which handles the so-called exploration-exploitation trade-off\nThe challenge of delayed consequences is solved by the learning rule, which updates the value beliefs in light of new information using the Bellman equation, as in Lecture 03.01-SequentialDecisions.\n\n\nBehavioral rule | Exploration-exploitation trade-off\nThe exploration-exploitation trade-off poses a fundamental problem for decision-making under uncertainty.\nUnder too much exploitation, the agent may pick an action that is not optimal, as it has not yet sufficiently explored all possible actions. It acts under the false belief that its current value beliefs are already correct or optimal. Thus, it loses out on possible rewards it would have gotten if it had explored more and discovered that a different course of action is better.\nUnder too much exploration, the agent may continue to try all actions to gain as much information about the transitions and reward distributions as possible. It is losing out because it never settles on the best course of action, continuing to pick all actions until the end.\nWhat is needed is a behavioral rule that balances exploitation and exploration to explore enough to find the best option but not too much so that the best option is exploited as much as possible.\nWe use the so-called softmax function,\n\\[x(s, a) = \\frac{\\exp \\beta Q(s, a)}{\\sum_{b \\in \\mathcal A}\\exp \\beta Q(s, b)},\\]\nwhich converts any set of value beliefs into probabilities that sum to one.\nThe higher the relative value belief, the higher the relative probability.\n\ndef obtain_softmax_probabilities(ChoiceIntensity, ValueBeliefs):\n    expValueBeliefs = np.exp(ChoiceIntensity*np.array(ValueBeliefs))\n    return expValueBeliefs / expValueBeliefs.sum(-1, keepdims=True)\n\nThe softmax function contains a parameter \\(\\beta\\), denoting the choice intensity (sometimes called inverse temperature, that determines how exploitative (or greedy) the agent is.\nWhen \\(\\beta = 0\\), arms are chosen entirely at random with no influence of the Q values. This is super exploratory, as the agent continues to choose all arms irrespective of observed rewards.\n\nobtain_softmax_probabilities(0, ValueBeliefs_Qoa)\n\narray([[0.5, 0.5],\n       [0.5, 0.5]])\n\n\nAs \\(\\beta\\) increases, there is a higher probability of picking the arm with the highest Q value. This is increasingly exploitative (or ‘greedy’).\n\nobtain_softmax_probabilities(1, ValueBeliefs_Qoa)\n\narray([[9.62632074e-01, 3.73679265e-02],\n       [9.99380298e-01, 6.19702179e-04]])\n\n\nWhen \\(\\beta\\) is very large, then only the arm that currently has the highest Q value will be chosen, even if other arms might actually be better.\n\nobtain_softmax_probabilities(50, ValueBeliefs_Qoa).round(9)\n\narray([[1., 0.],\n       [1., 0.]])\n\n\nFor example, assuming we are in state zero and using \\(\\beta=1\\), we can select an action by\n\nobs = 0\nXoa = obtain_softmax_probabilities(1, ValueBeliefs_Qoa)\nnp.random.choice([0,1], p=Xoa[obs])\n\n0\n\n\nWe summarize this logic in the agent’s act method:\n\ndef act(self, obs):\n    Xoa = self.obtain_policy_Xoa()\n    return np.random.choice(self.ActionIxs, p=Xoa[obs])\nLearner.act = act\n\nwhere we define the ActionIxs as range(self.NrActions) in the __init__ method of the agent. We also define the obtain_policy_Xoa method in the agent class:\n\ndef obtain_policy_Xoa(self):\n    return obtain_softmax_probabilities(self.ChoiceIntensity, self.ValueBeliefs_Qoa)\nLearner.obtain_policy_Xoa = obtain_policy_Xoa\n\nTesting the act and obtain_policy_Xoa methods:\n\nlearner = Learner(np.ones((2,2)), 0.9, 0.1, 1.0)\n\n\nlearner.obtain_policy_Xoa()[0]\n\narray([0.5, 0.5])\n\n\nSelecting action uniformly at random for 1000 times should give a mean index of approx. 0.5:\n\nnp.mean([learner.act(0) for _ in range(1000)])\n\n0.479\n\n\n\n\nLearning rule | Temporal-difference learning\nThe learning rule solves the challenge of delayed consequences. The value beliefs are updated using the Bellman equation in light of new information. As the Bellman equation describes how state(-action) values relate at different timesteps, this reinforcement learning update class is called temporal-difference learning.\nGiven an observed state \\(s\\), the agent selects an action \\(a\\) and receives a reward \\(r\\). Then, the agent updates its value beliefs (for state-action pair \\(s\\)-\\(a\\)) according to\n\\[Q_{t+1}(s, a) = Q_{t}(s, a) + \\alpha\\left((1-\\gamma)r + \\gamma \\sum_b x_t(s', b) Q_t(s',b) - Q_{t}(s, a)\n\\right). \\tag{10.1}\\]\nDeepDive | There is some freedom into designing the specifics of the temporal-difference update, especially regarding estimating the value of the next state or observation. The specific update used above is called Expected SARSA. It is beyond the scope of this course to discuss the different temporal-difference learning algorithms. The interested reader is referred to excellent material on (multi-agent) reinforcement learning, e.g., (Albrecht et al., 2024; Sutton & Barto, 2018).\nThe extent to which the value beliefs are updated is controlled by a second parameter, \\(\\alpha \\in (0, 1)\\), called the learning rate.\nWhen \\(\\alpha=0\\), there is no updating, and the reward does not affect value beliefs,\n\\[Q_{t+1}(s, a) = Q_{t}(s, a) =: \\text{old estimate}.\\]\nThe value belief update always remains the old estimate of the value beliefs.\nWhen \\(\\alpha = 1\\), the value belief for the state-action pair (\\(s, a\\)) becomes a discount-factor weighted average between the current reward \\(r\\) and the expected value of the next state \\(\\sum_b x_t(s', b) Q_t(s',b)\\),\n\\[Q_{t+1}(s, a) = (1-\\gamma)r + \\gamma \\sum_b x_t(s', b) Q_t(s',b) =: \\text{new estimate}.\\]\nThe value belief update is entirely determined by the new estimate of the value beliefs, which is the current reward received \\(r\\) plus the discount factor \\(\\gamma\\) multiplied by the expected value of the following state \\(\\sum_b x_t(s', b) Q_t(s',b)\\), and adequately normalized with the prefactor \\((1-\\gamma)\\).\nWhen \\(0 &lt; \\alpha &lt; 1\\), the new value belief for the rewarded arm is a weighted average between old value belief and new reward information,\n\\[\\begin{align}\nQ_{t+1}(s, a) &= (1-\\alpha) \\ \\text{old estimate} &+ &\\alpha \\ \\text{new estimate}\\\\\n&= (1-\\alpha) \\ Q_{t}(s, a) &+ &\\alpha \\left((1-\\gamma)r + \\gamma \\sum_b x_t(s', b) Q_t(s',b)\\right).\n\\end{align}\\]\nOnce more, we face a trade-off. Clearly, setting \\(\\alpha = 0\\) is ineffective since the agent does not acquire knowledge. Yet, if \\(\\alpha\\) is excessively high; the agent tends to forget previously learned state-action information.\nTemporal-difference reward-prediction error. Another way to think about the update equation (Equation 10.1) is as follows: The value beliefs are updated by the temporal-difference reward-prediction error (TDRP error) times the learning rate. The TDRP error equals the difference between the new estimate and the old estimate of the value beliefs. If the TDRP error is zero, the agent correctly predicted the next reward, and thus, no further adjustments in the value beliefs are necessary.\n\\[\\begin{align}\nQ_{t+1}(s, a) &= Q_{t}(s, a) + \\alpha \\qquad\\qquad\\qquad\\qquad\\qquad \\text{TDRP-error}, \\\\\n&= Q_{t}(s, a) + \\alpha \\Big( \\qquad \\qquad   \\text{new estimate} \\qquad \\qquad \\ - \\text{old estimate} \\Big), \\\\\n&= Q_{t}(s, a) + \\alpha \\Big( (1-\\gamma)r + \\gamma \\sum_b x_t(s', b) Q_t(s',b) - \\quad Q_{t}(s, a) \\quad \\Big)\n\\end{align}\\]\nDeepDive | The exact terminology temporal-difference reward-prediction error is used rather rarely. We use it here to express the interdisciplinary nature of temporal-difference reward-prediction learning. In machine learning, the term temporal-difference error is more common. It describes the difference between the predicted and the observed reward. In psychology and neuroscience, the term reward-prediction error is used in the context of the brain’s dopamine system, where it is thought to signal the difference between the expected and the observed reward. The term temporal-difference reward-prediction error combines both terms, expressing the idea that the agent learns by predicting future rewards.\nExecuting the value belief update in Python may look like\n\ndef update(self, \n           obs: int,\n           action: int,\n           reward: float,\n           next_obs: int,\n          ):\n    \"\"\"Updates the value beliefs / Q-value of an action.\"\"\"\n\n    temporal_difference =self.obtain_temporal_difference(\n        obs, action, reward, next_obs)\n    \n    self.ValueBeliefs_Qoa[obs, action] = (\n        self.ValueBeliefs_Qoa[obs, action] + self.LearningRate * temporal_difference\n    )\n\nLearner.update = update\n\nWe program the update method highly modular. It calls the obtain_temporal_difference method to compute the temporal-difference error and then updates the value beliefs accordingly. The obtain_temporal_difference method is defined as follows:\n\ndef obtain_temporal_difference(self,\n                               obs: int,\n                               action: int,\n                               reward: float,\n                               next_obs: int,\n                              ):\n    \"\"\"Compute temporal-difference eorror\"\"\"\n    next_Qoa = self.obtain_nextQoa(next_obs)\n    new_estimate = (1-self.DiscountFactor) * reward + self.DiscountFactor * next_Qoa\n    old_estimate = self.ValueBeliefs_Qoa[obs][action]\n    return new_estimate - old_estimate\nLearner.obtain_temporal_difference = obtain_temporal_difference\n\nIn here, we call the obtain_nextQoa method to compute the expected value of the next state. The obtain_nextQoa method is defined as follows:\n\ndef obtain_nextQoa(self, next_obs: int):\n    policy_Xoa = self.obtain_policy_Xoa()\n    return np.sum(policy_Xoa[next_obs] * self.ValueBeliefs_Qoa[next_obs])\nLearner.obtain_nextQoa = obtain_nextQoa\n\nTesting the update method: First, let’s assume the agent does not care about future rewards at all and has a discount factor of zero\n\nlearner = Learner(ValueBeliefs_Qoa = np.ones((2,2)), \n                  DiscountFactor = 0.0,\n                  LearningRate = 0.1,\n                  ChoiceIntensity = 1.0)\n\n\nlearner.ValueBeliefs_Qoa\n\narray([[1., 1.],\n       [1., 1.]])\n\n\nLet’s assume the agent selected the action with index 0 after observing the state with index 0, received a reward of zero, and observed the next state with index 1.\n\nlearner.update(obs=0, action=0, reward=0, next_obs=1)\nlearner.ValueBeliefs_Qoa.round(4)\n\narray([[0.9, 1. ],\n       [1. , 1. ]])\n\n\nThe value belief for the action 0 in state 0 is updated exactly as a learning rate weighted average: \\(\\alpha \\cdot \\text{new estimate} + (1-\\alpha) \\cdot \\text{old estimate}\\) \\(= \\alpha 0 + (1-\\alpha) 1\\) \\(= 0.1 \\cdot 0 + 0.9 \\cdot 1\\).\nRepeating this update a hundred more time steps updates the value beliefs for the action 0 in state 0 to the expected value of zero.\n\nfor _ in range(100): learner.update(obs=0, action=0, reward=0, next_obs=1)\nlearner.ValueBeliefs_Qoa.round(4)\n\narray([[0., 1.],\n       [1., 1.]])\n\n\nNow, we repeat that test, but with an agent with a discount factor of \\(\\gamma=0.8\\).\n\nlearner = Learner(ValueBeliefs_Qoa = 1*np.ones((2,2)), \n                  DiscountFactor = 0.8,\n                  LearningRate = 0.1,\n                  ChoiceIntensity = 1.0)\n\n\nlearner.ValueBeliefs_Qoa\n\narray([[1., 1.],\n       [1., 1.]])\n\n\n\nfor _ in range(100): learner.update(obs=0, action=0, reward=0, next_obs=1)\nlearner.ValueBeliefs_Qoa.round(4)\n\narray([[0.8, 1. ],\n       [1. , 1. ]])\n\n\nNow, the value belief for the action 0 in state 0 is updated to \\(0.8\\). Can you explain why?\nWe have convince ourselves that the learner’s update methods works as we expect.\nNow, we are ready to let it learn in the risk-reward dilemma environment.\n\n\nTesting the interface\n\nlearner = Learner(ValueBeliefs_Qoa = np.ones((2,2)), \n                  DiscountFactor = 0.9,\n                  LearningRate = 0.1,\n                  ChoiceIntensity = 1.0)\nprint(learner.obtain_policy_Xoa())\n\nenv = RiskRewardDilemma(CollapseProbability=0.2, RecoveryProbability=0.1, \n                        SafeReward=0.8, RiskyReward=1.0, DegradedReward=0.0)\n\nprint(\" - - - - \")\ndf = interface_run(learner, env, 10000)\nprint(\" - - - - \")\n\nprint(learner.obtain_policy_Xoa())\n\n[[0.5 0.5]\n [0.5 0.5]]\n - - - - \n - - - - \n[[0.5091271  0.4908729 ]\n [0.50860671 0.49139329]]\n\n\nThe learning agent’s policy changed. However, not too much. We know from Lecture 03.01-SequentialDecisions that the agent should learn to prefer the cautious action in both states under these parameter settings.\nTry re-executing the above cell while make some changes to the parameters. Can you get an intuition what is important for a successful learning process?\nThe process of finding the right parameters for the agent is called hyperparameter tuning. It is a crucial step in machine learning and often requires a lot of trial and error.\nFrom a modeling point of view, we aim to go beyond finding the right parameter. We aim to understand how the parameters influence the learning process.\nComparing the initial with the final policy is not the best way to facilitate both aims. We need a more refined way to keep track of the learning process.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Individual learning</span>"
    ]
  },
  {
    "objectID": "04.02-IndividualLearning.html#investigating-the-learning-process",
    "href": "04.02-IndividualLearning.html#investigating-the-learning-process",
    "title": "10  Individual learning",
    "section": "10.5 Investigating the learning process",
    "text": "10.5 Investigating the learning process\nTo keep track of the learning process, we store the value beliefs and the policy in a pandas DataFrame. Aditionally, we also record the learning rate and the choice intensity.\n\ndef interface_run(agent, env, NrOfTimesteps):\n    \"\"\"Run the multi-agent environment for several time steps.\"\"\"\n\n    columns = [\"action\", \"observation\", \"reward\", \"beliefs\", \"policy\",\n               \"ChoiceIntensity\", \"LearningRate\"]\n    df = pd.DataFrame(index=range(NrOfTimesteps), columns=columns)\n\n    observations = env.observe()\n    \n    for t in range(NrOfTimesteps):\n                \n        action = agent.act(observations[0])\n        \n        next_observations, rewards, info = env.step([action])\n\n        agent.update(observations[0], action, \n                     rewards[0], next_observations[0])\n\n        df.loc[t] = (action, next_observations[0], rewards[0],\n                     deepcopy(agent.ValueBeliefs_Qoa), \n                     deepcopy(agent.obtain_policy_Xoa()),\n                     deepcopy(agent.ChoiceIntensity), \n                     deepcopy(agent.LearningRate))\n\n        observations = next_observations\n\n    return df\n\n\nlearner = Learner(ValueBeliefs_Qoa = 0*np.ones((2,2)), \n                  DiscountFactor = 0.9,\n                  LearningRate = 0.05,\n                  ChoiceIntensity = 8.0)\n\nprint(learner.obtain_policy_Xoa())\n\nenv = RiskRewardDilemma(CollapseProbability=0.2, RecoveryProbability=0.1, \n                        SafeReward=0.8, RiskyReward=1.0, DegradedReward=0.0)\n\ndf = interface_run(learner, env, 10000)\n\n[[0.5 0.5]\n [0.5 0.5]]\n\n\nAs we stored the value beliefs and policies as two-dimensional numpy arrays, we convert them into three-dimensional numpy with time running on the first dimension:\n\nbeliefs_Qtoa = np.array(df.beliefs.values.tolist())\npolicy_Xtoa = np.array(df.policy.values.tolist())\nbeliefs_Qtoa.shape, policy_Xtoa.shape\n\n((10000, 2, 2), (10000, 2, 2))\n\n\nWe include these conversions into a plotting function that visualizes the learning process\n\ndef plot_learning_process(df, plot_varying_parameters=False):\n    beliefs_Qtoa = np.array(df.beliefs.values.tolist())\n    policy_Xtoa = np.array(df.policy.values.tolist())\n    beliefs_Qtoa.shape, policy_Xtoa.shape\n\n    fig = plt.figure(figsize=(14,6))\n    \n    ax0 = fig.add_subplot(311)\n    ax0.plot(beliefs_Qtoa[:,p,c], label='Q(p,c)', color='blue', lw=2)\n    ax0.plot(beliefs_Qtoa[:,p,r], label='Q(p,r)', color='red', lw=2)\n    ax0.plot(beliefs_Qtoa[:,d,c], label='Q(d,c)', color='darkblue', ls='--')\n    ax0.plot(beliefs_Qtoa[:,d,r], label='Q(d,r)', color='darkred', ls='--')\n    ax0.set_ylabel('Value beliefs');\n    ax0.legend(loc='center right'); ax0.set_xlim(-10, len(df)*1.1)\n    \n    ax1 = fig.add_subplot(312, sharex=ax0)\n    ax1.plot(policy_Xtoa[:,p,c], label='X(p,c)', color='blue', lw=2)\n    ax1.plot(policy_Xtoa[:,p,r], label='X(p,r)', color='red', lw=2)\n    ax1.plot(policy_Xtoa[:,d,c], label='X(d,c)', color='darkblue', ls='--')\n    ax1.plot(policy_Xtoa[:,d,r], label='X(d,r)', color='darkred', ls='--')\n    ax1.set_ylabel('Policy'); ax1.set_xlabel('Time steps')\n    ax1.legend(loc='center right')\n\n    if plot_varying_parameters:\n        ax2 = fig.add_subplot(615, sharex=ax0)\n        ax2.plot(df.LearningRate, label='LearningRate', color='k')\n        ax2.set_ylabel(\"Learning\\nRate\")\n        \n        ax3 = fig.add_subplot(616, sharex=ax0)\n        ax3.plot(df.ChoiceIntensity, label='ChoiceIntensity', color='k')\n        ax3.set_ylabel(\"Choice\\nIntensity\"), ax3.set_xlabel('Time steps')\n        \n    # plt.tight_layout()\n    plt.subplots_adjust(hspace=0.35)\n    \n    # plt.legend()\n    ax0.set_ylim(0, 1);\n\n\nTo little exploitation | To much exploration\n\nlearner = Learner(ValueBeliefs_Qoa = 0*np.ones((2,2)), \n                  DiscountFactor = 0.9,\n                  LearningRate = 0.1,\n                  ChoiceIntensity = 1.0)\n\nenv = RiskRewardDilemma(CollapseProbability=0.2, RecoveryProbability=0.1, \n                        SafeReward=0.8, RiskyReward=1.0, DegradedReward=0.0)\n\ndf = interface_run(learner, env, 10000)\n\n\nplot_learning_process(df)\n\n\n\n\nLearning the risk-reward dilemma with to much exploration.\n\n\n\n\nThe order of the value beliefs seems roughly consistent with the optimal policy, that prefers the cautious action over the risky on in both states. However, the agents policy is fluctuating around. The action choice probabilities are fluctuating around their uniformly random value of 0.5. This is a sign that the agent explores too much and exploits too little.\n\n\nTo much exploitation | To little exploration\n\nlearner = Learner(ValueBeliefs_Qoa = 0*np.ones((2,2)), \n                  DiscountFactor = 0.9,\n                  LearningRate = 0.1,\n                  ChoiceIntensity = 100.0)\n\nenv = RiskRewardDilemma(CollapseProbability=0.2, RecoveryProbability=0.1, \n                        SafeReward=0.8, RiskyReward=1.0, DegradedReward=0.0)\n\ndf = interface_run(learner, env, 10000)\n\n\nplot_learning_process(df)\n\n\n\n\nLearning the risk-reward dilemma with to much exploitation.\n\n\n\n\nIncreasing the choice intensity leads to a more exploitative policy. However, what the agent learns depends on the stochasticity of the learning process. Try to convince yourself of that fact by re-executing the above cell multiple times.\nIn other words, the agent may not learn the optimal policy. To little exploration harms the learning.\n\n\nDecaying exploration | Increasing exploitation\nTo mitigate the negative effects of too much exploration towards the end of the learning process and the negative effects of too much exploitation towards the beginning of the learning process, we could let the choice intensity increase over time. This is called decaying exploration.\nWe implement this idea by creating a new agent class AdjustingLearner that inherits from the Learner class. This shows the power of object-oriented programming. We overwrite the update method to include an increasing choice intensity. We also make the learning rate decay over time.\n\nclass AdjustingLearner(Learner):\n\n    def __init__(self, \n                 ValueBeliefs_Qoa, \n                 DiscountFactor, \n                 LearningRate, MinLearningRate, LearningRateDecayFactor,\n                 ChoiceIntensity, MaxChoiceIntensity, ChoiceIntensityGrowthFactor,\n                ):\n\n        self.DiscountFactor = DiscountFactor\n        \n        self.LearningRate = LearningRate\n        self.MinLearningRate = MinLearningRate\n        self.LearningRateDecayFactor = LearningRateDecayFactor\n        \n        self.ChoiceIntensity = ChoiceIntensity\n        self.MaxChoiceIntensity = MaxChoiceIntensity\n        self.ChoiceIntensityGrowthFactor = ChoiceIntensityGrowthFactor\n\n        self.ValueBeliefs_Qoa = ValueBeliefs_Qoa\n        self.ActionIxs = range(ValueBeliefs_Qoa.shape[1])\n\n\ndef update(self, \n           obs: int,\n           action: int,\n           reward: float,\n           next_obs: int,\n          ):\n    \"\"\"Updates the value beliefs / Q-value of an action.\"\"\"\n\n    temporal_difference =self.obtain_temporal_difference(\n        obs, action, reward, next_obs)\n    \n    self.ValueBeliefs_Qoa[obs, action] = (\n        self.ValueBeliefs_Qoa[obs, action] + self.LearningRate * temporal_difference\n    )\n\n    self.LearningRate = max(self.MinLearningRate, \n                            self.LearningRate * self.LearningRateDecayFactor)\n    self.ChoiceIntensity = min(self.MaxChoiceIntensity, \n                               self.ChoiceIntensity * self.ChoiceIntensityGrowthFactor)\n\nAdjustingLearner.update = update\n\nNow, we are ready to perform a new learning simulation.\n\nlearner = AdjustingLearner(ValueBeliefs_Qoa = 0*np.ones((2,2)), \n                           DiscountFactor = 0.9,\n                           LearningRate = 0.1, MinLearningRate = 0.01, LearningRateDecayFactor = 0.999,\n                           ChoiceIntensity = 1.0, MaxChoiceIntensity = 50.0, ChoiceIntensityGrowthFactor = 1.001)\n\nenv = RiskRewardDilemma(CollapseProbability=0.2, RecoveryProbability=0.1, \n                        SafeReward=0.8, RiskyReward=1.0, DegradedReward=0.0)\n\ndf = interface_run(learner, env, 10000)\n\n\nplot_learning_process(df, plot_varying_parameters=True)\n\n\n\n\nLearning the risk-reward dilemma with an increasing choice intensity.\n\n\n\n\nWe find the AdjustingLearner is able to consitently learn the optimal policy. Try to convince yourself that this is true by re-executing the simulation above multiple times. We also find that it learns the optimal policy in approx. less than 4000 time steps. How does the learning process depend on the additional parameters?\nObviously, the AdjustingLearner is a more complex agent than the Learner. There is also a prominent trick to give our simpler Learner agent an initial exploration bonus.\n\n\nInitial exploration bonus\nWe can give the agent an initial exploration bonus by setting the initial value beliefs to a high value. This is called optimistic initialization.\n\nlearner = Learner(ValueBeliefs_Qoa = 8*np.ones((2,2)), \n                  DiscountFactor = 0.9,\n                  LearningRate = 0.1,\n                  ChoiceIntensity = 60.0)\n\nenv = RiskRewardDilemma(CollapseProbability=0.2, RecoveryProbability=0.1, \n                        SafeReward=0.8, RiskyReward=1.0, DegradedReward=0.0)\n\ndf = interface_run(learner, env, 10000)\n\n\nplot_learning_process(df)\n\n\n\n\nLearning the risk-reward dilemma with an initial exploration bonus.\n\n\n\n\nSo far, we have investigated the learning process in a single environment.\nNext, we will explore learning in normal-form games to illustrate the modularity of the agent-environment interface, utilizing the same learning agents as previously discussed.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Individual learning</span>"
    ]
  },
  {
    "objectID": "04.02-IndividualLearning.html#multi-agent-environments-games",
    "href": "04.02-IndividualLearning.html#multi-agent-environments-games",
    "title": "10  Individual learning",
    "section": "10.6 Multi-agent environments | Games",
    "text": "10.6 Multi-agent environments | Games\n\nInterface\nFirst, we adjust the interface_run function to work with the multi-agent environment.\nWe can make the columns of a dataframe adaptive to the number of agents.\n\nNrAgents = 2; NrOfTimesteps = 4\n\ndef create_dataframe(NrAgents, NrOfTimesteps):\n    columns = list(np.array([(f\"action{i}\", f\"observation{i}\", f\"reward{i}\",\n                              f\"beliefs{i}\", f\"policy{i}\", \n                              f\"ChoiceIntensity{i}\", f\"LearningRate{i}\") \n                             for i in range(NrAgents)]).flatten())\n    \n    return pd.DataFrame(index=range(NrOfTimesteps), columns=columns)\n\ncreate_dataframe(NrAgents, NrOfTimesteps)\n\n\n\n\n\n\n\n\naction0\nobservation0\nreward0\nbeliefs0\npolicy0\nChoiceIntensity0\nLearningRate0\naction1\nobservation1\nreward1\nbeliefs1\npolicy1\nChoiceIntensity1\nLearningRate1\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nWe also write a function to populate the DataFrame with the values of the learning process.\n\ndef fill_dataframe(actions, next_observations, rewards, agents):\n    data = []\n    for i in range(len(agents)):\n        data += [actions[i], next_observations[i], rewards[i],\n                 deepcopy(agents[i].ValueBeliefs_Qoa), \n                 deepcopy(agents[i].obtain_policy_Xoa()),\n                 deepcopy(agents[i].ChoiceIntensity), \n                 deepcopy(agents[i].LearningRate)]\n    return data\n\nAdjusting the interface_run function to work with the multi-agent environment yields a clean and readable implementation.\n\ndef interface_run(agents, env, NrOfTimesteps):\n    \"\"\"Run the multi-agent environment for several time steps.\"\"\"\n    df = create_dataframe(len(agents), NrOfTimesteps)\n\n    observations = env.observe()\n    \n    for t in range(NrOfTimesteps):\n                \n        actions = [agent.act(observations[i])\n            for i, agent in enumerate(agents)]\n\n        next_observations, rewards, info = env.step(actions)\n\n        for i, agent in enumerate(agents):\n            agent.update(observations[i], actions[i], rewards[i], next_observations[i])\n    \n        df.loc[t] = fill_dataframe(actions, next_observations, rewards, agents)\n\n        observations = next_observations\n    \n    return df\n\nTesting whether it works in our previous environment, the risk-reward dilemma looks promising.\n\nlearner = Learner(ValueBeliefs_Qoa = 8*np.ones((2,2)), \n                  DiscountFactor = 0.9,\n                  LearningRate = 0.1,\n                  ChoiceIntensity = 60.0)\n\nenv = RiskRewardDilemma(CollapseProbability=0.2, RecoveryProbability=0.1, \n                        SafeReward=0.8, RiskyReward=1.0, DegradedReward=0.0)\n\n# Note: we need to pass the learner as a list of agents\ndf = interface_run([learner], env, 4) \n\n\ndf\n\n\n\n\n\n\n\n\naction0\nobservation0\nreward0\nbeliefs0\npolicy0\nChoiceIntensity0\nLearningRate0\n\n\n\n\n0\n1\n0\n1.0\n[[8.0, 7.93], [8.0, 8.0]]\n[[0.9852259683067277, 0.014774031693272396], [...\n60.0\n0.1\n\n\n1\n0\n0\n0.8\n[[7.927906923600332, 7.93], [8.0, 8.0]]\n[[0.46864505269071266, 0.5313549473092875], [0...\n60.0\n0.1\n\n\n2\n0\n0\n0.8\n[[7.8567279493493345, 7.93], [8.0, 8.0]]\n[[0.012172569010807209, 0.9878274309891928], [...\n60.0\n0.1\n\n\n3\n1\n0\n1.0\n[[7.8567279493493345, 7.86061972818162], [8.0,...\n[[0.4418871301229423, 0.5581128698770577], [0....\n60.0\n0.1\n\n\n\n\n\n\n\nThe real test, however, comes with a true multi-agent environment.\n\n\nSocial dilemmas environment\nLet’s extend our treatment of reinforcement learning to multiple agents. From the perspective of each individual agent, other agents make the environment non-stationary. This can complicate reinforcement learning significantly.\nWe here focus on normal-form games and use the generic model of a social dilemma, introduced in Lecture 03.02-StrategicInteractions.\n\\[\n\\begin{array}{c|cc}\n\\text{} & \\color{blue}{\\mathsf{Abate}} & \\color{blue}{\\mathsf{Pollute}} \\\\\n\\hline\n\\color{red}{\\mathsf{Abate}} & {\\color{red}{1}} \\ | \\ {\\color{blue}{1}} & {\\color{red}{-1-F}} \\ | \\ {\\color{blue}{+1+G}} \\\\\n\\color{red}{\\mathsf{Pollute}} & {\\color{red}{+1+G}} \\ | \\ {\\color{blue}{-1-F}} & {\\color{red}{-1}} \\ | \\ {\\color{blue}{-1}} \\\\\n\\end{array}\n\\]\nDepending on whether the greed \\(G\\) and fear \\(F\\) are positive or negative, we can distinguish four types of games Figure 10.3.\n\n\n\n\n\n\nFigure 10.3: Dimensions of a social dilemma with ordinal payoffs and Nash equilibira shown in boxes.\n\n\n\nIn Figure 10.3, the payoff values are ordinal, meaning that only their order, \\(3&gt;2&gt;1&gt;0\\), is considered of relevance.\nWe also implement it as a class using the same interface as before and letting it inherit from our base environment.\n\nclass SocialDilemma(Environment):\n    \"\"\"A simple social dilemma environment.\"\"\"\n\n    def obtain_StateSet(self):\n        return ['.']  # a dummy state\n\n    def obtain_ActionSets(self):\n        # abate, pollute for two agents\n        return [['a', 'p'], ['a', 'p']]  \n\nDue to the absence of environmental state transitions, the environment consistently exists in a single, effective dummy state. Consequently, the transition tensor is simplified significantly.\n\ndef create_TransitionTensor(self):\n    \"\"\"Create the transition tensor.\"\"\"\n    return np.ones((self.Z, self.M, self.M, self.Z))\n    \nSocialDilemma.create_TransitionTensor = create_TransitionTensor\n\nThe reward tensor is slighlty more complicated. The two defining parameters of the social dilemma environemtn are the greed \\(G\\) and the fear \\(F\\).\n\nF, G = sp.symbols('F G')\n\nWe represent rewards using a five-dimensional tensor with dimensions \\(N \\times Z \\times M \\times M \\times Z\\). Here, \\(N\\) denotes the number of agents, \\(Z=1\\) signifies the state count, and \\(M\\) indicates the number of actions. A uni-dimensional state dimension is essential for accommodating multi-state environments.\n\nR = np.zeros((2,1,2,2,1), dtype=object)\n\nHelper variable for the indices facilitate the construction of the reward tensor.\n\na = SocialDilemma().obtain_ActionSets()[0].index('a')\np = SocialDilemma().obtain_ActionSets()[0].index('p')\na,p\n\n(0, 1)\n\n\nMutual abatement yields a reward of one for both agents.\n\nR[0, 0, a, a, 0] = R[1, 0, a, a, 0] = 1\n\nMutual pollution yields a reward of minus one for both agents.\n\nR[0, 0, p, p, 0] = R[1, 0, p, p, 0] = -1\n\nPollution by one agent and abatement by the other agent yields a reward of one plus the greed for the polluting agent and minus the fear for the abating agent.\n\nR[0, 0, p, a, 0] = R[1, 0, a, p, 0] = 1 + G\nR[0, 0, a, p, 0] = R[1, 0, p, a, 0] = -1 - F\n\nIn sum, the reward tensor for agent zero reads,\n\nsp.Array(R[0,0,:,:,0])\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & - F - 1\\\\G + 1 & -1\\end{matrix}\\right]\\)\n\n\nand for agent one,\n\nsp.Array(R[1,0,:,:,0])\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & G + 1\\\\- F - 1 & -1\\end{matrix}\\right]\\)\n\n\n\ndef create_RewardTensor(self):\n    \"\"\"Create the reward tensor.\"\"\"\n    return substitute_in_array(\n        R, {F: self.Fear, G: self.Greed}).astype(float)\n    \nSocialDilemma.create_RewardTensor = create_RewardTensor\n\nThe two defining parameters of the social dilemma are the greed \\(G\\) and the fear \\(F\\).\n\ndef __init__(self, Greed, Fear):\n    self.N = 2; self.M = 2; self.Z = 1\n    \n    self.Greed = Greed\n    self.Fear = Fear\n    \n    self.StateSet = self.obtain_StateSet()\n    self.ActionSets = self.obtain_ActionSets()\n    self.TransitionTensor = self.create_TransitionTensor()\n    self.RewardTensor = self.create_RewardTensor()\n    \n    self.state = 0\nSocialDilemma.__init__ = __init__\n\n\n\nTesting the implementation:\n\nenv = SocialDilemma(Fear=0.65, Greed=0.75)\n\nMutual cooperation by two abating agents:\n\nenv.step([a,a])\n\n([0, 0], array([1., 1.]), {'state': 0})\n\n\nMutual defection by two polluting agents:\n\nenv.step([p,p])\n\n([0, 0], array([-1., -1.]), {'state': 0})\n\n\nDifferent actions:\n\nenv.step([a,p])\n\n([0, 0], array([-1.65,  1.75]), {'state': 0})\n\n\n\nenv.step([p,a])\n\n([0, 0], array([ 1.75, -1.65]), {'state': 0})\n\n\nTesting whether the implementation works,\n\nlearner1 = Learner(ValueBeliefs_Qoa = 8*np.ones((1,2)), \n                  DiscountFactor = 0.9,\n                  LearningRate = 0.1,\n                  ChoiceIntensity = 60.0)\nlearner2 = deepcopy(learner1)\n\nenv = SocialDilemma(Fear=1, Greed=2)\n\ndf = interface_run([learner1, learner2], env, 4) \ndf\n\n\n\n\n\n\n\n\naction0\nobservation0\nreward0\nbeliefs0\npolicy0\nChoiceIntensity0\nLearningRate0\naction1\nobservation1\nreward1\nbeliefs1\npolicy1\nChoiceIntensity1\nLearningRate1\n\n\n\n\n0\n1\n0\n3.0\n[[8.0, 7.95]]\n[[0.9525741268224331, 0.047425873177566774]]\n60.0\n0.1\n0\n0\n-2.0\n[[7.9, 8.0]]\n[[0.0024726231566347743, 0.9975273768433652]]\n60.0\n0.1\n\n\n1\n0\n0\n-2.0\n[[7.899786583570701, 7.95]]\n[[0.0468507276383191, 0.9531492723616809]]\n60.0\n0.1\n1\n0\n3.0\n[[7.9, 7.94997774639159]]\n[[0.047486230263144295, 0.9525137697368558]]\n60.0\n0.1\n\n\n2\n1\n0\n-1.0\n[[7.899786583570701, 7.860288271841277]]\n[[0.9145029408174301, 0.08549705918256983]]\n60.0\n0.1\n1\n0\n-1.0\n[[7.9, 7.860264375998088]]\n[[0.9156096786764698, 0.08439032132353015]]\n60.0\n0.1\n\n\n3\n0\n0\n1.0\n[[7.830484788680395, 7.860288271841277]]\n[[0.14329244697889626, 0.8567075530211037]]\n60.0\n0.1\n0\n0\n1.0\n[[7.830698202813024, 7.860264375998088]]\n[[0.14504926660635398, 0.854950733393646]]\n60.0\n0.1\n\n\n\n\n\n\n\nthrows no erros.\n\n\nTransient cooperation\nIn this section, we show that reinforcement learning agents can learn to cooperate in a tragedy social dilemma environment. However, this cooperation is not stable. It is only a transient phenomenon (Goll et al., 2024).\nWe use a social dilemma with fear \\(F=1\\) and greed \\(G=2\\).\n\nenv = SocialDilemma(Fear=1, Greed=2)\n\nWe want to give the agents an inital boost to cooperate or abate. Thus, we give them an inital higher value belief for abate than pollute.\n\nlearner1 = Learner(ValueBeliefs_Qoa = np.array([[0.5, -0.5]]), \n                   DiscountFactor = 0.9,\n                   LearningRate = 0.01,\n                   ChoiceIntensity = 5.0)\n\nWe assume the second agent to be identical to the first one.\n\nlearner2 = deepcopy(learner1)\n\n\nnp.random.seed(42)\ndf = interface_run([learner1, learner2], env, 20000) \n\n\ndef plot_TwoAgentBeliefsPolicies(df):\n    beliefs0_Qtoa = np.array(df.beliefs0.values.tolist())\n    policy0_Xtoa = np.array(df.policy0.values.tolist())\n    beliefs1_Qtoa = np.array(df.beliefs1.values.tolist())\n    policy1_Xtoa = np.array(df.policy1.values.tolist())\n    \n    fig = plt.figure(figsize=(14,6))\n    \n    ax0 = fig.add_subplot(311); ax0.set_ylabel('Value beliefs');\n    ax0.plot(beliefs0_Qtoa[:,0,a], label='Q1(a)', color='blue',lw=2)\n    ax0.plot(beliefs0_Qtoa[:,0,p], label='Q1(p)', color='red', lw=2)\n    ax0.plot(beliefs1_Qtoa[:,0,a], label='Q2(a)', color='darkblue', ls='--')\n    ax0.plot(beliefs1_Qtoa[:,0,p], label='Q2(p)', color='darkred', ls='--')\n    ax0.legend(loc='center right'); ax0.set_xlim(-10, len(df)*1.1)\n    \n    ax1 = fig.add_subplot(312, sharex=ax0); ax1.set_ylabel('Policy')\n    ax1.plot(policy0_Xtoa[:,0,a], label='X1(a)', color='blue', lw=2)\n    ax1.plot(policy0_Xtoa[:,0,p], label='X1(p)', color='red', lw=2)\n    ax1.plot(policy1_Xtoa[:,0,a], label='X2(a)', color='darkblue', ls='--')\n    ax1.plot(policy1_Xtoa[:,0,p], label='X2(p)', color='darkred', ls='--')\n    ax1.set_xlabel('Time steps'); ax1.legend(loc='center right')\n\n\nplot_TwoAgentBeliefsPolicies(df)\n\n\n\n\nTransient cooperation in a tragedy social dilemma.\n\n\n\n\nA propensity for cooperation coupled with excessive exploitation initially leads to transient cooperation within the stochastic learning dynamics. If one is unaware of this phenomenon, it may seem that the issue of cooperation in social dilemmas is resolved. However, as learning continues, this cooperation phase diminishes, resulting in increased defection (pollution) over cooperation (abate). During this phase, agents explore excessively, necessitating a higher choice intensity to establish more deterministic policies that align with the Nash equilibrium of full defection.\nThe timeing of when the breakdown of cooperation happens is stochastic. Re-run the simulation above with differnt random seeds to see that this is true. Beaware, that you must re-initalize the learners to begin from scratch.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Individual learning</span>"
    ]
  },
  {
    "objectID": "04.02-IndividualLearning.html#learning-goals-revisited",
    "href": "04.02-IndividualLearning.html#learning-goals-revisited",
    "title": "10  Individual learning",
    "section": "10.7 Learning goals revisited",
    "text": "10.7 Learning goals revisited\n\nReinforcement learning is valuable in models of human-environment interactions as a principled take to integrate individual cognition in dynamic environments and emerging collective behavior\nWe implemented the different elements of the multi-agent environment framework (interface, environment, agents).\n\nWe implemented and applied a basic temporal-different learning agent.\nWe implemented and applied the risk-reward dilemma (Lecture 03.01) and social dilemma (Lecture 03.02)\nWe visualized the learning process\n\nWe introduced and studied the exploration-exploitation trade-off, a general challenge for decision-making under uncertainty.\nWe made all of this possible by using the Python library pandas to manage data and refining our skills in object-oriented programming.\n\n\nKey advantages of an RL framework\n\nCognitive mechanisms are more integrated / less fragmented than behavioral theories\nCognitive mechanisms (as in RL) are more formalized than behavioral theories\nThe RL frame provides a natural dynamic extension to some economic equilibrium decision models.\nThe Rl frame allows for the study of behavior changes (e.g., after experimental policy interventions or environmental catastrophes)\n\n\n\nChallenges\n\nThe learning is inefficient. The agents require many interactions with the environment to learn what to do as they do not learn any model of the environment. This is a cognitive wasteful process.\nDealing with rare states/events is challenging when learning from only experience. Even more sample interactions are required to have enough experience of the raw events.\nThe stochasticity and hyperparameter tuning make it a cumbersome modeling tool. Both elements are invaluable for RL as an optimization method. For RL as a model of the cognitive processes underpinning human behavior, stochasticity and hyperparameter tuning complicate the modeling process considerably. They make studying the learning dynamics more difficult than necessary.\n\nUp next: Deterministic approximations\n\n\n\n\nAlbrecht, S. V., Christianos, F., & Schäfer, L. (2024). Multi-Agent Reinforcement Learning: Foundations and Modern Approaches.\n\n\nBarfuss, W., Flack, J. C., Gokhale, C. S., Hammond, L., Hilbe, C., Hughes, E., Leibo, J. Z., Lenaerts, T., Levin, S. A., Madhushani Sehwag, U., McAvoy, A., Meylahn, J. M., & Santos, F. P. (2024). Collective Cooperative Intelligence. Forthcomming in the Proceedings of the National Academy of Sciences.\n\n\nBotvinick, M., Wang, J. X., Dabney, W., Miller, K. J., & Kurth-Nelson, Z. (2020). Deep Reinforcement Learning and Its Neuroscientific Implications. Neuron, 107(4), 603–616. https://doi.org/10.1016/j.neuron.2020.06.014\n\n\nConstantino, S. M., Schlüter, M., Weber, E. U., & Wijermans, N. (2021). Cognition and behavior in context: A framework and theories to explain natural resource use decisions in social-ecological systems. Sustainability Science, 16(5), 1651–1671. https://doi.org/10.1007/s11625-021-00989-w\n\n\nGoll, D., Heitzig, J., & Barfuss, W. (2024). Deterministic Model of Incremental Multi-Agent Boltzmann Q-Learning: Transient Cooperation, Metastability, and Oscillations (arXiv:2501.00160). arXiv. https://doi.org/10.48550/arXiv.2501.00160\n\n\nSchlüter, M., Baeza, A., Dressler, G., Frank, K., Groeneveld, J., Jager, W., Janssen, M. A., McAllister, R. R. J., Müller, B., Orach, K., Schwarz, N., & Wijermans, N. (2017). A framework for mapping and comparing behavioural theories in models of social-ecological systems. Ecological Economics, 131, 21–35. https://doi.org/10.1016/j.ecolecon.2016.08.008\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (Second edition). The MIT Press.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Individual learning</span>"
    ]
  },
  {
    "objectID": "04.03-LearningDynamics.html",
    "href": "04.03-LearningDynamics.html",
    "title": "11  Learning dynamics",
    "section": "",
    "text": "11.1 Motivation\nThis chapter introduces collective reinforcement learning dynamics - treating the multi-agent reinforcement learning process as a non-linear dynamic system.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Learning dynamics</span>"
    ]
  },
  {
    "objectID": "04.03-LearningDynamics.html#motivation",
    "href": "04.03-LearningDynamics.html#motivation",
    "title": "11  Learning dynamics",
    "section": "",
    "text": "Modeling model-based reinforcement learning agents\n\n\n\nRecap | Reinforcement learning\nIn chapter 04.02-IndividualLearning, we introduced the basics of the temporal-difference reward-prediction reinforcement learning process. In essence, learning means updating the quality estimates, \\(Q^i_t(s,a)\\), with the current reward-prediction error, \\(\\delta^i_t(s, a)\\), after selection action \\(a_t\\) in state \\(s_t\\) according to\n\\[\nQ^i_{t+1}(s_t, a_t) = Q^i_{t}(s_t, a_t) + \\alpha^i \\delta^i_t(s_t, a_t),\n\\tag{11.1}\\]\nwhere \\(\\alpha^i \\in (0,1)\\) is the learning rate of agent \\(i\\), which regulates how much new information the agent uses for the update.\nThe reward-prediction error, \\(\\delta^i_t(s_t, a_t)\\), equals the difference of the new quality estimate, \\((1-\\gamma^i) r^i_t + \\gamma^i \\mathcal Q_n^i(s_{t+1})\\), and the current quality estimate, \\(\\mathcal Q_c^i(s_{t})\\),\n\\[\n\\delta^i_t(s_t, a_t) = (1-\\gamma^i) r^i_t + \\gamma^i \\mathcal{Q}^i_n(s_{t+1}, a_{t+1}) - \\mathcal Q^i_c(s_{t}, a_{t}),\n\\tag{11.2}\\]\nwhere the \\(\\mathcal{Q}_n^i\\) represents the quality estimate of the next state and \\(\\mathcal{Q}_c^i\\) represents the quality estimate of the current state. Depending on how we choose, \\(\\mathcal{Q}_n^i\\), and \\(\\mathcal{Q}_c^i\\), we recover various well-known temporal-difference reinforcement learning update schemes (Barfuss et al., 2019).\nFor example, we covered the Expected SARSA update with \\(\\mathcal{Q}_n^i (s_{t+1}, a_{t+1})  = \\mathcal{Q}_n^i (s_{t+1}) = \\sum_b x_t^i(s_{t+1},b) Q^i_t(s_{t+1}, b)\\), and \\(\\mathcal{Q}_c^i = Q^i_t\\). The temporal-difference reward-prediction error then reads,\n\\[\\delta^i_t(s_t, a_t) = (1-\\gamma^i) r^i_t + \\gamma^i \\sum_b x_t^i(s_{t+1},b) Q^i_t(s_{t+1}, b) - Q^i_t(s_{t}, a_{t}).\\]\n\n\nModeling challenges of reinforcement learning\nClassic reinforcement learning processes are highly stochastic since, generally, all agent strategies \\(x^i(s, a)\\), and the environments transition function \\(T(s, \\boldsymbol a, s')\\) are probability distributions. This stochasticity induces some challenges for using reinforcement learning as a modeling tool in complex human-environment systems:\n\nSample inefficiency. The agents need many samples to learn something, as they immediately forget a sample experience after a value-belief update.\nComputationally intense. Learning simulations are computationally intense since one requires many simulations to make sense of the noise, and each takes a long time to address the sample inefficiency.\nRare events. Due to the stochasticity, dealing with rare events is particularly difficult to learn from experience alone.\nHard to explain. The stochasticity can sometimes make it hard to explain why a phenomenon occurred in a simulation.\n\nIn contrast, human learning is highly efficient. Thus, as a model of human behavior, this basic reinforcement learning update scheme is implausible:\n\nHuman cognition is not that simplistic, and their actions are not that stochastic.\nHumans typically build and use a model of the world around them.\nSometimes, it is possible to invest into multiple options at the same time\n\nHow can we address these challenges?\n\n\nDynamics of collective reinforcement learning\nThe essential idea of the collective reinforcement learning dynamics approach is to replace the individual sample realizations of the temporal-difference reward-prediction error with its strategy average plus a small error term,\n\\[\\boldsymbol \\delta \\leftarrow {\\boldsymbol\\delta_\\mathbf{x}} + \\boldsymbol\\epsilon.\\]\nThus, collective reinforcement learning dynamics describe how agents with access to (a good approximation of) the strategy-average reward-prediction error would learn.\nThere are multiple interpretations to motivate how the agents can obtain the strategy averages:\n\nModel-based learners. Agents have a model of how the environment works, including how the other agents behave currently, but not how the other agents learn. The agents use their world model to stabilize learning. In the limit of a perfect model (and sufficient cognitive resources), the error term vanishes, \\(\\boldsymbol\\epsilon \\rightarrow 0\\).\nBatch learners. The agents store experiences (state observations, rewards, actions, next state observations) inside a memory batch and replay these experiences to make the learning more stable. Batch learning is a common algorithmic technique in machine learning. In the limit of an infinite memory batch, the error term vanishes, \\(\\boldsymbol\\epsilon \\rightarrow 0\\) (Barfuss, 2020).\nDifferent timescales. The agents learn on two different time scales. On one time scale, the agents interact with the environment, collecting experiences and integrating them to improve their quality estimates while keeping their strategies fixed. On the other time scale, they use the accumulated experiences to adapt their strategy. Timescale separation is a common technique used in theoretical physics. In the limit of a complete time scale separation, having infinite experiences between two strategy updates, the error term vanishes, \\(\\boldsymbol\\epsilon \\rightarrow 0\\) (Barfuss, 2022).\nProportional investors. Instead of choosing actions individually, agents can invest an endowment into actions proportional to their policy. Assuming by analogy, that the environment is not in one of its states but described by its state distribution, agents receive feedback proportionally to their investment. When there is no noise in the rewards itself, the error term vanishes, \\(\\boldsymbol\\epsilon \\rightarrow 0\\)\n\nIn the following, we focus on the idealized case of a vanishing error term, \\(\\boldsymbol\\epsilon \\rightarrow 0\\).\n\n\nLearning goals\nAfter this chapter, students will be able to:\n\nExplain the rationale of a dynamic systems treatment of reinforcement learning for complex human-environment interactions.\nStudy dynamic system properties of multi-agent reinforcement learning in human-environment models\nUse open-source Python packages.\n\nIn the next section, we will derive the strategy-average deterministic approximation model of the multi-agent reinforcement learning process. It goes beyond this lecture to implement the learning dynamics ourselves (although we could if we invested enough time). Luckily, we can utilize an open-source Python package to apply and study the learning dynamics, which we will do in the section afterward.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Learning dynamics</span>"
    ]
  },
  {
    "objectID": "04.03-LearningDynamics.html#derivation",
    "href": "04.03-LearningDynamics.html#derivation",
    "title": "11  Learning dynamics",
    "section": "11.2 Derivation",
    "text": "11.2 Derivation\nWe import our usual libraries.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\n\nimport matplotlib.style as style; style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (7.8, 2.5); plt.rcParams['figure.dpi'] = 300\ncolor = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]  # get the first color of the default color cycle\nplt.rcParams['axes.facecolor'] = 'white'; plt.rcParams['grid.color'] = 'gray'; plt.rcParams['grid.linewidth'] = 0.25; \n\nThen, we install the pyCRLD package from Github to compare the mathematical derivation with the respective code method.\n!pip install git+https://github.com/barfusslab/pyCRLD.git\n\nfrom pyCRLD.Agents.Base import abase as AgentBaseClass\n\nFrom Equation 11.2, \\[\n\\delta^i_t(s_t, a_t) = (1-\\gamma^i) r^i_t + \\gamma^i \\mathcal{Q}^i_n(s_{t+1}, a_{t+1}) - \\mathcal Q^i_c(s_{t}, a_{t}),\n\\]\nwe see that we need to construct the strategy-average reward, the strategy-average value of the next state, and the strategy-average value of the current state.\n\n1) Rewards\nThe strategy-average version of the current reward is obtained by considering each agent \\(i\\) taking action \\(a\\) in state \\(s\\) when all other agents \\(j\\) act according to their strategy \\(x^j(s, a^j)\\), causing the environment to transition to the next state \\(s'\\) with probability \\(T(s, \\boldsymbol a, s')\\), during which agent \\(i\\) receives reward \\(R^i(s, \\boldsymbol a, s')\\). Mathematically, we write,\n\\[\nR^i_\\mathbf{x}(s, a) = \\sum_{s'} \\sum_{a^j} \\prod_{j\\neq i} x^j(s, a^j) T(s, \\boldsymbol a, s') R^i(s, \\mathbf a, s').\n\\]\nNotation-wise, the formulation \\(\\sum_{a^j} \\prod_{j\\neq i} X^j(s, a^j)\\) is short for\n\\[\n\\sum_{a^j} \\prod_{j\\neq i} X^j(s, a^j) =\n\\sum_{a^1 \\in \\mathcal A^1} \\cdots \\sum_{a^{i-1} \\in \\mathcal A^{i-1}}\n\\sum_{a^{i+1} \\in \\mathcal A^{i+1}} \\cdots \\sum_{a^N \\in \\mathcal A^N}\nx^1(s, a^1) \\cdots x^{i-1}(s, a^{i-1}) x^{i+1}(s, a^{i+1}) \\cdots x^N(s, a^N)\n\\]\nIn the pyCRLD package, it is implemented as follows.\n\nAgentBaseClass.Risa??\n\n\nSignature:      AgentBaseClass.Risa(self, Xisa: jax.Array) -&gt; jax.Array\nCall signature: AgentBaseClass.Risa(*args, **kwargs)\nType:           PjitFunction\nString form:    &lt;PjitFunction of &lt;function abase.Risa at 0x140f4c4a0&gt;&gt;\nFile:           ~/Other/miniconda3/envs/iw-dev/lib/python3.11/site-packages/pyCRLD/Agents/Base.py\nSource:        \n    @partial(jit, static_argnums=0)    \n    def Risa(self,\n             Xisa:jnp.ndarray # Joint strategy\n            ) -&gt; jnp.ndarray:  # Average reward\n        \"\"\"Compute average reward `Risa`, given joint strategy `Xisa`\"\"\"\n        i = 0; a = 1; s = 2; s_ = 3  # Variables\n        j2k = list(range(4, 4+self.N-1))  # other agents\n        b2d = list(range(4+self.N-1, 4+self.N-1 + self.N))  # all actions\n        e2f = list(range(3+2*self.N, 3+2*self.N + self.N-1))  # all other acts\n \n        sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  # sum inds\n        otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))\n        args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX\\\n            + [self.T, [s]+b2d+[s_], self.R, [i, s]+b2d+[s_],\n               [i, s, a]]\n        return jnp.einsum(*args, optimize=self.opti)\n\n\n\nThe @partial(jit, static_argnums=0) decorator above the method makes the code execution fast. jit stands for just-in-time compilation and comes from the Python package JAX. Using JAX is very similar to using numpy. Hence, there is the JAX numpy module, jnp. See, for example, jnp.einsum in the code above.\nAnother trick is the use of the self.Omega object, which is a tensor of zeros and ones constructed to make the summation \\(\\sum_{a^j} \\prod_{j\\neq i} X^j(s, a^j)\\) work with the fast einsum method.\n\n\n2) Next quality estimates\nThe strategy average of the following state value is likewise computed by averaging the over all actions of the other agents and the following states.\nFor each agent \\(i\\), state \\(s\\), and action \\(a\\), all other agents \\(j\\neq i\\) choose their action \\(a^j\\) with probability \\(x^j(s, a^j)\\). Consequently, the environment transitions to the next state \\(s'\\) with probability \\(T(s, \\boldsymbol a, s')\\). At \\(s'\\), the agent estimates the quality of the next state to be of \\(v_\\mathbf{x}^i(s') = \\sum_{a^i \\in \\mathcal A^i} x^i(s', a^i) q_\\mathbf{x}^i(s', a^i)\\). Mathematically, we write,\n\\[\n{}^{n}\\!{Q}_\\mathbf{x}^i(s, a) = \\sum_{s'} \\sum_{a^j} \\prod_{j \\neq i} x^j(s, a^j) T(s, \\boldsymbol a, s') v_\\mathbf{x}^i(s').\n\\]\n\n\nState values\nWe compute the state values \\(v_\\mathbf{x}^i(s)\\) exactly like in Chapters 03.01 and 03.03. We write the Bellman equation in matrix form and bring the values \\(\\mathbf v^i_\\mathbf{x}\\) on one side,\n\\[\\mathbf v^i_\\mathbf{x} = (1-\\gamma^i) (\\mathbf 1_Z - \\gamma^i\\underline{\\mathbf T}_\\mathbf{x})^{-1} \\mathbf R^i_\\mathbf{x}.\\]\nIn the pyCRLD package, it is implemented as follows.\n\nAgentBaseClass.Vis??\n\n\nSignature:     \nAgentBaseClass.Vis(\n    self,\n    Xisa: jax.Array,\n    Ris: jax.Array = None,\n    Tss: jax.Array = None,\n    Risa: jax.Array = None,\n) -&gt; jax.Array\nCall signature: AgentBaseClass.Vis(*args, **kwargs)\nType:           PjitFunction\nString form:    &lt;PjitFunction of &lt;function abase.Vis at 0x140f4cb80&gt;&gt;\nFile:           ~/Other/miniconda3/envs/iw-dev/lib/python3.11/site-packages/pyCRLD/Agents/Base.py\nSource:        \n    @partial(jit, static_argnums=0)            \n    def Vis(self,\n            Xisa:jnp.ndarray, # Joint strategy\n            Ris:jnp.ndarray=None, # Optional reward for speed-up\n            Tss:jnp.ndarray=None, # Optional transition for speed-up\n            Risa:jnp.ndarray=None  # Optional reward for speed-up\n           ) -&gt; jnp.ndarray:  # Average state values\n        \"\"\"Compute average state values `Vis`, given joint strategy `Xisa`\"\"\"\n        # For speed up\n        Ris = self.Ris(Xisa, Risa=Risa) if Ris is None else Ris\n        Tss = self.Tss(Xisa) if Tss is None else Tss\n        \n        i = 0  # agent i\n        s = 1  # state s\n        sp = 2  # next state s'\n        n = np.newaxis\n        Miss = np.eye(self.Z)[n,:,:] - self.gamma[:, n, n] * Tss[n,:,:]\n        \n        invMiss = jnp.linalg.inv(Miss)\n               \n        return self.pre[:,n] * jnp.einsum(invMiss, [i, s, sp], Ris, [i, sp],\n                                          [i, s], optimize=self.opti)\n\n\n\n\n\nTransition matrix\nThe transition matrix \\(\\underline{\\mathbf T}_\\mathbf{x}\\) is a \\(Z \\times Z\\) matrix, where the element \\(T_\\mathbf{x}(s,s')\\) is the probability of transitioning from state \\(s\\) to \\(s'\\) under the joint policy \\(\\mathbf x\\). It is computed as\n\\[T_\\mathbf{x}(s,s') = \\sum_{a^i}\\prod_i x^i(s, a^i) T(s, \\mathbf a, s').\\]\nIn the pyCRLD package, it is implemented as follows.\n\nAgentBaseClass.Tss??\n\n\nSignature:      AgentBaseClass.Tss(self, Xisa: jax.Array) -&gt; jax.Array\nCall signature: AgentBaseClass.Tss(*args, **kwargs)\nType:           PjitFunction\nString form:    &lt;PjitFunction of &lt;function abase.Tss at 0x140f4afc0&gt;&gt;\nFile:           ~/Other/miniconda3/envs/iw-dev/lib/python3.11/site-packages/pyCRLD/Agents/Base.py\nSource:        \n    @partial(jit, static_argnums=0)    \n    def Tss(self, \n            Xisa:jnp.ndarray  # Joint strategy\n           ) -&gt; jnp.ndarray: # Average transition matrix\n        \"\"\"Compute average transition model `Tss`, given joint strategy `Xisa`\"\"\"\n        # i = 0  # agent i (not needed)\n        s = 1  # state s\n        sprim = 2  # next state s'\n        b2d = list(range(3, 3+self.N))  # all actions\n        X4einsum = list(it.chain(*zip(Xisa, [[s, b2d[a]] for a in range(self.N)])))\n        args = X4einsum + [self.T, [s]+b2d+[sprim], [s, sprim]]\n        return jnp.einsum(*args, optimize=self.opti)\n\n\n\n\n\nState rewards\nThe average reward \\(\\mathbf R^i_\\mathbf{x}\\) is a \\(N \\times Z\\)-matrix, where the element \\(R_\\mathbf{x}^i(s)\\) is the expected reward agent \\(i\\) receives in state \\(s\\) under the joint policy \\(\\mathbf x\\). It is computed as\n\\[ R_\\mathbf{x}^i(s) = \\sum_{s'} \\sum_{a^i}\\prod_i x^i(s, a^i) T(s, \\mathbf a, s') R^i(s, \\mathbf a, s').\\]\nIn the pyCRLD package, it is implemented as follows.\n\nAgentBaseClass.Ris??\n\n\nSignature:      AgentBaseClass.Ris(self, Xisa: jax.Array, Risa: jax.Array = None) -&gt; jax.Array\nCall signature: AgentBaseClass.Ris(*args, **kwargs)\nType:           PjitFunction\nString form:    &lt;PjitFunction of &lt;function abase.Ris at 0x140f4bd80&gt;&gt;\nFile:           ~/Other/miniconda3/envs/iw-dev/lib/python3.11/site-packages/pyCRLD/Agents/Base.py\nSource:        \n    @partial(jit, static_argnums=0)    \n    def Ris(self,\n            Xisa:jnp.ndarray, # Joint strategy\n            Risa:jnp.ndarray=None # Optional reward for speed-up\n           ) -&gt; jnp.ndarray: # Average reward\n        \"\"\"Compute average reward `Ris`, given joint strategy `Xisa`\"\"\" \n        if Risa is None:  # for speed up\n            # Variables      \n            i = 0; s = 1; sprim = 2; b2d = list(range(3, 3+self.N))\n        \n            X4einsum = list(it.chain(*zip(Xisa,\n                                    [[s, b2d[a]] for a in range(self.N)])))\n            args = X4einsum + [self.T, [s]+b2d+[sprim],\n                               self.R, [i, s]+b2d+[sprim], [i, s]]\n            return jnp.einsum(*args, optimize=self.opti)\n        \n        else:  # Compute Ris from Risa \n            i=0; s=1; a=2\n            args = [Xisa, [i, s, a], Risa, [i, s, a], [i, s]]\n            return jnp.einsum(*args, optimize=self.opti)\n\n\n\n\n\n3) Current quality estimates\nAssuming that agents select their actions according to a softmax policy function,\n\\[\nx^i_t(s, a) = \\frac{\\exp \\beta^i Q^i_t(s, a)}{\\sum_{b}\\exp \\beta^i Q^i_t(s, b)},\n\\tag{11.3}\\]\nwhere \\(\\beta^i\\) is the intensity of choice of agent \\(i\\), we can reformulate the update of the state-action quality estimates (Equation 11.1) into an update of the policy, i.e., state-action probabilities. Doing so reduces the dynamic system’s state space size, as we do not need to track the quality estimates of each agent in each state-action pair. Instead, we only need to track the state-action probabilities of each agent in each state-action pair. This is advantageous as the lower dimensional dynamic state space is more straightforward to analyze and visualize.\nFor the derivation of the joint policy update, we need to solve the policy function for \\(Q^i_t(s, a)\\),\n\\[\\begin{align}\nQ^i_t(s,a) &= \\frac{1}{\\beta^i} \\ln x^i_t(s, a) + \\frac{1}{\\beta^i}\\ln\\left[ \\sum_b \\exp \\beta^i Q^i_t(s,b) \\right] \\\\\n&= \\frac{1}{\\beta^i} \\ln x^i_t(s, a) + C^i(s)\n\\end{align}\\]\nwhere \\(C^i(s)\\) denots a constant in actions. It may vary for each agent and state but not for actions.\nThe step-by-step derivation of the joint policy update is as follows:\n\\[\\begin{align}\n    x^i_{t+1}(s, a) &= \\frac{\\exp \\beta^i Q^i_{t+1}(s, a)}{\\sum_{b} \\exp \\beta^i Q^i_{t+1}(s,b)} \\\\[1em]\n    %\n    %\n    &= \\frac{\\exp\\left[ \\beta^i \\left(Q^i_{t}(s, a) + \\alpha^i \\delta^i_t(s, a)\\right)\\right]}\n    {\\sum_{b} \\exp\\left[ \\beta^i \\left(Q_{t}(s, b) + \\alpha^i \\delta^i_t(s, b)\\right)\\right]}\n    \\qquad \\text{Inserting the belief update} \\\\[1em]\n    %\n    %\n    &= \\frac{\\exp\\left[ \\beta^i Q^i_{t}(s, a)\\right] \\exp\\left[\\alpha^i\\beta^i \\delta^i_t(s, a)\\right]}\n    {\\sum_{b} \\exp\\left[ \\beta^i Q_{t}(s, b) \\right] \\exp\\left[ \\alpha^i\\beta^i \\delta^i_t(s, b)\\right]}\n    \\qquad \\text{Factoring the exponentials} \\\\[1em]\n    %\n    %\n    %\n    &= \\frac{x^i_t(s, a) \\exp\\left[\\alpha^i\\beta^i \\delta^i_t(s, a)\\right]}\n    {\\sum_{b} x^i_t(s, b) \\exp\\left[ \\alpha^i\\beta^i \\delta^i_t(s, b)\\right]}\n    \\qquad \\text{Multiplying by $\\frac{\\frac{1}{z}}{\\frac{1}{z}}$ with $z=\\sum_{c} \\exp\\beta^i Q^i_t(s, c)$}\\\\[1em]\n    %\n    %\n    &= \\frac{x^i_t(s, a) \\exp\\left[\\alpha^i\\beta^i \\delta^i_\\mathbf{x}(s, a)\\right]}\n    {\\sum_{b} x^i_t(s, b) \\exp\\left[ \\alpha^i\\beta^i \\delta^i_\\mathbf{x}(s, b)\\right]}\n    \\qquad \\text{Replacing sample $\\delta^i_t$ with strategy-average $\\delta^i_\\mathbf{x}$}\\\\[1em]\n    %\n    %\n    %\n    &= \\frac{x^i_t(s, a) \\exp\\left[\\alpha^i\\beta^i \\left(\n        (1-\\gamma^i)R^i_\\mathbf{x}(s,a)\n        + \\gamma^i \\cdot {}^{n}\\!{Q}_\\mathbf{x}^i(s, a)\n        - Q^i_t(s,a)\\right)\\right]}\n    {\\sum_{b} x^i_t(s, b) \\exp\\left[ \\alpha^i\\beta^i \\left(\n        (1-\\gamma^i)R^i_\\mathbf{x}(s,b)\n        + \\gamma^i \\cdot {}^{n}\\!{Q}_\\mathbf{x}^i(s, b)\n        - Q^i_t(s,b)\\right)\\right]}\n    \\qquad \\text{Filling $\\delta^i_\\mathbf{x}$}\\\\[1em]\n    %\n    %\n    %\n    &= \\frac{x^i_t(s, a) \\exp\\left[\\alpha^i\\beta^i \\left(\n        (1-\\gamma^i)R^i_\\mathbf{x}(s,a)\n        + \\gamma^i \\cdot {}^{n}\\!{Q}_\\mathbf{x}^i(s, a)\n        - \\frac{1}{\\beta^i} \\ln x^i_t(s,a)\\right)\\right]}\n    {\\sum_{b} x^i_t(s, b) \\exp\\left[ \\alpha^i\\beta^i \\left(\n        (1-\\gamma^i)R^i_\\mathbf{x}(s,b)\n        + \\gamma^i \\cdot {}^{n}\\!{Q}_\\mathbf{x}^i(s, b)\n        - \\frac{1}{\\beta^i} \\ln x^i_t(s,b)\\right)\\right]} \\\\\n    & \\qquad \\text{Using $Q^i_t(s, a) = \\frac{1}{\\beta^i} \\ln x_t(s, a) + C^i(s)$}\n\\end{align}\\]\nIn summary, the strategy-average of the current state-action value, \\(Q_t^i(s,a)\\) is\n\\[\\frac{1}{\\beta^i} \\ln x^i(s, a).\\]\n\n\nStrategy-average reward-prediction temporal-difference error\n\nfrom pyCRLD.Agents.StrategySARSA import stratSARSA\n\nTaken together, the strategy-average reward-prediction error is\n\\[\n\\delta_\\mathbf{x}^i(s, a) = (1-\\gamma^i) R^i_\\mathbf{x}(s, a) + \\gamma^i \\cdot {}^{n}\\!{Q}_\\mathbf{x}^i(s, a) - \\frac{1}{\\beta^i} \\ln x^i(s, a),\n\\]\nto be inserted in the joint policy update,\n\\[\nx^i_{t+1}(s, a) = \\frac{x^i_t(s, a) \\exp\\left[\\alpha^i\\beta^i \\delta^i_\\mathbf{x}(s, a)\\right]}\n    {\\sum_{b} x^i_t(s, b) \\exp\\left[ \\alpha^i\\beta^i \\delta^i_\\mathbf{x}(s, b)\\right]}.\n\\]\nWe made the strategy update independent of the quality beliefs.\nIn the pyCRLD package, update step is implement as follows,\n\nstratSARSA.step??\n\n\nSignature:      stratSARSA.step(self, Xisa) -&gt; tuple\nCall signature: stratSARSA.step(*args, **kwargs)\nType:           PjitFunction\nString form:    &lt;PjitFunction of &lt;function strategybase.step at 0x140f907c0&gt;&gt;\nFile:           ~/Other/miniconda3/envs/iw-dev/lib/python3.11/site-packages/pyCRLD/Agents/StrategyBase.py\nSource:        \n    @partial(jit, static_argnums=0)\n    def step(self,\n             Xisa  # Joint strategy\n            ) -&gt; tuple:  # (Updated joint strategy, Prediction error)\n        \"\"\"\n        Performs a learning step along the reward-prediction/temporal-difference error\n        in strategy space, given joint strategy `Xisa`.\n        \"\"\"\n        TDe = self.TDerror(Xisa)\n        n = jnp.newaxis\n        XexpaTDe = Xisa * jnp.exp(self.alpha[:,n,n] * TDe)\n        return XexpaTDe / XexpaTDe.sum(-1, keepdims=True), TDe\n\n\n\nThe step method comes from a parent class, called strategybase, and calls the TDerror method, which is initialized upon creating a specific agent collective with the concrete reward-prediction error method from the SARSA agent.\nThe reward-prediction error of the SARSA agent is implemented as follows.\n\nstratSARSA.RPEisa??\n\n\nSignature:      stratSARSA.RPEisa(self, Xisa, norm=False) -&gt; numpy.ndarray\nCall signature: stratSARSA.RPEisa(*args, **kwargs)\nType:           PjitFunction\nString form:    &lt;PjitFunction of &lt;function stratSARSA.RPEisa at 0x140f905e0&gt;&gt;\nFile:           ~/Other/miniconda3/envs/iw-dev/lib/python3.11/site-packages/pyCRLD/Agents/StrategySARSA.py\nSource:        \n    @partial(jit, static_argnums=(0,2))\n    def RPEisa(self,\n               Xisa,  # Joint strategy\n               norm=False # normalize error around actions? \n               ) -&gt; np.ndarray:  # RP/TD error\n        \"\"\"\n        Compute reward-prediction/temporal-difference error for \n        strategy SARSA dynamics, given joint strategy `Xisa`.\n        \"\"\"\n        R = self.Risa(Xisa)\n        NextQ = self.NextQisa(Xisa, Risa=R)\n        n = jnp.newaxis\n        E = self.pre[:,n,n]*R + self.gamma[:,n,n]*NextQ - 1/self.beta[:, n, n] * jnp.log(Xisa)\n        E *= self.beta[:,n,n]\n        E = E - E.mean(axis=2, keepdims=True) if norm else E\n        return E",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Learning dynamics</span>"
    ]
  },
  {
    "objectID": "04.03-LearningDynamics.html#application",
    "href": "04.03-LearningDynamics.html#application",
    "title": "11  Learning dynamics",
    "section": "11.3 Application",
    "text": "11.3 Application\nLet us apply the collective reinforcement learning dynamics to the ecological public good environment from Chapter 03.03. We will highlight the complex dynamics phenomena that arise from the collective reinforcement learning dynamics (Barfuss et al., 2024).\n\n\n\nEcological public good collective decision-making environment\n\n\nFor convenience, we import the environment class from the pyCRLD package. However, you now possess all the skills needed to implement it on your own.\n\nfrom pyCRLD.Environments.EcologicalPublicGood import EcologicalPublicGood as EcoPG\n\nWe initialize the environment with two agents, a benefit-to-cost ratio of \\(f=1.2\\), a cost of \\(c=5\\), a collapse impact of \\(m=-5\\), a collapse leverage of \\(0.2\\), and a recovery probability of \\(0.01\\). We set the degraded_choice parameter to False to remove all agency from the agents in the degraded state. In other word, regardless what they do in the degraded state, they have to wait for the recovery on average \\(1/q_r\\) timesteps.\n\n# Inititalize the ecological public good environment\nenv = EcoPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\n\nThese parameters ensure to have the same short-term welfare values in the prosperous state as shown in the Figure above.\n\np = env.Sset.index('p'); g = env.Sset.index('g') # indices of the prosperous and degraded state\nprint(\"Agent zero's welfare\\n\", env.R[0, p, :, :, p])\nprint(\"\\nAgent one's welfare\\n\", env.R[1, p, :, :, p])\n\nAgent zero's welfare\n [[ 1. -2.]\n [ 3.  0.]]\n\nAgent one's welfare\n [[ 1.  3.]\n [-2.  0.]]\n\n\n\nLearning trajectories\nWe create a multi-agent-environment interface MAEi composed of SARSA agents with a learning rate of \\(0.05\\), a choice intensity of \\(50.0\\), and a discount factor of \\(0.75\\). We set the use_prefactor parameter to True to use the pre-factor \\((1-\\gamma)\\) in the policy update.\n\nMAEi = stratSARSA(env, learning_rates=0.05, choice_intensities=50.0, discount_factors=0.75, use_prefactor=True)\n\nLet us evolve the learning from a random initial joint policy,\n\nx = MAEi.random_softmax_strategy()\nx\n\nArray([[[0.65391284, 0.34608716],\n        [0.540063  , 0.45993698]],\n\n       [[0.23748323, 0.7625168 ],\n        [0.6527203 , 0.34727973]]], dtype=float32)\n\n\nfor a maximum of 5000 time steps with a convergence tolerance of \\(10^{-5}\\). Thus, if two consecutive joint policies are closer than \\(10^{-5}\\), the learning process stops.\n\npolicy_trajectory_Xtisa, fixedpointreached = MAEi.trajectory(x, Tmax=5000, tolerance=10**-5)\nfixedpointreached\n\nTrue\n\n\nWe have reached a fixed point and the learning trajecotry has a length of\n\nlen(policy_trajectory_Xtisa)\n\n153\n\n\nLet us visualize the time evolution of learning trajectory.\n\nc = env.Aset[0].index('c'); d = env.Aset[0].index('d')  # action indices\nplt.plot(policy_trajectory_Xtisa[:, 0, p, c], label='Agent zero in prosperous state', c='blue', lw=3, alpha=0.5)\nplt.plot(policy_trajectory_Xtisa[:, 1, p, c], label='Agent one in prosperous state', c='blue', ls='--')\nplt.plot(policy_trajectory_Xtisa[:, 0, g, c], label='Agent zero in degraded state', c='red', lw=3, alpha=0.5)\nplt.plot(policy_trajectory_Xtisa[:, 1, g, c], label='Agent one in degraded state', c='red', ls='--');\nplt.xlabel('Time steps'); plt.ylabel('Cooperation probability'); plt.legend(); plt.ylim(0, 1);\n\n\n\n\n\n\n\n\nLet’s repeat this serveral times from different random joint policies. Exectue the cell below multiple times and observe what happens.\n\nx = MAEi.random_softmax_strategy()\npolicy_trajectory_Xtisa, fixedpointreached = MAEi.trajectory(x, Tmax=5000, tolerance=10**-5)\n\nplt.plot(policy_trajectory_Xtisa[:, 0, p, c], label='Agent zero in prosperous state', c='blue', lw=3, alpha=0.5)\nplt.plot(policy_trajectory_Xtisa[:, 1, p, c], label='Agent one in prosperous state', c='blue', ls='--')\nplt.plot(policy_trajectory_Xtisa[:, 0, g, c], label='Agent zero in degraded state', c='red', lw=3, alpha=0.5)\nplt.plot(policy_trajectory_Xtisa[:, 1, g, c], label='Agent one in degraded state', c='red', ls='--');\nplt.xlabel('Time steps'); plt.ylabel('Cooperation probability'); plt.legend(); plt.ylim(0, 1);\n\n\n\n\n\n\n\n\nSome observations you should make:\n\nLearning occurs fast. The agents quickly attain a stable state within just a few hundred steps, and their execution is remarkably rapid.\nLearning is deterministic. Given an initial joint policy, the learning process has no stochastic fluctuations. The agents learn deterministically. However, what they learn is a probability distribution.\nOutcome is bistable. The agents learn to either cooperate or defect completely in the prosperous state, depending on where they start. If they start closer to cooperation, they learn to cooperate. If they start closer to defection, they learn to defect.\nAgents randomize. In the degraded state, agents learn to randomize over actions fully, i.e., choose each of their two options with a probability of 0.5. This is because the agents cannot influence the outcome of their actions and, thus, are driven only by exploration. You can imagine the desire to explore as a form of intrinsic motivation that dominates here without controllable extrinsic rewards.\n\n\n\nFlow plot\nThe determinism and the fast computation allow for an improved visualization of the learning process. As with any deterministic dynamic system, we can visualize the flow plot of the dynamics (See Chapter 02.01).\nIn the pyCRLD package, we have a special module for that purpose.\n\nfrom pyCRLD.Utils import FlowPlot as fp\n\nApplying this function yields a flow plot of the learning dynamics which highlights the bistability of the learning process in the prosperous state and the randomization in the degraded state.\n\nx = ([0], [g,p], [c])  # which (agent, observation, action) to plot on x axis\ny = ([1], [g,p], [c])  # which (agent, observation, action) to plot on y axis\neps=10e-3; action_probability_points = np.linspace(0+eps, 1.0-eps, 9)\nax = fp.plot_strategy_flow(MAEi, x, y, action_probability_points, conds=env.Sset)\n\n\n\n\nBasic flow of collective reinforcement learning dynamics.\n\n\n\n\nThese flow plots allow for a geometric understanding of the collective learning dynamics over the whole joint policy space. In contrast to a standard flow plot, per default, the arrows show the temporal-difference reward prediction error. Thus, they have a cognitive interpretation.\nWe may use them to study how the parameters of the learning agents and the environment influence the outcome.\n\ndef plot_flow(DiscountFactor=0.75, ChoiceIntensity=50, CollapseImpact=-5, CollapseLeverage=0.2):\n    env = EcoPG(N=2, f=1.2, c=5, m=CollapseImpact, qc=CollapseLeverage,\n                qr=0.01, degraded_choice=False)\n    MAEi = stratSARSA(env, learning_rates=0.05, choice_intensities=ChoiceIntensity, \n                      discount_factors=DiscountFactor, use_prefactor=True)\n    \n    x = ([0], [g,p], [c])  # which (agent, observation, action) to plot on x axis\n    y = ([1], [g,p], [c])  # which (agent, observation, action) to plot on y axis\n    eps=10e-3; action_probability_points = np.linspace(0+eps, 1.0-eps, 9)\n    ax = fp.plot_strategy_flow(MAEi, x, y, action_probability_points, conds=env.Sset)\n\nWhen working with this material in a Jupyter notebook, we can interactively study the parameter dependence of the flow plot.\nFor example, caring more for the future makes the cooperative basin of attraction larger.\n\nplot_flow(DiscountFactor=0.8)\n\n\n\n\nLearning flow with more future caring.\n\n\n\n\nSo does a more severe collapse impact,\n\nplot_flow(CollapseImpact=-6)\n\n\n\n\nLearning flow with a more severe collapse impact.\n\n\n\n\nand a collapse that occurse more likely or faster.\n\nplot_flow(CollapseLeverage=0.3)\n\n\n\n\nLearning flow with a higher collapse leverage.\n\n\n\n\nThe flow in the degraded state is unaffected by these parameter modulations.\nA very low choice intensity makes the desire to explore (i.e., randomize) dominate also in the prosperous state.\n\nplot_flow(ChoiceIntensity=1)\n\n\n\n\nLearning flow with a small intensity of choice makes explorative behavior dominant.\n\n\n\n\n\n\nCritical transition\nLet us study the learning behavior around the separatrix of the bistable region.\nFirst, we define a function that allows us to enter initial cooperation probabilities for both agents and return a proper joint policy. This function sets the cooperation probability in the degraded state to 0.5 for both agents, as we have seen that the agents will eventually learn to randomize in the degraded state and we are not interested in that part of the learning behavior.\n\ndef compile_strategy(p0c:float,  # cooperation probability of agent zero\n                     p1c:float):  # cooperation probability of agent one\n    Pi = np.array([0.5, p0c])  # coop. prob. in the degraded state set to 0.5\n    Pj = np.array([0.5, p1c])\n    xi = np.array([Pi, 1-Pi]).T\n    xj = np.array([Pj, 1-Pj]).T\n    return np.array([xi, xj])              \n\nWe setup the multiagent-environment interaface.\n\nenv = EcoPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\nMAEi = stratSARSA(env=env, learning_rates=0.01, choice_intensities=100, discount_factors=0.75,\n                  use_prefactor=True)\n\nTo get a feeling for the critical transition, we create three well chosen learning trajectories.\n\nxtrajs = []  # storing strategy trajectories \nfprs = []    # and whether a fixed point is reached\nfor pc in [0.18, 0.19, 0.20]:  # cooperation probability of agent 1\n    X = compile_strategy(pc, 0.95)\n    xtraj, fixedpointreached = MAEi.trajectory(X, Tmax=5000, tolerance=10**-5)\n    xtrajs.append(xtraj); fprs.append(fixedpointreached)\n    print(\"Trajectory length:\",len(xtraj))\n\nTrajectory length: 178\nTrajectory length: 234\nTrajectory length: 174\n\n\nWe plot them ontop of the learning flow.\n\nfig = plt.figure(figsize=(12, 3.5)); ax = fig.add_subplot(132) # to center the plot\nfig.add_subplot(131, xticks=[], yticks=[]); fig.add_subplot(133, xticks=[], yticks=[]);\n\nx = ([0], [p], [c])  # which (agent, observation, action) to plot on x axis\ny = ([1], [p], [c])  # which (agent, observation, action) to plot on y axis\neps=10e-3; action_probability_points = np.linspace(0+eps, 1.0-eps, 9)\nfp.plot_strategy_flow(MAEi, x, y, action_probability_points, axes=[ax])\n   \n# Add trajectories to flow plot\nfp.plot_trajectories(xtrajs, x=x, y=y, fprs=fprs, cols=['red','blue','blue'],\n                     lws=[2], msss=[2], lss=['-'], alphas=[0.75], axes=[ax]);\n\nax.set_ylabel(\"Agent 2's cooperation probability\"); \nax.set_xlabel(\"Agent 1's cooperation probability\");\n\n\n\n\n\n\n\n\nNext, we create a more fine-grained bundle of learning trajectories.\n\n# Cooperation probability of agent 1\npcs = np.concatenate([np.linspace(0.01, 0.99, 51), np.linspace(0.185, 0.195, 151)])\npcs = np.sort(np.unique(pcs))\n\nXktisa = []  # storing strategy trajectories \nfprs = []    # and whether a fixed point is reached\nfor i, pc in enumerate(pcs):\n    print(f\"Progress: {((i+1)/len(pcs)):.2%}\", end=\"\\r\")\n    X = compile_strategy(pc, 0.95)\n    PolicyTrajectories_Xtisa, fixedpointreached = MAEi.trajectory(X, Tmax=5000, tolerance=10**-5)\n    Xktisa.append(PolicyTrajectories_Xtisa)\n    fprs.append(fixedpointreached)\n\nProgress: 100.00%\n\n\nWe obtain the critical point in this bundle of learning trajectories where the two agents switch or tip from complete defection to complete cooperation.\nFirst, we check that all trajectories converged.\n\nnp.all(fprs)\n\nTrue\n\n\nThen, we obtain the cooperation probabilities at convergence.\n\nconverged_pcs = np.array([Xtisa[-1][:, p, c] for Xtisa in Xktisa])\nconverged_pcs.shape\n\n(201, 2)\n\n\nLast, we show the biomodal distribution of full defection and full cooperation.\n\nnp.histogram(np.array(converged_pcs).mean(-1), range=(0,1))[0]\n\narray([ 80,   0,   0,   0,   0,   0,   0,   0,   0, 121])\n\n\nThus, the critical point lies at the index\n\ncp = np.histogram(np.array(converged_pcs).mean(-1), range=(0,1))[0][0]\ncp\n\n80\n\n\nand has an approximate value between\n\nprint(pcs[cp-1], 'and', pcs[cp], '.')\n\n0.18966666666666668 and 0.18973333333333334 .\n\n\nWe use this more fine-grained bundle of learning trajectories to visualize the phenomenon of a critical slowing down by plotting the time steps required to reach convergence.\n\nplt.plot(pcs[:cp], [len(Xtisa) for Xtisa in Xktisa[:cp]],\n         '-', color='red', lw=2, alpha=0.8)  # defectors in red\nplt.plot(pcs[cp:], [len(Xtisa) for Xtisa in Xktisa[cp:]], \n         '-', color='blue', lw=2, alpha=0.6) # cooperators in blue\nplt.ylim(0); plt.ylabel('Timesteps to convergence')\nplt.xlabel(f\"Agent 1's cooperation probability in the prosperous state\");\n\n\n\n\nTime steps required to convergence show a critical slowing down around the tipping point.\n\n\n\n\nWe also observe a kind of transient tipping point in the learning dynamics, when plotting the two closest trajectories around the critical point.\n\ndef plot_TransientTipping(xlim=None):\n    # Plot the defecting learners in red\n    plt.plot(Xktisa[cp-1][:, 0, p, c], color='red', lw=5, ls=':', label='Agent zero') \n    plt.plot(Xktisa[cp-1][:, 1, p, c], color='red', lw=4, ls=\"--\", alpha=0.4, label='Agent one')\n    \n    # Plot the cooperating learners in blue\n    plt.plot(Xktisa[cp][:, 0, p, c], color='blue', lw=3, ls=':', label='Agent zero')\n    plt.plot(Xktisa[cp][:, 1, p, c], color='blue', lw=2, ls=\"--\", alpha=0.4, label='Agent one')\n    \n    plt.xlim(xlim); plt.legend(); plt.xlabel(\"Timesteps\"); plt.ylabel(\"Cooperation\")\n\n\nplot_TransientTipping()\n\n\n\n\nEmergent time scale seperation at the critical point.\n\n\n\n\nDuring this emergent timescale separation, the learning process seems to settle on a mixed policy after approximately 50 timesteps. It remains at this point for another 50 steps, which is the same duration it took to reach this mixed policy (Figure 11.1). The learning adjusts the policies more rapidly after this period until they converge to two deterministic policies.\n\nplot_TransientTipping((0, 95))\n\n\n\n\n\n\n\nFigure 11.1: Apparent convergence to a mixed policy.\n\n\n\n\n\n\n\nHysteresis\nThe last phenomenon we want to highlight is hysteresis (See Chapter 02.02). We study the cooperation probabilities of the agents in the prosperous state as a function of the discount factor \\(\\gamma\\). We know from Chapter 03.03 that caring for the future can turn a tragedy of the commons into a comedy while passing through the coordination regime.\nIn the following, we start at a relatively low level of caring for the future, increase it, and then decrease it again, all while letting the agent learn along\nFirst, let us create the discount factor values.\n\ndcfs = list(np.arange(0.6, 0.9, 0.005))\nhystcurve = dcfs + dcfs[::-1]\n\nThen, we set up the environment and start the simulation from a random policy. We let the agents learn for 2500 time steps or until the learning process converges with a tiny tolerance. Then, we record the final policy, advance the discount factor, and restart from the previous final policy.\n\n# Set up the ecological public goods environment\nenv = EcoPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\n\ncoops = []  # for storing the cooperation probabilities\nX = MAEi.random_softmax_strategy() \nfor i, dcf in enumerate(hystcurve):\n    # Adjust multi-agent environment interface with discount factor\n    MAEi = stratSARSA(env=env, discount_factors=dcf, use_prefactor=True,\n                      learning_rates=0.05, choice_intensities=50)\n    trj, fpr = MAEi.trajectory(X, Tmax=2500, tolerance=10e-12)\n    print(f\"Progress: {((i+1)/len(hystcurve)):6.2%} |\",\n          f\"Discount Factor {dcf:5.4} | Conv?: {fpr}\" , end=\"\\r\")        \n    X = trj[-1] # select last strategy\n    coops.append(X[:, 1, 0]) # append to storage container\n\nProgress: 100.00% | Discount Factor   0.6 | Conv?: True\n\n\nNow, we plot the computed data. We use the points’ size and color to indicate the time dimensions of the discount factor changes. The time flows from big to small data points and from dark to light ones.\n\n# Plot background line\nplt.plot(hystcurve, np.array(coops).mean(-1),'-',alpha=0.5,color='k',zorder=-1)\n# Plot data points with size and color indicating the time dimension\nplt.scatter(hystcurve, np.array(coops).mean(-1), alpha=0.9, cmap='viridis',\n            s=np.arange(len(hystcurve))[::-1]+1, c=np.arange(len(hystcurve)))\n\nplt.ylabel('Cooperation'); plt.xlabel('Discount Factor'); #plt.ylim(0,1)\n\n\n\n\nHysteresis curve\n\n\n\n\nThe hysteresis curve shows that the probability of cooperation among agents in the prosperous state depends on the history of the discount factor. The agents’ learning dynamics exhibit a memory of the past, a typical feature of complex systems.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Learning dynamics</span>"
    ]
  },
  {
    "objectID": "04.03-LearningDynamics.html#learning-goals-revisited",
    "href": "04.03-LearningDynamics.html#learning-goals-revisited",
    "title": "11  Learning dynamics",
    "section": "11.4 Learning goals revisited",
    "text": "11.4 Learning goals revisited\nIn this chapter,\n\nwe introduced deterministic approximation models of the stochastic reinforcement learning process as a valuable tool for modeling complex human-environment interactions. Collective reinforcement learning dynamics model adaptive agents (in stylized model environments)\n\nthat use a perfect model of the world\nin a computationally fast\ntransparent\nand deterministically evolving way.\n\nWe studied complex dynamic phenomena of multi-agent reinforcement learning in the ecological public good environment.\nTo do so, we used the open-source pyCRLD Python package.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Learning dynamics</span>"
    ]
  },
  {
    "objectID": "04.03-LearningDynamics.html#synthesis",
    "href": "04.03-LearningDynamics.html#synthesis",
    "title": "11  Learning dynamics",
    "section": "11.5 Synthesis",
    "text": "11.5 Synthesis\nCollective reinforcement learning dynamics bridge agent-based, equation-based (dynamic systems), and equilibrium-based modeling:\n\nagent-based: derived from individual agent characteristics\nequation-based: treated as a dynamical systems\nequilibrium-based: fixed points are (close to) the classic equilibrium solutions\n\n\n\n\nThree types of models\n\n\n\n\n\n\nBarfuss, W. (2020). Reinforcement Learning Dynamics in the Infinite Memory Limit. Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, 1768–1770.\n\n\nBarfuss, W. (2022). Dynamical systems as a level of cognitive analysis of multi-agent learning. Neural Computing and Applications, 34(3), 1653–1671. https://doi.org/10.1007/s00521-021-06117-0\n\n\nBarfuss, W., Donges, J. F., & Kurths, J. (2019). Deterministic limit of temporal difference reinforcement learning for stochastic games. Physical Review E, 99(4), 043305. https://doi.org/10.1103/PhysRevE.99.043305\n\n\nBarfuss, W., Flack, J. C., Gokhale, C. S., Hammond, L., Hilbe, C., Hughes, E., Leibo, J. Z., Lenaerts, T., Levin, S. A., Madhushani Sehwag, U., McAvoy, A., Meylahn, J. M., & Santos, F. P. (2024). Collective Cooperative Intelligence. Forthcomming in the Proceedings of the National Academy of Sciences.",
    "crumbs": [
      "Transformation Agency",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Learning dynamics</span>"
    ]
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Albrecht, S. V., Christianos, F., & Schäfer, L. (2024).\nMulti-Agent Reinforcement Learning:\nFoundations and Modern Approaches.\n\n\nAnderies, J. M., Folke, C., Walker, B., & Ostrom, E. (2013).\nAligning Key Concepts for Global Change\nPolicy: Robustness, Resilience, and\nSustainability. Ecology and Society,\n18(2). https://www.jstor.org/stable/26269292\n\n\nAnderies, J. M., & Janssen, M. A. (2016). Sustaining the\ncommons. Independent. https://dlc.dlib.indiana.edu/dlc/handle/10535/8839\n\n\nArmstrong McKay, D. I., Staal, A., Abrams, J. F., Winkelmann, R.,\nSakschewski, B., Loriani, S., Fetzer, I., Cornell, S. E., Rockström, J.,\n& Lenton, T. M. (2022). Exceeding 1.5∘C global\nwarming could trigger multiple climate tipping points. Science,\n377(6611), eabn7950. https://doi.org/10.1126/science.abn7950\n\n\nBarfuss, W. (2020). Reinforcement Learning Dynamics in the\nInfinite Memory Limit. Proceedings of the 19th\nInternational Conference on Autonomous Agents\nand MultiAgent Systems, 1768–1770.\n\n\nBarfuss, W. (2022). Dynamical systems as a level of cognitive analysis\nof multi-agent learning. Neural Computing and Applications,\n34(3), 1653–1671. https://doi.org/10.1007/s00521-021-06117-0\n\n\nBarfuss, W., Donges, J. F., & Kurths, J. (2019). Deterministic limit\nof temporal difference reinforcement learning for stochastic games.\nPhysical Review E, 99(4), 043305. https://doi.org/10.1103/PhysRevE.99.043305\n\n\nBarfuss, W., Donges, J. F., Lade, S. J., & Kurths, J. (2018). When\noptimization for governing human-environment tipping elements is neither\nsustainable nor safe. Nature Communications, 9(1),\n2354. https://doi.org/10.1038/s41467-018-04738-z\n\n\nBarfuss, W., Donges, J. F., Vasconcelos, V. V., Kurths, J., & Levin,\nS. A. (2020). Caring for the future can turn tragedy into comedy for\nlong-term collective action under risk of collapse. Proceedings of\nthe National Academy of Sciences, 117(23), 12915–12922. https://doi.org/10.1073/pnas.1916545117\n\n\nBarfuss, W., Donges, J., & Bethge, M. (2024).\nEcologically-mediated collective action in commons with tipping\nelements. OSF. https://doi.org/10.31219/osf.io/7pcnm\n\n\nBarfuss, W., Flack, J. C., Gokhale, C. S., Hammond, L., Hilbe, C.,\nHughes, E., Leibo, J. Z., Lenaerts, T., Levin, S. A., Madhushani Sehwag,\nU., McAvoy, A., Meylahn, J. M., & Santos, F. P. (2024). Collective\nCooperative Intelligence. Forthcomming in the\nProceedings of the National Academy of Sciences.\n\n\nBarrett, S. (1994). Self-Enforcing International Environmental\nAgreements. Oxford Economic Papers,\n46(Supplement_1), 878–894. https://doi.org/10.1093/oep/46.Supplement_1.878\n\n\nBarrett, S. (2005). Environment and Statecraft:\nThe Strategy of Environmental\nTreaty-Making. Oxford University Press. https://doi.org/10.1093/0199286094.001.0001\n\n\nBarrett, S., & Dannenberg, A. (2012). Climate negotiations under\nscientific uncertainty. Proceedings of the National Academy of\nSciences, 109(43), 17372–17376. https://doi.org/10.1073/pnas.1208417109\n\n\nBiggs, R., Preiser, R., de Vos, A., Schlüter, M., Maciejewski, K., &\nClements, H. (2021). The Routledge Handbook of\nResearch Methods for Social-Ecological\nSystems (1st ed.). Routledge. https://doi.org/10.4324/9781003021339\n\n\nBoers, N., & Rypdal, M. (2021). Critical slowing down suggests that\nthe western Greenland Ice Sheet is close to a tipping\npoint. Proceedings of the National Academy of Sciences,\n118(21), e2024192118. https://doi.org/10.1073/pnas.2024192118\n\n\nBotvinick, M., Wang, J. X., Dabney, W., Miller, K. J., &\nKurth-Nelson, Z. (2020). Deep Reinforcement Learning and\nIts Neuroscientific Implications. Neuron,\n107(4), 603–616. https://doi.org/10.1016/j.neuron.2020.06.014\n\n\nBrander, J. A., & Taylor, M. S. (1998). The Simple\nEconomics of Easter Island: A Ricardo-Malthus\nModel of Renewable Resource Use. The American\nEconomic Review, 88(1), 119–138. https://www.jstor.org/stable/116821\n\n\nBrockmann, D. (2021). Im Wald vor lauter Bäumen: Unsere komplexe Welt besser\nverstehen. Deutscher Taschenbuch Verlag.\n\n\nCarpenter, S., Walker, B., Anderies, J. M., & Abel, N. (2001). From\nMetaphor to Measurement:\nResilience of What to What?\nEcosystems, 4(8), 765–781. https://doi.org/10.1007/s10021-001-0045-9\n\n\nConstantino, S. M., Schlüter, M., Weber, E. U., & Wijermans, N.\n(2021). Cognition and behavior in context: A framework and theories to\nexplain natural resource use decisions in social-ecological systems.\nSustainability Science, 16(5), 1651–1671. https://doi.org/10.1007/s11625-021-00989-w\n\n\nDaniel, C. J., Frid, L., Sleeter, B. M., & Fortin, M.-J. (2016).\nState-and-transition simulation models: A framework for forecasting\nlandscape change. Methods in Ecology and Evolution,\n7(11), 1413–1423. https://doi.org/10.1111/2041-210X.12597\n\n\nElsawah, S., Filatova, T., Jakeman, A. J., Kettner, A. J., Zellner, M.\nL., Athanasiadis, I. N., Hamilton, S. H., Axtell, R. L., Brown, D. G.,\nGilligan, J. M., Janssen, M. A., Robinson, D. T., Rozenberg, J., Ullah,\nI. I. T., & Lade, S. J. (2020). Eight grand challenges in\nsocio-environmental systems modeling. Socio-Environmental Systems\nModelling, 2, 16226–16226. https://doi.org/10.18174/sesmo.2020a16226\n\n\nEpstein, J. M. (1999). Agent-based computational models and generative\nsocial science. Complexity, 4(5), 41–60. https://doi.org/10.1002/(SICI)1099-0526(199905/06)4:5&lt;41::AID-CPLX9&gt;3.0.CO;2-F\n\n\nFarahbakhsh, I., Bauch, C. T., & Anand, M. (2022). Modelling coupled\nhuman–environment complexity for the future of the biosphere: Strengths,\ngaps and promising directions. Philosophical Transactions of the\nRoyal Society B: Biological Sciences, 377(1857), 20210382.\nhttps://doi.org/10.1098/rstb.2021.0382\n\n\nFolke, C., Carpenter, S., Walker, B., Scheffer, M., Chapin, T., &\nRockström, J. (2010). Resilience Thinking:\nIntegrating Resilience, Adaptability and\nTransformability. Ecology and Society,\n15(4). https://doi.org/10.5751/ES-03610-150420\n\n\nGarbe, J., Albrecht, T., Levermann, A., Donges, J. F., & Winkelmann,\nR. (2020). The hysteresis of the Antarctic Ice Sheet.\nNature, 585(7826), 538–544. https://doi.org/10.1038/s41586-020-2727-5\n\n\nGiupponi, C., Ausseil, A.-G., Balbi, S., Cian, F., Fekete, A., Gain, A.\nK., Essenfelder, A. H., Martínez-López, J., Mojtahed, V., Norf, C.,\nRelvas, H., & Villa, F. (2022). Integrated modelling of\nsocial-ecological systems for climate change adaptation.\nSocio-Environmental Systems Modelling, 3, 18161–18161.\nhttps://doi.org/10.18174/sesmo.18161\n\n\nGoll, D., Heitzig, J., & Barfuss, W. (2024). Deterministic\nModel of Incremental Multi-Agent Boltzmann\nQ-Learning: Transient Cooperation,\nMetastability, and Oscillations\n(arXiv:2501.00160). arXiv. https://doi.org/10.48550/arXiv.2501.00160\n\n\nHoffman, M., & Yoeli, E. (2022). Hidden Games:\nThe Surprising Power of Game Theory to\nExplain Irrational Human Behaviour. Hachette UK.\n\n\nIzquierdo, L. R., Izquierdo, S. S., & Sandholm, W. H. (2024).\nAgent-Based Evolutionary Game Dynamics. https://doi.org/10.5281/zenodo.13938500\n\n\nLenton, T. M., Armstrong McKay, D. I., Loriani, S., Abrams, J. F., Lade,\nS. J., Donges, J. F., Milkoreit, M., Powell, T., Smith, S. R., Zimm, C.,\nBuxton, J. E., Bailey, E., Laybourn, L., Ghadiali, A., & Dyke, J. G.\n(Eds.). (2023). The Global Tipping Points Report\n2023. https://global-tipping-points.org\n\n\nLevin, S., & Xepapadeas, A. (2021). On the Coevolution\nof Economic and Ecological Systems. Annual\nReview of Resource Economics, 13(1), 355–377. https://doi.org/10.1146/annurev-resource-103020-083100\n\n\nMacy, M. W., & Flache, A. (2002). Learning dynamics in social\ndilemmas. Proceedings of the National Academy of Sciences,\n99(suppl_3), 7229–7236. https://doi.org/10.1073/pnas.092080099\n\n\nMarescot, L., Chapron, G., Chadès, I., Fackler, P. L., Duchamp, C.,\nMarboutin, E., & Gimenez, O. (2013). Complex decisions made simple:\nA primer on stochastic dynamic programming. Methods in Ecology and\nEvolution, 4(9), 872–884. https://doi.org/10.1111/2041-210X.12082\n\n\nMeadows, D. H. (2009). Thinking in systems: A primer.\nEarthscan.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\nBellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K.,\nOstrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I.,\nKing, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D.\n(2015). Human-level control through deep reinforcement learning.\nNature, 518(7540), 529–533. https://doi.org/10.1038/nature14236\n\n\nMotesharrei, S., Rivas, J., & Kalnay, E. (2014). Human and nature\ndynamics (HANDY): Modeling inequality and use\nof resources in the collapse or sustainability of societies.\nEcological Economics, 101, 90–102. https://doi.org/10.1016/j.ecolecon.2014.02.014\n\n\nMüller, B., Hoffmann, F., Heckelei, T., Müller, C., Hertel, T. W.,\nPolhill, J. G., van Wijk, M., Achterbosch, T., Alexander, P., Brown, C.,\nKreuer, D., Ewert, F., Ge, J., Millington, J. D. A., Seppelt, R.,\nVerburg, P. H., & Webber, H. (2020). Modelling food security:\nBridging the gap between the micro and the macro scale.\nGlobal Environmental Change, 63, 102085. https://doi.org/10.1016/j.gloenvcha.2020.102085\n\n\nMüller-Hansen, F., Cardoso, M. F., Dalla-Nora, E. L., Donges, J. F.,\nHeitzig, J., Kurths, J., & Thonicke, K. (2017). A matrix clustering\nmethod to explore patterns of land-cover transitions in\nsatellite-derived maps of the Brazilian Amazon.\nNonlinear Processes in Geophysics, 24(1), 113–123. https://doi.org/10.5194/npg-24-113-2017\n\n\nNowak, M. A. (2006). Five Rules for the\nEvolution of Cooperation. Science. https://doi.org/10.1126/science.1133755\n\n\nOstrom, E. (1990). Governing the commons: The evolution\nof institutions for collective action. Cambridge university press.\n\n\nOstrom, E., Dietz, T., Dolšak, N., Stern, P. C., Stonich, S., &\nWeber, E. U. (Eds.). (2002). The drama of the commons. National\nAcademies Press. http://www.nap.edu/catalog/10287\n\n\nPage, S. E. (2018). The model thinker: What you need to\nknow to make data work for you. Basic Books.\n\n\nPolasky, S., Carpenter, S. R., Folke, C., & Keeler, B. (2011).\nDecision-making under great uncertainty: Environmental management in an\nera of global change. Trends in Ecology & Evolution,\n26(8), 398–404. https://doi.org/10.1016/j.tree.2011.04.007\n\n\nRaworth, K. (2017). Doughnut economics: Seven ways to think like a\n21st-century economist. Chelsea Green Publishing.\n\n\nReyers, B., Moore, M.-L., Haider, L. J., & Schlüter, M. (2022). The\ncontributions of resilience to reshaping sustainable development.\nNature Sustainability, 1–8. https://doi.org/10.1038/s41893-022-00889-6\n\n\nRockström, J., Gaffney, O., Rogelj, J., Meinshausen, M., Nakicenovic,\nN., & Schellnhuber, H. J. (2017). A roadmap for rapid\ndecarbonization. Science. https://doi.org/10.1126/science.aah3443\n\n\nSayama, H. (2023). Introduction to the Modeling and\nAnalysis of Complex Systems. https://math.libretexts.org/Bookshelves/Scientific_Computing_Simulations_and_Modeling/Book%3A_Introduction_to_the_Modeling_and_Analysis_of_Complex_Systems_(Sayama)\n\n\nScheffer, M., Carpenter, S., Foley, J. A., Folke, C., & Walker, B.\n(2001). Catastrophic shifts in ecosystems. Nature,\n413(6856), 591–596. https://doi.org/10.1038/35098000\n\n\nSchill, C., Anderies, J. M., Lindahl, T., Folke, C., Polasky, S.,\nCárdenas, J. C., Crépin, A.-S., Janssen, M. A., Norberg, J., &\nSchlüter, M. (2019). A more dynamic understanding of human behaviour for\nthe Anthropocene. Nature Sustainability,\n2(12), 1075–1082.\n\n\nSchlüter, M., Baeza, A., Dressler, G., Frank, K., Groeneveld, J., Jager,\nW., Janssen, M. A., McAllister, R. R. J., Müller, B., Orach, K.,\nSchwarz, N., & Wijermans, N. (2017). A framework for mapping and\ncomparing behavioural theories in models of social-ecological systems.\nEcological Economics, 131, 21–35. https://doi.org/10.1016/j.ecolecon.2016.08.008\n\n\nSchultz, W., Stauffer, W. R., & Lak, A. (2017). The phasic dopamine\nsignal maturing: From reward via behavioural activation to formal\neconomic utility. Current Opinion in Neurobiology, 43,\n139–148. https://doi.org/10.1016/j.conb.2017.03.013\n\n\nSmaldino, P. E. (2017). Models Are Stupid, and We\nNeed More of Them. In R. R. Vallacher, S. J. Read,\n& A. Nowak (Eds.), Computational Social\nPsychology (1st ed., pp. 311–331). Routledge. https://doi.org/10.4324/9781315173726-14\n\n\nSteffen, W., Broadgate, W., Deutsch, L., Gaffney, O., & Ludwig, C.\n(2015). The trajectory of the Anthropocene: The Great\nAcceleration. The Anthropocene Review, 2(1),\n81–98. https://doi.org/10.1177/2053019614564785\n\n\nSteffen, W., Rockström, J., Richardson, K., Lenton, T. M., Folke, C.,\nLiverman, D., Summerhayes, C. P., Barnosky, A. D., Cornell, S. E.,\nCrucifix, M., Donges, J. F., Fetzer, I., Lade, S. J., Scheffer, M.,\nWinkelmann, R., & Schellnhuber, H. J. (2018). Trajectories of the\nEarth System in the Anthropocene.\nProceedings of the National Academy of Sciences,\n115(33), 8252–8259. https://doi.org/10.1073/pnas.1810141115\n\n\nSterman, J. D., & Sweeney, L. B. (2007). Understanding public\ncomplacency about climate change: Adults’ mental models of climate\nchange violate conservation of matter. Climatic Change,\n80(3), 213–238. https://doi.org/10.1007/s10584-006-9107-5\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An\nintroduction (Second edition). The MIT Press.\n\n\nWilliams, B. K. (2009). Markov decision processes in natural resources\nmanagement: Observability and uncertainty. Ecological\nModelling, 220(6), 830–840. https://doi.org/10.1016/j.ecolmodel.2008.12.023",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "01.02ex-IntroToPython.html",
    "href": "01.02ex-IntroToPython.html",
    "title": "Ex | Introduction to Python",
    "section": "",
    "text": "Set up\nThis text is written in a computer file called Jupyter Notebook, typically with the extension .ipynb. Jupyter is an interactive platform where you can write code and text and make visualizations. To run the notebook, i.e., to execute the Python code inside, you need to connect the notebook to a so-called Python kernel or runtime. There are two options: you run the kernel locally on your computer or in the cloud on the internet.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Introduction to Python"
    ]
  },
  {
    "objectID": "01.02ex-IntroToPython.html#exercise-sustainability-systems-science-generator",
    "href": "01.02ex-IntroToPython.html#exercise-sustainability-systems-science-generator",
    "title": "Ex | Introduction to Python",
    "section": "Exercise 📝: Sustainability Systems Science Generator",
    "text": "Exercise 📝: Sustainability Systems Science Generator\nThere are many word combinations in the literature that broadly refer to the kind of science we will be exploring in this course. Take, for example, the word combination,\nCoupled Social-Ecological Systems Modeling.\nYour task is to write a Python function that randomly generates alternative names for this kind of science. Tip: Start by considering alternatives for each part of the word combination above.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Introduction to Python"
    ]
  },
  {
    "objectID": "01.02ex-IntroToPython.html#saving-notebooks",
    "href": "01.02ex-IntroToPython.html#saving-notebooks",
    "title": "Ex | Introduction to Python",
    "section": "Saving notebooks",
    "text": "Saving notebooks\nThe easiest way to save a notebook as a PDF file for sharing is to print it via your browser’s print dialogue.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Introduction to Python"
    ]
  },
  {
    "objectID": "01.02ex-IntroToPython.html#footnotes",
    "href": "01.02ex-IntroToPython.html#footnotes",
    "title": "Ex | Introduction to Python",
    "section": "",
    "text": "See also 1  Introduction to Python and Jupyter Notebooks and 01.01-Getting-Started-with-Python-and-Jupyter-Notebooks, which served as valuable sources for this tutorial.↩︎\nsee 2  Data structures – Introduction to Data Science with Python for a more detailed exposition.↩︎",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Introduction to Python"
    ]
  },
  {
    "objectID": "02.01ex-Nonlinearity.html",
    "href": "02.01ex-Nonlinearity.html",
    "title": "Ex | Nonlinearity",
    "section": "",
    "text": "Model 1 | Human-Nature interactions\nIn the lecture, we asked the question, whether the osscilations we observed in the interaction model between human economic captial (\\(y\\)) and natural capital (\\(x\\)),\n\\[\\begin{align}\nx_{t+1} &= x_t - a y_t \\\\\ny_{t+1} &= y_t + b x_t\n\\end{align}\\]\nare due to a special set of parameters (\\(a\\) and \\(b\\)), or whether they are a general feature of the system’s structure.\nNow, you will use Python’s sympy library to investigate this question. sympy is a powerful library for symbolic mathematics. It allows you to define variables and equations symbolically, and then solve these equations symbolically. This is very useful for investigating the properties of mathematical models.\nimport sympy as sp\nFirst, we define the symbols for the variables in the model:\nx, y, a, b = sp.symbols('x y a b')",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Nonlinearity"
    ]
  },
  {
    "objectID": "02.01ex-Nonlinearity.html#model-1-human-nature-interactions",
    "href": "02.01ex-Nonlinearity.html#model-1-human-nature-interactions",
    "title": "Ex | Nonlinearity",
    "section": "",
    "text": "02.01-EconomyNature.dio.png\n\n\n\n\n\n\n\nStep 1: Coefficient matrix\nFormulate a sympy.Matrix object A that represents the coefficient matrix of the system of equations, \\(\\mathbf x_{t+1} = A \\mathbf x_t\\).\n\n# ...\n\n\n\nStep 2: Calculate the eigenvalues of the matrix A\nNow, calculate the eigenvalues of the matrix A using the A.eigenvals() method.\n\n# ...\n\n\n\nStep 3: Interpret the results\nWhat do the eigenvalues tell you about the stability of the system? Are the oscillations in the system due to a special set of parameters, or are they a general feature of the system’s structure? Write your answer in the markdown cell below.\n…\nAs discussed in the lecture, the fact that economic and natural captial may enter negative values is not very intuitive. Therefore, we will now refine the model, turning it into a model with nonlinear changes.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Nonlinearity"
    ]
  },
  {
    "objectID": "02.01ex-Nonlinearity.html#model-2-lotka-volterra-equations",
    "href": "02.01ex-Nonlinearity.html#model-2-lotka-volterra-equations",
    "title": "Ex | Nonlinearity",
    "section": "Model 2 | Lotka-Volterra equations",
    "text": "Model 2 | Lotka-Volterra equations\nDuring this part, you will use Python to implement and investigate the Lotka-Volterra equations, also known as the predator-prey model. The predator-prey equations are an iconic model in population ecology, which describe the dynamics of biological systems in which two species interact, one as a predator and the other as prey. It is the foundation for many dynamic system models of human-enviornment interactions, where human societies are the predators and the natural environment the prey. The Easter Island model (Brander & Taylor, 1998) or the HANDY (human and nature dynamics) model (Motesharrei et al., 2014) are prototypical examples of such models; (see also Section 2 of the review by Farahbakhsh et al., 2022).\nIn discrete time, the Lotka-Volterra equations read\n\\[\\begin{align}\n\\Delta x &= x_{t+1} - x_t = \\alpha x_t - \\beta x_t y_t \\\\\n\\Delta y &= y_{t+1} - y_t = \\delta x_t y_t - \\gamma y_t\n\\end{align}\\]\nwhere, in our case, \\(x\\) denotes the health of the natural environment and \\(y\\) the level of development of a human society. The parameters \\(\\alpha\\), \\(\\beta\\), \\(\\delta\\), and \\(\\gamma\\) are positive constants that determine the dynamics of the system. The parameter \\(\\alpha\\) represents the natural growth rate of the environment, \\(\\beta\\) the rate at which the human society depletes the environment, \\(\\delta\\) the rate at which the human society grows by exploiting the environment, and \\(\\gamma\\) the natural decay rate of the human society.\n\nModel 2 | The Lotka-Volterra equations\nYou will use the following set of parameters as default values:\n\n# Parameters\nalpha = 0.1  # Growth rate of prey\nbeta = 0.02  # Rate at which predators destroy prey\ngamma = 0.3  # Death rate of predators\ndelta = 0.01 # Rate at which predators increase by consuming prey\n\n# Initial conditions\nX = 40  # Initial prey population\nY = 9   # Initial predator population\n\n\n\nStep 1 | Implement the model\nWrite a function called lotkavolterra that implements the update of the Lotka-Volterra equations.\n\n# ...\n\nWrite (or copy-paste-and-adjust) an iterate_model function that iterates a dynamic systems model forward in time, given an initial state and a function (plus its parameters) that updates the state.\n\n# ...\n\nWrite (or copy-paste-and-adjust) a plot_stock_evolution function that plots the time series of the state variables. Label the two-dimensional output of our system as “Natural environment” and “Human society”.\n\n# ...\n\n\n\nStep 2 | Plot the time series\nVisualize the time series of the natural environment and the human society for the default parameter values for 100 time steps.\n\n# ...\n\nYou should observe oscillations in the time series of the natural environment and the human society. Crucially, natural environment and human socitey values (almost) never become negative.\nOptional | Feel free to interact with the time series plot using the interact function.\n\n# ...\n\n\n\nStep 3 | Visualize the dynamics in the phase space\nAdjust the plot_flow function from the lecture material to visualize the dynamics of the system in the phase space. The updated plot_flow function should take the following arguments (and default values):\n\nupdate_func: the function that updates the state of the system\nxextent=10: the x-axis extent of the phase space plot\nyextent=10: the y-axis extent of the phase space plot\nnr_points=11: the number of points in the phase space plot\nax=None: the axis object to plot on\n**update_params: additional keyword arguments for the update_func\n\n\n# ...\n\nVisualize the dynamics of the Lotka-Volterra equations in the phase space for the default parameter values and an xextent of 70.\n\n# ...\n\nNow, adjust the plot_flow_trajectory function from the lecture material to visualize the trajectory of the system in the phase space next to the time series plot. The updated plot_flow_trajectory function should take the following arguments (and default values):\n\nnr_timesteps: the number of time steps to simulate\ninitial_value: the initial value of the state variables\nupdate_func: the function that updates the state of the system\nxextent=10: the x-axis extent of the phase space plot\nyextent=10: the y-axis extent of the phase space plot\nnr_points=11: the number of points in the phase space plot\n**update_params: additional keyword arguments for the update_func\n\n\n# ...\n\nVisualize a trajectory from the initial values (\\(x_0 = 28, y_0=5\\)) of the system in the phase space next to the time series plot for the default parameter values and 250 time steps. The xextent should be 70.\n\n# ...\n\nYou should observe that the system diverges with ossiclations around an equilibrium point that are more complex than simple sin and cosin fuctions.\nOptional | Feel free to interact with plot using the interact function.\n\n# ...\n\nBut where exactly are the equilibirum points of the system?\n\n\nStep 4 | Equilibrium points\nCalculate the analytical solutions for the equilibrium points \\(x_e\\) of the Lotka-Volterra system, i.e., how \\(x_e\\) and \\(y_e\\) depend on the parameters \\(\\alpha\\), \\(\\beta\\), \\(\\delta\\), and \\(\\gamma\\).\nYou can do this either by hand or using the sympy library.\n\n# ...\n\nYou should have found two equilibrium points. Include both in the visualization of the phase space trajectory from the initial values (\\(x_0 = 28, y_0=5\\)) for the default parameter values and 250 time steps. The xextent should be 70.\n\n# ...\n\n\n\nStep 5 | Derive the Jacobian matrix\nThe Jacobian matrix of the descrete-time system (\\(\\mathbf x_{t+1} = F(\\mathbf x_t)\\)) is the multidimensional version of the derivative. It is a matrix of all first-order partial derivatives of a vector-valued function. The Jacobian matrix of a two-dimensional system is given by\n\\[\\begin{equation}\nJ = \\begin{bmatrix}\n\\frac{\\partial F_x}{\\partial x} & \\frac{\\partial F_x}{\\partial y} \\\\\n\\frac{\\partial F_y}{\\partial x} & \\frac{\\partial F_y}{\\partial y}\n\\end{bmatrix},\n\\end{equation}\\]\nwhere \\(F_x\\) and \\(F_y\\) are the two functions that describe the dynamics of the \\(x\\) and \\(y\\) component of the system.\nYou can do this either by hand or using the sympy library.\n…\n\\[\\begin{equation}\nJ = \\begin{bmatrix}\n1+\\alpha -\\beta y & - \\beta x \\\\\n\\delta y & 1 + \\delta x - \\gamma\n\\end{bmatrix}.\n\\end{equation}\\]\n\n\nStep 6 | Eigenvalues at the equilibrium points\nCompute the eigenvalues of the Jacobian matrix at the two equilibrium points using the symbolic mathematics library sympy.\n\n# ...\n\nSimilarly as in the first model, you should observe that for positive parameter values, you always have to take the square root of a negative number, leading to complex numbers which indicates that the system oscillates around the equilibrium points.\nIn the basic Lotka-Volterra model, the natural environment would grow indefinitely without human society, which is not very realistic. Therefore, we will now refine the model, make the growth of the natural environment logistic.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Nonlinearity"
    ]
  },
  {
    "objectID": "02.01ex-Nonlinearity.html#model-3-extended-lotka-volterra-model",
    "href": "02.01ex-Nonlinearity.html#model-3-extended-lotka-volterra-model",
    "title": "Ex | Nonlinearity",
    "section": "Model 3 | Extended Lotka-Volterra model",
    "text": "Model 3 | Extended Lotka-Volterra model\nWe extent the Lotka-Volerra model such that the natural environment has a finite carrying capacity \\(C\\) and its natural growth follows the logistic map,\n\\[\\begin{align}\n\\Delta x &= x_{t+1} - x_t = \\alpha x_t (1 - \\frac{x_t}{C}) - \\beta x_t y_t, \\\\\n\\Delta y &= y_{t+1} - y_t = \\delta x_t y_t - \\gamma y_t.\n\\end{align}\\]\nAll other parameters remain the same.\n\nStep 1 | Implement the model\nWrite a function called lotkavolterraX that implements the update of the extended Lotka-Volterra equations.\n\n# ...\n\n\n\nStep 2 | Visualize the dynamics in the phase space\nFor the default parameters and a carrying capacity of \\(C=40\\), visualize a trajectory of the dynamics of the extended Lotka-Volterra equations in the phase space, using the plot_flow_trajectory function from above. The initial values should be \\(x_0 = 40\\) and \\(y_0=0.001\\), representing a natural environment in equilibirum and a human society that is just starting to grow. Simulate the system for 250 time steps. Set the xextent to 50, and the yextent to 3.\n\n# ...\n\nBriefly describe and interpret the dynamics you observe in the markdown cell below.\n…\nLast, plot the evolution of the human society trajectory alone. What do you observe?\n\n# ...\n\n\n\nOptional step | Eigenvalues at the equilibrium points\nUse sympy to calculate the eigenvalues at the equilibirum points. What do they tell about the stability of the system?\n\n# ...\n\n\n\n\n\nBrander, J. A., & Taylor, M. S. (1998). The Simple Economics of Easter Island: A Ricardo-Malthus Model of Renewable Resource Use. The American Economic Review, 88(1), 119–138. https://www.jstor.org/stable/116821\n\n\nFarahbakhsh, I., Bauch, C. T., & Anand, M. (2022). Modelling coupled human–environment complexity for the future of the biosphere: Strengths, gaps and promising directions. Philosophical Transactions of the Royal Society B: Biological Sciences, 377(1857), 20210382. https://doi.org/10.1098/rstb.2021.0382\n\n\nMotesharrei, S., Rivas, J., & Kalnay, E. (2014). Human and nature dynamics (HANDY): Modeling inequality and use of resources in the collapse or sustainability of societies. Ecological Economics, 101, 90–102. https://doi.org/10.1016/j.ecolecon.2014.02.014",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Nonlinearity"
    ]
  },
  {
    "objectID": "02.02ex-TippingElements.html",
    "href": "02.02ex-TippingElements.html",
    "title": "Ex | Tipping elements",
    "section": "",
    "text": "Robustness of the tipping elements model\nIn this exercise, we will investigate the robustness of the tipping elements model. We will do this changing the functional forms of the reinforcing and balancing feedback loops.\n\\[\\Delta x =  (x^3 - ax^5 + c) \\frac{1}{\\tau},\\]\nwhere, as in the model of the lecture, \\(\\tau\\) represents the typical time scale of the system, and thus, inverse strength of the system’s change, and \\(a\\) is a parameter that determines the strength of the balancing feedback loop in relation to the reinforcing feedback loop (with unit stength). Compare this form with the one in the lecture. What is the difference?",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Tipping elements"
    ]
  },
  {
    "objectID": "02.02ex-TippingElements.html#robustness-of-the-tipping-elements-model",
    "href": "02.02ex-TippingElements.html#robustness-of-the-tipping-elements-model",
    "title": "Ex | Tipping elements",
    "section": "",
    "text": "02.02-TippingModel.dio.png\n\n\n\n\n\nStep 1 | Time evolution\nImplement the update_function for this new model in Python.\n\n# ...\n\nShowcae the bistability of this model by plotting the time evoltion of the system state for different initial conditions.\n\n# ...\n\nVisualize the same inital conditions in a cobweb plot.\n\n# ...\n\n\n\nStep 2 | Bifurcation analysis\nConduct a bifurcation analysis accodring to the one in the lecture, including the calculation and plotting of the equilibirum points and their stability.\n\n# ...\n\n…\n\n# ...\n\n\n\nStep 3 | Potential function\nDerive and plot the potential function for this model\n\n# ...\n\n…\n\n# ...\n\n\n\nStep 4 | Conclusion\nBriefly summarize what your findings mean for the robustness of the tipping elements model.\n…",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Tipping elements"
    ]
  },
  {
    "objectID": "02.02ex-TippingElements.html#the-subcritical-pitchfork-bifurcation",
    "href": "02.02ex-TippingElements.html#the-subcritical-pitchfork-bifurcation",
    "title": "Ex | Tipping elements",
    "section": "The subcritical pitchfork bifurcation",
    "text": "The subcritical pitchfork bifurcation\nBifurcation theory orders different kinds of bifurcations. A so-called subcritical pitchfork bifurcation is defined as one where an unstable equilibrium point split into three, two unstable and one stable. Its difference equation (in normal form) is given by\n\\[\\Delta x =x^3 - cx,\\]\nwhere \\(c\\) is a parameter that controls the system’s stability and \\(x\\) is the system state.\nYour task is to implement this model and conduct a bifurcation analysis. We will also use this model in the next lecture.\n\nStep 1 | Stability analysis\nFind the critical thresholds of the parameter \\(c\\) at which bifurcations occur and study the stability of each equilibrium point in dependence of the parameter value.\n\n# ...\n\n…\n\n# ...\n\n\n\nStep 2 | Bifurcation diagramm\nDraw an analytical bifurcation diagram of this model for \\(-1&lt;c&lt;2\\), showing the equilibrium points and their stability.\n\n# ...\n\nNow, you should be able to observe why this bifurcation is called a pitchfork bifurcation.\n\n\nStep 3 | Potential function\nDerive and plot the potential function for this model. Also include the equilibirum points and their stability.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Tipping elements"
    ]
  },
  {
    "objectID": "02.02ex-TippingElements.html#another-kind-of-bifurcation-in-the-logistic-map",
    "href": "02.02ex-TippingElements.html#another-kind-of-bifurcation-in-the-logistic-map",
    "title": "Ex | Tipping elements",
    "section": "Another kind of bifurcation in the logistic map",
    "text": "Another kind of bifurcation in the logistic map\nIn this exercise we will investigate another kind of bifurcation in the logistic map. Let’s revisit the logistic map in the form, \\[x_{t+1} = cx_{t}(1-x_{t}),\\] where \\(c&gt;0\\).\n\nStep 1 | Stability analysis\nFind the critical thresholds of the parameter \\(c\\) at which bifurcations occur and study the stability of each equilibrium point in dependence of the parameter value.\n\n# ...\n\n…\n\n# ...\n\n\n\nStep 2 | Simulations\nSimulate the model with several selected values of to confirm the results of analysis.\n\n# ...\n\n…\n\n# ...\n\n\n\nStep 3 | Bifurcation diagramm\nDraw (simulate) a bifurcation diagram of this model for \\(0&lt;c&lt;4\\).\n\n# ...\n\n\n\nStep 4 | Sensitivity to initial conditions\nDraw two trajectories from almost the same but a different initial condition for a paramter value \\(3.7&lt;c&lt;4.0\\). Draw the two trajectories ontop of each other with different color and linestyle.\n\n# ...\n\nYou should see that the trajectories diverge after a while. This is called sensitive dependence on initial conditions, a key property of so-called chaotic systems.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Tipping elements"
    ]
  },
  {
    "objectID": "02.03ex-Resilience.html",
    "href": "02.03ex-Resilience.html",
    "title": "Ex | Resilience",
    "section": "",
    "text": "Discontinous systems\nIn this note, we used the classical dynamcial systems from bifurcation theory which are all continuous, meaning that we can write their functional form as one continuous mathematical equation.\nThe pictorial resilience models often portray a simple cup, such as this,\ndef G(x, a): return np.where(np.abs(x)&lt;a, a/2*x**2, None)\n\ndef plot_potential(a=1.0):\n    xs=np.linspace(-2,2,301)\n    plt.plot(xs, G(xs, a), color='blue')\n    plt.ylim(-0.1, 1.1); plt.xlim(-2, 2)\n    \ninteract(plot_potential, a=(0, 2., 0.01),);\nLet’s interpret this cup function as a quasi-potential for the following difference equation,\n\\[\nx_{t+1} =\n\\begin{cases}\n    x_{t} - \\frac{dG}{dx}(x_{t}) + bn_{t} = x_{t} - ax_t + bn_{t} & \\text{if } -a \\leq x \\leq a, \\\\\n    -10 & \\text{if } x &lt; -a \\\\\n    +10 & \\text{if } x &gt; a.\n\\end{cases}\n\\]\ndef F(x, a, b):\n    if x&lt;-a:\n        return -10.0\n    elif x&gt;a:\n        return 10.0\n    else:\n        return x - a*x + b*np.random.randn()",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Resilience"
    ]
  },
  {
    "objectID": "02.03ex-Resilience.html#discontinous-systems",
    "href": "02.03ex-Resilience.html#discontinous-systems",
    "title": "Ex | Resilience",
    "section": "",
    "text": "Showcase robustness resilience with this system\n\n# ...\n\n\n\nShowcase adaptation resilience with this system\n\n# ...\n\n\n\nShowcase the critical slowing with this system\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Resilience"
    ]
  },
  {
    "objectID": "02.03ex-Resilience.html#heavy-tailed-shocks",
    "href": "02.03ex-Resilience.html#heavy-tailed-shocks",
    "title": "Ex | Resilience",
    "section": "Heavy tailed shocks",
    "text": "Heavy tailed shocks\nSo far, we assumed the unpredictable and external shocks are distributed according to a normal distribution with mean zero.\nReal-world shocks may not exhibit this property. They often come with so-called heavy tails, meaning that large shocks are more probable compared to a normal distribution.\nThe Student’s t-distribution (or simply the t-distribution) is a continuous probability distribution that generalizes the standard normal distribution. Like the latter, it is symmetric around zero and bell-shaped. The t-distribution has one more parameter than the normal distribution, called the degrees of freedom, df.\n\nWhen df\\(\\rightarrow \\infty\\), the t-distribution becomes the normal distribution.\nWhen df\\(= 1\\), the t-distribution becomes the so-called Cauchy distribution.\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import t, norm, cauchy\n\nx = np.linspace(-5, 5, 100)\ndegrees_of_freedom = [1, 2, 5, 100]  # Varying degrees of freedom\n\n# Plotting T-distribution curves for different degrees of freedom\nfor df in degrees_of_freedom:\n    y = t.pdf(x, df)  # Using default location and scale parameters (0 and 1)\n    plt.plot(x, y, label=f\"Degrees of Freedom = {df}\")\n    \nz = norm.pdf(x)\nplt.plot(x, z, 'k--', label='Normal')\nz = cauchy.pdf(x)\nplt.plot(x, z, 'k:', label='Cauchy')\n\nplt.xlabel('x'); plt.ylabel('PDF'); plt.legend();\nplt.title('T-Distribution with Varying Degrees of Freedom');\n\n\n\n\n\n\n\n\n\nInvestigate the impact of heavy-tailed shocks on resilience.\nTip: Define a difference equation with t-distributed shocks and a degree-of-freedom parameter to control the shocks’ heavy-tailedness.\n\n# ...\n\n\n\nAutocorrelation with heavy-tailed shocks\nInvestigate how shocks’ heavy-tailedness impacts the lag-1 temporal autocorrelation early-warning indicator.\n\n# ..",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Resilience"
    ]
  },
  {
    "objectID": "02.04ex-StateTransitions.html",
    "href": "02.04ex-StateTransitions.html",
    "title": "Ex | State transitions",
    "section": "",
    "text": "Step 1 | Transition matrix\nSet up the transition matrix using the variable with the following default values,\npc = 0.005 # collapse probability\npr = 0.001 # recovery probability\npt = 0.05 # transformation probability\npo = 0.01 # overusage probability (relapse to prosperous state)\nTip: Self-transitions are not shown in the diagram, but they are possible.\n# T = ...\nMake sure and show below that the rows of the transition matrix sum up to 1, as required for a probabilistic transition matrix of a Markov chain.\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | State transitions"
    ]
  },
  {
    "objectID": "02.04ex-StateTransitions.html#step-2-simulation",
    "href": "02.04ex-StateTransitions.html#step-2-simulation",
    "title": "Ex | State transitions",
    "section": "Step 2 | Simulation",
    "text": "Step 2 | Simulation\nCompare the time-evolution of an ensemble stochastic runs with the time-evolution of the state distribution.\n\nStep 2.1 | Stochastic run\nFirst, simulate a stochastic run of the system for 100 time steps with the risky state as the initial state.\n\n# ...\n\nVisulaize the results in a plot.\n\n# ...\n\n\n\nStep 2.2 | Ensemble\nSecond, create an ensemble of 300 stochastic runs of the system for 100 time steps with the risky state as the initial state.\n\n# ...\n\nMake sure that your variable, into which you stored the runs of the ensemble, is a two-dimensional numpy array with shape (300, 100).\n\n# ..\n\n\n\nStep 2.3 | Visualize the ensemble\nVisualize each of the 300 trajectories in a time series plot by looping through the trajectories of the ensemble and plotting them in the same plot with a low alpha value (plt.plot(..., alpha=0.05)) to make the individual trajectories visible.\n\n# ...\n\nYou should realize, that we cannot visulaize the average of the ensemble in a single line plot, as we did in the lecture, because the state space has three states.\nTakeing the average of the numerical values representing the states does not make sense in this case. For example, if the risky state is represented by the number 1, the sustainable state by the number 0, and the degraded state by the number 2, the average of the ensemble could not distinguish between all probability in the risky state and 50% in the sustainable and degraded state.\nBut we still can visualize the evolution of the distribution of the ensemble at each time step.\nTo do so, we need to transform our ensemble of 300 stochastic runs into a distribution of the ensemble at each time step. We can do so by counting the number of state visits in the ensemble at each time step. The numpy.histogram function can help us with this task. Suppose that the ensemble is stored in the variable ensemble with shape (300, 100). Then, the following code snippet will count the number of visits of each state in the ensemble at the first time step:\nstochastic_evolution = np.array([np.histogram(samp, bins=3, range=(0,2))[0] for samp in ensemble.T])\nThe loop through ensemble.T iterates through each time step. The np.histogram function counts the number of visits of each state in the ensemble at each time step. The bins=3 argument specifies that we have three states, and the range=(0,2) argument specifies that the states’ numerical representations are in the range from 0 to 2. The [0] at the end of the list comprehension extracts the counts of the states from the histogram function.\nApply this code snippet to the ensemble and make sure that the variable stochastic_evolution has the shape (100, 3).\n\n# ...\n\nFinally, visualize the state distribution over time using the plt.imshow function. The x-axis should represent the time steps, and the y-axis should represent the states. The color intensity should represent the number of visits of each state at each time step. A code like this should do the job:\nplt.imshow(stochastic_evolution.T, aspect='auto', interpolation='None')\n\n# ...\n\n\n\nStep 2.4 | State distribution evolution\nTo compare this result to the time evolution of the state distribution, you first need to compute the latter.\n\n# ..\n\nCheck that the sum of your state distribution evolution at each time step is equal to 1.\n\n# ..\n\nFinally, visualize the state distribution evolution over time using the plt.imshow function as above.\n\n# ...\n\nYou should observe that both plot look similar, but the state distribution evolution is smoother than the ensemble plot. This is because the ensemble plot results from the individual trajectories, while the state distribution evolution plot shows the average of the ensemble at each time step.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | State transitions"
    ]
  },
  {
    "objectID": "02.04ex-StateTransitions.html#step-3-stationary-distribution",
    "href": "02.04ex-StateTransitions.html#step-3-stationary-distribution",
    "title": "Ex | State transitions",
    "section": "Step 3 | Stationary distribution",
    "text": "Step 3 | Stationary distribution\nCompute the stationary distribution of the system by computing the eigenvector of the transition matrix corresponding to the eigenvalue 1, first numerically and then analytically.\n\nStep 3.1 | Numerical solution\nUse the numpy.linal.eig method to compute the stationary distribution of the system. Make sure to normalize the correct eigenvector to sum up to 1, as it represents a probability distribution.\n\n# ...\n\nHow does the result compare to the time-evolution above?\nDoes the system reach the stationary distribution at the end?\nIf not, what does that say about the time evolution simulations above?",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | State transitions"
    ]
  },
  {
    "objectID": "03.01ex-SequentialDecisions.html",
    "href": "03.01ex-SequentialDecisions.html",
    "title": "Ex | Sequential Decisions",
    "section": "",
    "text": "Step 1 | Transition and rewards tensors\nTransform the description of the MDP into two Python functions that can be called with the model parameters and return a transition and reward tensor, respectively. Make sure to write these functions to be used both with numpy and sympy.\n# ...\nTest that both functions work and check that the transition tensors are proper probability distributions for three exemplary parameter combinations.\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Sequential Decisions"
    ]
  },
  {
    "objectID": "03.01ex-SequentialDecisions.html#step-2-state-values",
    "href": "03.01ex-SequentialDecisions.html#step-2-state-values",
    "title": "Ex | Sequential Decisions",
    "section": "Step 2 | State values",
    "text": "Step 2 | State values\nWrite a Python function to numerically compute the MDP’s state values, given a policy_Xsa, a transitions_Tsas tensor, a rewards_Rsas tensor, and a discount factor dcf.\n\n# ...\n\nTest that your function works with some arbitrary values for the policy_Xsa, transitions_Tsas tensor, rewards_Rsas tensors, and discount factor dcf.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Sequential Decisions"
    ]
  },
  {
    "objectID": "03.01ex-SequentialDecisions.html#step-3-policies",
    "href": "03.01ex-SequentialDecisions.html#step-3-policies",
    "title": "Ex | Sequential Decisions",
    "section": "Step 3 | Policies",
    "text": "Step 3 | Policies\nFormulate four different policies and represent them in Python:\nThe cautious policy always chooses the low-intensity action.\n\n# ...\n\nThe risky policy chooses the high-intensity action in the risky and sustainable state and the low-intensity action in the degraded state.\n\n# ...\n\nThe transformation policy chooses the transformation action in the risky state and the low-intensity action in the sustainable and degraded state.\n\n# ...\n\nThe overuse policy chooses the transformation action in the risky and sustainable state and the low-intensity action in the degraded state.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Sequential Decisions"
    ]
  },
  {
    "objectID": "03.01ex-SequentialDecisions.html#step-4-optimal-policy",
    "href": "03.01ex-SequentialDecisions.html#step-4-optimal-policy",
    "title": "Ex | Sequential Decisions",
    "section": "Step 4 | Optimal policy",
    "text": "Step 4 | Optimal policy\nWhat are the state values of the risky state for all the four policies at the parameter combination \\(p_c=0.2\\), \\(p_r=0.01\\), \\(p_t=0.04\\), \\(p_d=0.005\\), \\(p_o=0.02\\), \\(\\gamma=0.98\\), \\(r_o=1.2\\), \\(r_s=1.0\\), \\(r_h=0.9\\), \\(r_l=0.7\\), \\(r_t=0.65\\), \\(r_d=0.0\\)?\n\npc = 0.2; pr = 0.01; pt = 0.04; pd = 0.005; po = 0.02;\nro = 1.2; rs = 1.0; rh = 0.9; rl = 0.7; rt = 0.65; rd = 0.0\n\n\n# ...\n\nWhich is, therefore, the optimal policy for that parameter combination?\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Sequential Decisions"
    ]
  },
  {
    "objectID": "03.01ex-SequentialDecisions.html#step-5-optimal-policies-with-uncertainty",
    "href": "03.01ex-SequentialDecisions.html#step-5-optimal-policies-with-uncertainty",
    "title": "Ex | Sequential Decisions",
    "section": "Step 5 | Optimal policies with uncertainty",
    "text": "Step 5 | Optimal policies with uncertainty\nGiven the other above parameters, how does the optimal policy change with a varying discount factor \\(\\gamma\\)? Create a plot that shows the state values of the risky state for the four policies at \\(\\gamma \\in [0.001, 0.9999]\\) and interpret your result.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Sequential Decisions"
    ]
  },
  {
    "objectID": "03.02ex-StrategicInteractions.html",
    "href": "03.02ex-StrategicInteractions.html",
    "title": "Ex | Strategic Interactions",
    "section": "",
    "text": "Step 1 | Tragedy Dilemma\nNext to the model presented in the lecture, another common parametrization of the tragedy dilemma is the following: Actors can either cooperate or defect. Each cooperator contributes \\(c &gt; 0\\) to the public good at an individual cost of \\(c\\). The sum of all contributions is multiplied by a synergy factor \\(r\\) and then equally distributed among all actors. The payoff functions are given by:\n\\[\\begin{align}\nR_c &= \\frac{r c (N_c +1)}{N}  - c, \\\\\nR_d &= \\frac{r c N_c}{N},\n\\end{align}\\]\nwith \\(N_c\\) being the number of other actors cooperating and \\(N\\) being the total number of actors.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Strategic Interactions"
    ]
  },
  {
    "objectID": "03.02ex-StrategicInteractions.html#step-1-tragedy-dilemma",
    "href": "03.02ex-StrategicInteractions.html#step-1-tragedy-dilemma",
    "title": "Ex | Strategic Interactions",
    "section": "",
    "text": "Step 1.1 | Visualization\nPlot the payoff functions for cooperators and defectors as a function of \\(N\\) for different values of \\(r\\). Compare the results to the tragedy dilemma model presented in the lecture.\n\n# ...\n\n\n\nStep 1.2 | Conditions\nGive the conditions the parameters must hold for this model to be a tragedy dilemma.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Strategic Interactions"
    ]
  },
  {
    "objectID": "03.02ex-StrategicInteractions.html#step-2-agreements",
    "href": "03.02ex-StrategicInteractions.html#step-2-agreements",
    "title": "Ex | Strategic Interactions",
    "section": "Step 2 | Agreements",
    "text": "Step 2 | Agreements\nApply the reasoning from the lecture to compute how the critical participation levels depend on the parameters of the model \\(r\\), \\(c\\), and \\(N\\).\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Strategic Interactions"
    ]
  },
  {
    "objectID": "03.02ex-StrategicInteractions.html#step-3-threshold-public-goods",
    "href": "03.02ex-StrategicInteractions.html#step-3-threshold-public-goods",
    "title": "Ex | Strategic Interactions",
    "section": "Step 3 | Threshold Public Goods",
    "text": "Step 3 | Threshold Public Goods\nLet us now consider a variant of threshold public good, where the catastrophic impact \\(m\\) occurs probabilistically, and each polluting actor increases the probability of collapse. We assume that if all actors pollute, there is a probability \\(q_c\\) of collapse; if all actors cooperate, there is zero probability of collapse. The collapse probability increases linearly with the number of polluting actors \\(N_p\\), i.e., \\[ p_c = q_c \\frac{N_p}{N}.\\]\nFurthermore, we assume that if the collapse occurs, the actors won’t receive the payoffs from the public. The payoff functions are given by:\n\\[\\begin{align}\nR_a(N_a; r, c, q_c, m, N) &= (1 - q_c N_\\mathsf{p}/N) \\cdot (rc(N_\\mathsf{a}+1)/N - c) +  q_c N_\\mathsf{p}/N \\cdot m, \\\\\nR_p(N_a; r, c, q_c, m, N) &= (1 - q_c (N_\\mathsf{p}+1)/N) \\cdot (rcN_\\mathsf{a})/N) +  q_c (N_\\mathsf{p}+1)/N \\cdot m,\n\\end{align}\\]\nwhere \\(N_a\\) is the number of other actors abating and \\(N_p\\) is the number of other actors polluting. Thus, it must hold that the total number of actors \\(N = N_a + N_p + 1\\). Furthermore, \\(r\\) is the synergy factor, \\(c\\) is the cost of cooperation, \\(q_c\\) is the probability of collapse if all actors pollute, and \\(m\\) is the catastrophic impact.\n\nStep 3.1 | Visualization\nPlot the payoff functions for abating and polluting actors as a function of \\(N_a\\) for \\(f=4\\), \\(c=5\\), \\(m=-5\\) \\(qc=0.4\\), and \\(N=5\\). Compare the results to the threshold public goods model presented in the lecture.\n\n# ...\n\n\n\nStep 3.1 | Conditions\nCalculate the three critical conditions for this game of\n\nDilemma, i.e., the actors are indifferent between all abating and all polluting,\nGreed, i.e., the actors are indifferent between abating and polluting, give all others abate, and\nFear, i.e., the actors are indifferent between abating and polluting, given all others pollute.\n\nSolve the conditions for the collapse impact \\(m\\).\nYou may do this by hand or by using the sympy library. I recommend the latter.\n\n# ...\n\nVisualize the critical conditions for the collapse impact \\(m\\) as a function of \\(q_c\\) for \\(r=1.2\\), \\(c=5\\), and \\(N=2\\). Interpret the results briefly.\n\n# ...\n\nVisualize the critical conditions for the collapse impact \\(m\\) as a function of \\(N \\in [2, 3, \\dots, 15]\\) for \\(r=3\\), \\(c=5\\), and \\(qc=0.5\\). Interpret the results briefly.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Strategic Interactions"
    ]
  },
  {
    "objectID": "03.03ex-DynamicInteractions.html",
    "href": "03.03ex-DynamicInteractions.html",
    "title": "Ex | Dynamic Interactions",
    "section": "",
    "text": "Model description\nTwo representative decision-makers repeatedly interact (in discrete time) within an environment of two states, a prosperous and a degraded one. In the prosperous state, each agent can choose between a conservation and an intensification action, which affects the environment. For example, intensification corresponds to emitting a large, business-as-usual amount of carbon into the atmosphere, a high rate of rainforest deforestation, or a high level of freshwater extraction. In contrast, conservation corresponds to a significantly reduced amount of greenhouse gas emissions, rate of deforestation, or level of fresh-water extraction within the recovery capacity of the environment.\nProvisioning goods and immediate benefits. In a prosperous environment, the biosphere’s maintenance and regulating services function sufficiently well, and the environment can deliver provisioning goods to the agents. We assume these goods are entirely private, i.e., their benefits do not depend on the other agent’s action. Each agent opting for conservation receives a benefit \\(b_\\textsf{C}\\). Each agent opting for intensification receives a higher benefit \\(b_\\textsf{I} &gt; b_\\textsf{C}.\\) In the degraded state, however, the biosphere’s maintenance and regulating services are not functioning. Thus, whenever the environment collapses into or is in the degraded state, all agents receive only a benefit \\(b_\\textsf{D} &lt; b_\\textsf{C}\\) independent of their action.\nTo reduce the number of free model parameters, we summarize these three benefits into a single relative benefit ratio, \\(\\Delta r = f(b_\\textsf{C}) \\in [0, 1]\\) by transforming all benefit parameters according to \\(f(x) =(x - b_\\textsf{D}) / (b_\\textsf{I} - b_\\textsf{D})\\). \\(\\Delta r\\) describes how much less immediate benefit conservation delivers (in relation to the collapse impact \\(b_\\textsf{D}\\)) compared to intensification. Thus, a larger \\(\\Delta r\\) denotes either a larger conservation benefit, a smaller, more severe degradation benefit, or a smaller intensification benefit.\nEcological tipping and transitions. However, each agent employing intensification also increases the probability of triggering the tipping element and collapsing the environment into the degraded state. We parameterize the overall collapse probability \\(p_c\\) by the collapse leverage \\(q_c \\in [0,1)\\) each intensification actor exerts on the environment. The total collapse probability \\(p_c=0\\), if no actor chooses intensification, \\(p_c=q_c/2\\) if one actor chooses intensification, and \\(p_c=q_c\\) if both do. In the degraded state, the agents cannot influence the environment and have to wait on average for \\(1/p_r\\) rounds, parameterized by the recovery probability \\(p_r\\), until they enter the prosperous state again.\nDecision model. We assume that each agent’s conservation or intensification strategy is conditional solely on the current environmental state. They do not take longer histories, their own past choices, nor the choices of the other agent into account - either because they do not have the cognitive resources for more complex strategies or because they cannot observe or make sense of the other agent’s actions. But we assume that agents can plan their course of action into the future and that they care for future rewards but exponentially discount them with their discount factor \\(\\gamma \\in [0, 1)\\).\nNote that all direct social interaction in the model is deliberately stripped away. The agents’ choices do not influence the immediate benefit of the other agent. Agents are self-interested and do not consider the other agent’s actions for their strategy. Social interaction is only mediated indirectly through the environment. This is not to say that such direct social interaction does not exist.\nOur model aims to isolate the cooperation-promoting effects of the actors’ shared ecological embeddedness. There is already good evidence for the beneficial effect of direct social interaction on collective action (Nowak, 2006; Ostrom, 1990). We aim to assess the prospects for collective cooperation when such mechanisms cannot work, either because of the anonymity or the scale of the problem.\nIn summary, our theoretical model is determined by only four parameters, two ecological and two social ones: the collapse leverage \\(q_c\\), the recover probability \\(p_r\\), the discount factor \\(\\gamma\\), and the relative benefit ratio \\(\\Delta r\\). All of them are in the range between 0 and 1.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Dynamic Interactions"
    ]
  },
  {
    "objectID": "03.03ex-DynamicInteractions.html#task",
    "href": "03.03ex-DynamicInteractions.html#task",
    "title": "Ex | Dynamic Interactions",
    "section": "Task",
    "text": "Task\nVisualize the critical curves where the social dilemma incentive regimes change - solving for the discount factor \\(\\gamma\\) as a function of the collapse leverage \\(q_c\\) (assuming agents with identical parameters). The other two parameters are \\(\\Delta r = 0.5\\) and \\(p_r = 0.1\\). Briefly interpret your plot.\n\n# ...\n\n\n\n\n\nBarfuss, W., Donges, J., & Bethge, M. (2024). Ecologically-mediated collective action in commons with tipping elements. OSF. https://doi.org/10.31219/osf.io/7pcnm\n\n\nNowak, M. A. (2006). Five Rules for the Evolution of Cooperation. Science. https://doi.org/10.1126/science.1133755\n\n\nOstrom, E. (1990). Governing the commons: The evolution of institutions for collective action. Cambridge university press.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Dynamic Interactions"
    ]
  },
  {
    "objectID": "04.01ex-BehavioralAgency.html",
    "href": "04.01ex-BehavioralAgency.html",
    "title": "Ex | Behavioral Agency",
    "section": "",
    "text": "Task 1 | Implement Schelling’s model\nDefine a Python function run_model(agents) that simulates Schelling’s model for a given iterable of agents (agents), the number of agents regarded as neighbors num_neighbors and the number of neighbors required to be of the same type require_same_type such that an agent is happy. The function should return an iterable of agents after the model has converged. The model should run until no agent wants to move anymore. Print out the current cycle number while the model is running.\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Behavioral Agency"
    ]
  },
  {
    "objectID": "04.01ex-BehavioralAgency.html#task-2-run-the-model",
    "href": "04.01ex-BehavioralAgency.html#task-2-run-the-model",
    "title": "Ex | Behavioral Agency",
    "section": "Task 2 | Run the model",
    "text": "Task 2 | Run the model\nTest your implementation by running the model with 500 agents, 10 neighbors, and 5 neighbors of the same type required. Initialize the agents with 250 agents of type 0 and 250 agents of type 1. Plot the distribution of agents at the beginning, and the end of the model run in a matplotlib figure with two axes next to each other.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Behavioral Agency"
    ]
  },
  {
    "objectID": "04.01ex-BehavioralAgency.html#task-3-performance-metric",
    "href": "04.01ex-BehavioralAgency.html#task-3-performance-metric",
    "title": "Ex | Behavioral Agency",
    "section": "Task 3 | Performance metric",
    "text": "Task 3 | Performance metric\nHaving a visual understanding of the model’s behavior is essential. However, it is also useful to have a quantitative measure of the model’s performance. Define a Python function homogeneity(agents, num_neighbors) that returns the average homogeneity of the agents. The homogeneity of an agent is the fraction of its neighbors that are of the same type. The average homogeneity is the average of the homogeneity of all agents.\n\n# ...\n\nTest your implementation by calculating the homogeneity of the initial and final agent distributions from the simulation above.\n\n# ...\n\nConvince yourself that the homogeneity is a useful measure of the model’s performance by repeating the simulation with the same parameters as above, except the number of neighbors required to be happy is set to 9. Visualize and calculate the homogeneity of the final agent distribution.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Behavioral Agency"
    ]
  },
  {
    "objectID": "04.01ex-BehavioralAgency.html#task-4-sensitivity-analysis",
    "href": "04.01ex-BehavioralAgency.html#task-4-sensitivity-analysis",
    "title": "Ex | Behavioral Agency",
    "section": "Task 4 | Sensitivity analysis",
    "text": "Task 4 | Sensitivity analysis\nPerform a simulation to investigate how the final homogeneity depends on the number of neighbors considered, num_neighbors, and the number of neighbors of the same type required to be happy, require_same_type.\nFirst, we keep the number of neighbors required to be happy fixed to be exactly half the number of neighbors considered. How sensitive is the final homogeneity to the number of neighbors considered? Run the model for num_neighbors in the range from 2 to 20 in increments of two. As the simulation is stochastic, run each simulation precisely five times. Plot the final homogeneities (plus their averages) versus the number of neighbors considered. Briefly interpret your result.\n\n# ...\n\nFinally, run the model for num_neighbors in the range from 2 to 14, and for each num_enighbors, the require_same_type from 1 to num_neighbors-1. Plot the final homogeneity as a heatmap.\nTip: Store the final homogeneities in a two-dimensional NumPy array\nfinal_homogeneities=np.NaN * np.ones((15, 15)),\nwhere the first dimension corresponds to the number of neighbors considered and the second dimension to the number of neighbors of the same type required to be happy. The np.NaN values will be useful for plotting the heatmap as the plotting functions ignore the ‘NaN’ values. You can you use the imshow function from matplotlib to plot the heatmap,\nplt.imshow(final_homogeneities, cmap='RdBu', vmin=0, vmax=1.0, origin='lower').\nInclude a color bar and a line indicating where the number of neighbors considered is equal to half the number of neighbors of the same type required to be happy.\nInterpret your results regarding the maximum number of neighbors required to be happy to achieve a well-mixed society. What happens to the final homogeneity when the number of neighbors required to be happy exceeds half the number of neighbors considered?\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Behavioral Agency"
    ]
  },
  {
    "objectID": "04.02ex-IndividualLearning.html",
    "href": "04.02ex-IndividualLearning.html",
    "title": "Ex | Individual Learning",
    "section": "",
    "text": "Task 1 | Learning the risky policy\nIn the lecture, we explored how the agent learns a cautious policy within the risk-reward dilemma. Investigate the learning process for parameter combinations that make the risky policy optimal (DiscountFactor=0.6, CollapseProbability=0.1, RecoveryProbability=0.1, SafeReward=0.5, RiskyReward=1.0, DegradedReward=0.0). What parameters of the learning process, such as learning rate and choice intensity, allow the agent to consistently learn the risky policy?\n# ...\nHow does the learning process change if you change the transition probabilities to CollapseProbability=0.05, RecoveryProbability=0.005?\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Individual Learning"
    ]
  },
  {
    "objectID": "04.02ex-IndividualLearning.html#task-2-ecological-public-good",
    "href": "04.02ex-IndividualLearning.html#task-2-ecological-public-good",
    "title": "Ex | Individual Learning",
    "section": "Task 2 | Ecological public good",
    "text": "Task 2 | Ecological public good\nImplement the ecological public good from Lecture 03.03 as a reinforcement learning environment. Ensure your EcologicalPublicGood class inherits from the base Environment class.\n\n# ...\n\nLet two agents learn in it and visualize the learning process.\n\n# ...\n\nBriefly discuss your findings.",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Individual Learning"
    ]
  },
  {
    "objectID": "04.03ex-LearningDynamics.html",
    "href": "04.03ex-LearningDynamics.html",
    "title": "Ex | Learning Dynamics",
    "section": "",
    "text": "Task 1 | Social dilemma flows\nVisualize the flow plots for all four social dilemma environment we discussed in the course: Tragedy Prinsoner’s Dilemma, Divergence Chicken, Coordination Stag Hunt, and Comedy Harmony.\nYou can use the pyCRLD environment SocialDilemma by impporting\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Learning Dynamics"
    ]
  },
  {
    "objectID": "04.03ex-LearningDynamics.html#task-1-social-dilemma-flows",
    "href": "04.03ex-LearningDynamics.html#task-1-social-dilemma-flows",
    "title": "Ex | Learning Dynamics",
    "section": "",
    "text": "from pyCRLD.Environments.SocialDilemma import SocialDilemma",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Learning Dynamics"
    ]
  },
  {
    "objectID": "04.03ex-LearningDynamics.html#task-2-critical-transition",
    "href": "04.03ex-LearningDynamics.html#task-2-critical-transition",
    "title": "Ex | Learning Dynamics",
    "section": "Task 2 | Critical transition",
    "text": "Task 2 | Critical transition\nWe consider the following model: Two agents can either cooperate or defect. A cooperator contributes a benefit \\(b\\), which all agents receive. However, a cooperator must pay \\(c\\) for the contribution. A defector does not contribute and does not pay a cost. Thus, the payoff matrix is\n\n\n\n\nCooperate\nDefect\n\n\n\n\nCooperate\n\\(2b-c\\) , \\(2b-c\\)\n\\(b-c\\), \\(b\\)\n\n\nDefect\n\\(b\\), \\(b-c\\)\n\\(0, 0\\)\n\n\n\nLet us re-normalize the payoffs, devide all payoffs by \\(b\\) and express in the cost-to-benefit ratio \\(r = c/b\\).\n\n\n\n\nCooperate\nDefect\n\n\n\n\nCooperate\n\\(2-r\\) , \\(2-r\\)\n\\(1-r\\), \\(1\\)\n\n\nDefect\n\\(1\\), \\(1-r\\)\n\\(0, 0\\)\n\n\n\nSimulate the reinforcement learning dynamics in the game from 25 random initial joint policies for values of \\(r\\) in the range \\([0.5, 1.5]\\). Record the final joint policy for each initial policy and plot the critical transition from defection to cooperation as a function of \\(r\\). Also, visualize how long, on average, it takes for the agents to reach the final joint policy. Show a critical slowing down.\n\n# ...",
    "crumbs": [
      "Appendices",
      "Exercises",
      "Ex | Learning Dynamics"
    ]
  }
]