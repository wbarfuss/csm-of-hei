<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Individual learning – Complex Systems Modeling of Human-Environment Interactions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./References.html" rel="next">
<link href="./04.01-BehavioralAgency.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-dda8909163e0b4f3670ba323ebd66e56.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-TransformationAgency.html">Transformation Agency</a></li><li class="breadcrumb-item"><a href="./04.02-IndividualLearning.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/CSMofHEI_Logo.drawio.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="./images/CSMofHEI_Logo.drawio.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Complex Systems Modeling of Human-Environment Interactions</a> 
        <div class="sidebar-tools-main">
    <a href="./Complex-Systems-Modeling-of-Human-Environment-Interactions.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.01-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./02-DynamicSystems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dynamic Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.01-Nonlinearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Nonlinearity</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.02-TippingElements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tipping elements</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.03-Resilience.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Resilience</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.04-StateTransitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">State transitions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./03-TargetEquilibria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Target Equilibria</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.01-SequentialDecisions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Sequential Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.02-StrategicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Strategic Interactions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.03-DynamicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dynamic Interactions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./04-TransformationAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformation Agency</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.01-BehavioralAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Behavioral agency</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.02-IndividualLearning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Exercises</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.02ex-IntroToPython.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Introduction to Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.01ex-Nonlinearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Nonlinearity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.02ex-TippingElements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Tipping elements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.03ex-Resilience.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Resilience</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.04ex-StateTransitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | State transitions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.01ex-SequentialDecisions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Sequential Decisions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.02ex-StrategicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Strategic Interactions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.03ex-DynamicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Dynamic Interactions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.01ex-BehavioralAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Behavioral Agency</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.02ex-IndividualLearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Individual Learning</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation"><span class="header-section-number">10.1</span> Motivation</a>
  <ul class="collapse">
  <li><a href="#using-behavioral-theories-in-abms-is-challenging" id="toc-using-behavioral-theories-in-abms-is-challenging" class="nav-link" data-scroll-target="#using-behavioral-theories-in-abms-is-challenging">Using behavioral theories in ABMs is challenging</a></li>
  <li><a href="#reinforcement-learning-offers-a-principled-take" id="toc-reinforcement-learning-offers-a-principled-take" class="nav-link" data-scroll-target="#reinforcement-learning-offers-a-principled-take">Reinforcement learning offers a principled take</a></li>
  <li><a href="#an-integrating-platform-for-cognitive-mechanisms" id="toc-an-integrating-platform-for-cognitive-mechanisms" class="nav-link" data-scroll-target="#an-integrating-platform-for-cognitive-mechanisms">An integrating platform for cognitive mechanisms</a></li>
  <li><a href="#learning-goals" id="toc-learning-goals" class="nav-link" data-scroll-target="#learning-goals">Learning goals</a></li>
  </ul></li>
  <li><a href="#agent-environment-interface" id="toc-agent-environment-interface" class="nav-link" data-scroll-target="#agent-environment-interface"><span class="header-section-number">10.2</span> Agent-environment interface</a></li>
  <li><a href="#example-risk-reward-dilemma" id="toc-example-risk-reward-dilemma" class="nav-link" data-scroll-target="#example-risk-reward-dilemma"><span class="header-section-number">10.3</span> Example | Risk Reward Dilemma</a>
  <ul class="collapse">
  <li><a href="#transitions-environmental-dynamics" id="toc-transitions-environmental-dynamics" class="nav-link" data-scroll-target="#transitions-environmental-dynamics">Transitions | Environmental dynamics</a></li>
  <li><a href="#rewards-short-term-welfare" id="toc-rewards-short-term-welfare" class="nav-link" data-scroll-target="#rewards-short-term-welfare">Rewards | Short-term welfare</a></li>
  <li><a href="#init-method" id="toc-init-method" class="nav-link" data-scroll-target="#init-method">Init method</a></li>
  <li><a href="#basic-testing" id="toc-basic-testing" class="nav-link" data-scroll-target="#basic-testing">Basic testing</a></li>
  <li><a href="#tracking-the-learning-process" id="toc-tracking-the-learning-process" class="nav-link" data-scroll-target="#tracking-the-learning-process">Tracking the learning process</a></li>
  </ul></li>
  <li><a href="#reinforcement-learning-agent" id="toc-reinforcement-learning-agent" class="nav-link" data-scroll-target="#reinforcement-learning-agent"><span class="header-section-number">10.4</span> Reinforcement learning agent</a>
  <ul class="collapse">
  <li><a href="#goal" id="toc-goal" class="nav-link" data-scroll-target="#goal">Goal</a></li>
  <li><a href="#representation" id="toc-representation" class="nav-link" data-scroll-target="#representation">Representation</a></li>
  <li><a href="#value-beliefs" id="toc-value-beliefs" class="nav-link" data-scroll-target="#value-beliefs">Value beliefs</a></li>
  <li><a href="#behavioral-rule" id="toc-behavioral-rule" class="nav-link" data-scroll-target="#behavioral-rule">Behavioral rule</a></li>
  <li><a href="#learning-rule" id="toc-learning-rule" class="nav-link" data-scroll-target="#learning-rule">Learning rule</a></li>
  <li><a href="#testing-the-interface" id="toc-testing-the-interface" class="nav-link" data-scroll-target="#testing-the-interface">Testing the interface</a></li>
  </ul></li>
  <li><a href="#investigating-the-learning-process" id="toc-investigating-the-learning-process" class="nav-link" data-scroll-target="#investigating-the-learning-process"><span class="header-section-number">10.5</span> Investigating the learning process</a>
  <ul class="collapse">
  <li><a href="#too-much-exploration" id="toc-too-much-exploration" class="nav-link" data-scroll-target="#too-much-exploration">Too much exploration</a></li>
  <li><a href="#too-little-exploration" id="toc-too-little-exploration" class="nav-link" data-scroll-target="#too-little-exploration">Too little exploration</a></li>
  <li><a href="#decaying-exploration" id="toc-decaying-exploration" class="nav-link" data-scroll-target="#decaying-exploration">Decaying exploration</a></li>
  <li><a href="#initial-exploration-bonus" id="toc-initial-exploration-bonus" class="nav-link" data-scroll-target="#initial-exploration-bonus">Initial exploration bonus</a></li>
  </ul></li>
  <li><a href="#learning-goals-revisited" id="toc-learning-goals-revisited" class="nav-link" data-scroll-target="#learning-goals-revisited"><span class="header-section-number">10.6</span> Learning goals revisited</a>
  <ul class="collapse">
  <li><a href="#key-advantages-of-an-rl-framework" id="toc-key-advantages-of-an-rl-framework" class="nav-link" data-scroll-target="#key-advantages-of-an-rl-framework">Key advantages of an RL framework</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-TransformationAgency.html">Transformation Agency</a></li><li class="breadcrumb-item"><a href="./04.02-IndividualLearning.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>▶ <strong>Complex Systems Modeling of Human-Environment Interactions</strong> <br>&gt;&gt; Open the latest version on the <a href="https://wbarfuss.github.io/csm-of-hei/04.02-IndividualLearning.html"><em>web</em></a>, <a href="https://github.com/wbarfuss/csm-of-hei/blob/main/04.02-IndividualLearning.ipynb"><em>Github</em></a> or in <a href="https://colab.research.google.com/github/wbarfuss/csm-of-hei/blob/main/04.02-IndividualLearning.ipynb"><em>GoogleColab</em></a> &lt;&lt; <br> <a href="https://wbarfuss.github.io">Wolfram Barfuss</a> | <a href="https://www.uni-bonn.de">University of Bonn</a> | 2025/2026</p>
<div id="3b20cb0f-e4e1-41af-b0fd-44e514dcf80f" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np  </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact, interactive</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> deepcopy</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.style <span class="im">as</span> style<span class="op">;</span> style.use(<span class="st">'seaborn-v0_8'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">4</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>color <span class="op">=</span> plt.rcParams[<span class="st">'axes.prop_cycle'</span>].by_key()[<span class="st">'color'</span>][<span class="dv">0</span>] </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.facecolor'</span>] <span class="op">=</span> <span class="st">'white'</span><span class="op">;</span> plt.rcParams[<span class="st">'grid.color'</span>] <span class="op">=</span> <span class="st">'gray'</span><span class="op">;</span> </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'grid.linewidth'</span>] <span class="op">=</span> <span class="fl">0.25</span><span class="op">;</span> </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.dpi'</span>] <span class="op">=</span> <span class="dv">140</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(legacy<span class="op">=</span><span class="st">'1.25'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="motivation" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">10.1</span> Motivation</h2>
<blockquote class="blockquote">
<p>Give a man a fish, and he’ll eat for a day</p>
</blockquote>
<blockquote class="blockquote">
<p>Teach a man to fish, and he’ll eat for a lifetime</p>
</blockquote>
<blockquote class="blockquote">
<p>Give a man a taste for fish, and he’ll eat even if conditions change. [<a href="https://www.coursera.org/lecture/fundamentals-of-reinforcement-learning/michael-littman-the-reward-hypothesis-q6x0e">source</a>]</p>
</blockquote>
<p>In this chapter, we will introduce the basics of temporal-difference reward-prediction reinforcement learning.</p>
<section id="using-behavioral-theories-in-abms-is-challenging" class="level3">
<h3 class="anchored" data-anchor-id="using-behavioral-theories-in-abms-is-challenging">Using behavioral theories in ABMs is challenging</h3>
<p>General <strong>agent-based modeling</strong> is a flexible tool for studying different theories of human behavior. However, the social and behavioral sciences are not known for their tendency to integrate. <strong>Knowledge about human behavior is fragmented into many different, context-specific, and often not formalized theories</strong>. For example, in an attempt to order and use this knowledge for sustainability science, Constantino and colleagues presented a selection of 32 behavioral theories <span class="citation" data-cites="ConstantinoEtAl2021">(<a href="References.html#ref-ConstantinoEtAl2021" role="doc-biblioref">Constantino et al., 2021</a>)</span>.</p>
<p><img src="images/04.02-BehavioralTheories.dio.png" class="img-fluid"></p>
<p>The many behavioral theories pose a significant <strong>challenge for general agent-based modeling</strong> when it comes to incorporating human decision-making into models of Nature-society systems <span class="citation" data-cites="SchluterEtAl2017">(<a href="References.html#ref-SchluterEtAl2017" role="doc-biblioref">Schlüter et al., 2017</a>)</span>:</p>
<ol type="1">
<li><strong>Fragmentation of theories</strong>: A vast array of theories on human decision-making is scattered across different disciplines, making it difficult to navigate and select relevant theories. Each theory often focuses on specific aspects of decision-making, leading to <strong>fragmented knowledge</strong>.</li>
<li><strong>Incomplete theories</strong>: The degree of formalization varies across theories. Many decision-making theories are incomplete or not fully formalized, requiring modelers to fill logical gaps with assumptions to make simulations work. This step introduces more degrees of freedom and possibly arbitrariness into the modeling process.</li>
<li><strong>Correlation-based theories</strong>: Many theories focus on correlations rather than causal mechanisms, essential for dynamic modeling. This requires modelers to make explicit assumptions about causal relationships - introducing more degrees of freedom and possibly arbitrariness into the modeling process.</li>
<li><strong>Context-dependent theories</strong>: The applicability of theories can vary greatly depending on the context, which adds complexity to their integration into models.</li>
</ol>
</section>
<section id="reinforcement-learning-offers-a-principled-take" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-learning-offers-a-principled-take">Reinforcement learning offers a principled take</h3>
<p>Reinforcement learning (RL) offers a general prototype model for intelligent &amp; adaptive decision-making in agent-based models.</p>
<p><strong>Principle</strong></p>
<blockquote class="blockquote">
<p>“Do more of what makes you happy.”</p>
</blockquote>
<p><strong>We do not need to specify the behavior of an agent</strong> directly.</p>
<p><strong>Instead, we specify what an agent wants</strong> and how it learns. Crucially, how it learns does not depend on the details of the environment model. It is a more general process, applicable across different environments.</p>
<p><strong>Then, it learns for itself what to do</strong> and we study the learning process and the learned behaviors.</p>
<p>This approach is <strong>particularly valuable</strong> for studying human-environment systems <strong>when the decision environment changes</strong> through a policy intervention or a global change process, like climate change or biodiversity loss. If we had specified the agent’s behavior directly, the agent’s behavior could not change when the environment changes. In contrast, if we specify the underlying agent’s goal, we can study how the agent’s behavior changes when the environment changes. Reinforcement learning agents can <strong>learn while interacting</strong> with the environment.</p>
</section>
<section id="an-integrating-platform-for-cognitive-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="an-integrating-platform-for-cognitive-mechanisms">An integrating platform for cognitive mechanisms</h3>
<p>RL, broadly understood, offers an interdisciplinary platform for integrating <strong>cognitive mechanisms</strong> into ABMs. It offers a comprehensive framework for studying the interplay among <strong>learning</strong> (adaptive behavior), <strong>representation</strong> (beliefs), and <strong>decision-making</strong> (actions) <span class="citation" data-cites="BotvinickEtAl2020">(<a href="References.html#ref-BotvinickEtAl2020" role="doc-biblioref">Botvinick et al., 2020</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/04.02-CognitiveFrameworks.dio.png" class="img-fluid figure-img"></p>
<figcaption>RL-based frameworks with cognitive mechanisms</figcaption>
</figure>
</div>
<p>RL is also an <strong>interdisciplinary endeavor</strong>, studied in Psychology, Neuroscience, Behavioral economics, Complexity science, and Machine learning.</p>
<div id="fig-RLinBrain" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-RLinBrain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/04.02-DopamineRewardPredictionError.dio.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-RLinBrain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Reinforcement learning in the brain
</figcaption>
</figure>
</div>
<p><a href="#fig-RLinBrain" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> shows the <strong>remarkable analogy between the firing patterns of dopamine neurons in the brain and the prediction errors in a reinforcement learning simulation</strong>.</p>
<p><a href="#fig-RLinBrain" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> (a-c) shows prediction errors in a Pavlovian RL conditioning task simulation. A conditional stimulus (CS) is presented randomly, followed 2 seconds later by a reward (Unconditional Stimulus - US). (a) In the early training phase, the reward is not anticipated, leading to prediction errors when the reward is presented. As learning occurs, these prediction errors begin to affect prior events in the trial (examples from trials 5 and 10) because predictive values are learned. (b) After learning, the previously unexpected reward no longer creates a prediction error. Instead, the conditional stimulus now causes a prediction error when it occurs unexpectedly. (c) When the reward is omitted when expected, it results in a negative prediction error, signaling that what happened was worse than anticipated.</p>
<p><a href="#fig-RLinBrain" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> (d–f) Firing patterns of dopamine neurons in monkeys engaged in a similar instrumental conditioning task <span class="citation" data-cites="SchultzEtAl1997">(<a href="References.html#ref-SchultzEtAl1997" role="doc-biblioref"><strong>SchultzEtAl1997?</strong></a>)</span>. Each raster plot shows action potentials (dots) with different rows for different trials aligned with the cue (or reward) timing. Histograms show combined activity across the trials below. (d) When a reward is unexpectedly received, dopamine neurons fire rapidly. (e) After conditioning with a visual cue (which predicted a food reward if the animal performed correctly), the reward no longer triggers a burst of activity; now, the burst happens at the cue’s presentation. (f) If the food reward is omitted unexpectedly, dopamine neurons exhibit a distinct pause in firing, falling below their typical rate.</p>
<p><strong>Source of confusion.</strong> Because of its broad scope and interdisciplinary nature, simply the phrase “reinforcement learning” can mean different things to different people. To mitigate this possible source of confusion, it is good to acknowledge that RL can refer to a <strong>model of human learning</strong>, an <strong>optimization method</strong>, a <strong>problem description</strong>, and a <strong>field of research</strong>.</p>
</section>
<section id="learning-goals" class="level3">
<h3 class="anchored" data-anchor-id="learning-goals">Learning goals</h3>
<p>After this lecture, students will be able to:</p>
<ul>
<li>Explain why reinforcement learning is valuable in models of human-environment interactions</li>
<li>Implement and apply the different elements of the agent-environment framework, including a temporal-difference learning agent.</li>
<li>Explain and manage the trade-off between exploration and exploitation.</li>
<li>Visualize the learning process</li>
<li>Use the Python library <code>pandas</code> to manage data</li>
<li>Refine their skills in object-oriented programming</li>
</ul>
</section>
</section>
<section id="agent-environment-interface" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="agent-environment-interface"><span class="header-section-number">10.2</span> Agent-environment interface</h2>
<p>Generally, making sense of an agent without its environment is difficult, and vice versa.</p>
<div id="fig-RLinAEi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-RLinAEi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/04.02-RLinAEi.dio.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-RLinAEi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2: Reinforcement learning in the agent-environment interface
</figcaption>
</figure>
</div>
<section id="interface" class="level4">
<h4 class="anchored" data-anchor-id="interface">Interface</h4>
<p>At the interface between the agent and the environment are</p>
<ul>
<li>the agent’s <strong>set of</strong> (conceivable) <strong>actions</strong> - from agent to environment,</li>
<li>extrinsic <strong>reward signals</strong> - a single number from environment to agent,</li>
<li>possibly <strong>observation signals</strong> - from environment to agents.</li>
</ul>
<p>Note: In general, the environment is composed of the <em>natural</em> and the <em>social</em> environment.</p>
<div id="93e7193c-ab16-43c8-9830-e2fb42cd153a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interface_run(agent, env, NrOfTimesteps):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run the multi-agent environment for several time steps."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    observation <span class="op">=</span> env.observe()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(NrOfTimesteps):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> agent.act(observation)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        next_observation, reward <span class="op">=</span> env.step(action)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        agent.update(observation, action, reward, next_observation)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        observations <span class="op">=</span> next_observation</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="environment" class="level4">
<h4 class="anchored" data-anchor-id="environment">Environment</h4>
<p>The environment delivers <strong>extrinsic rewards</strong> (motivations) to the agents based on the <strong>agents’ chosen actions</strong> (choices) and the environment’s current and future state (see Lecture <a href="https://wbarfuss.github.io/csm-of-hei/03.01-SequentialDecisions.html">03.01-SequentialDecisions</a>).</p>
<div id="9e5ac398-939a-4169-9a6f-e0a47151e8a6" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Environment:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Abstract environment class."""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_StateSet(<span class="va">self</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Default state set representation `state_s`."""</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="bu">str</span>(s) <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.Z)]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_ActionSet(<span class="va">self</span>):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Default action set representation `action_a`."""</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="bu">str</span>(a) <span class="cf">for</span> a <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.M)]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>             action: <span class="bu">int</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            ) <span class="op">-&gt;</span> <span class="bu">tuple</span>:  <span class="co"># (observations_Oi, rewards_Ri, info)</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Iterate the environment one step forward.</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># choose a next state according to transition tensor T</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        tps <span class="op">=</span> <span class="va">self</span>.TransitionTensor[<span class="va">self</span>.state, action]</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(tps)), p<span class="op">=</span>tps)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># obtain the current reward</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="va">self</span>.RewardTensor[<span class="va">self</span>.state, action, next_state]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>                                    </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># advance the state and collect info</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state <span class="op">=</span> next_state</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># agent observes the environmental state</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        observation <span class="op">=</span> <span class="va">self</span>.observe()</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> observation, reward </span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> observe(<span class="va">self</span>):</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Observe the environment."""</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.state</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="agent" class="level4">
<h4 class="anchored" data-anchor-id="agent">Agent</h4>
<p>Agents choose <strong>actions</strong> based on their <strong>observations</strong> to achieve a goal (e.g., <strong>maximize reward</strong>). We need to specify their</p>
<ul>
<li><strong>Goal</strong>, describing what an agent wants (in the long run). They may contain <em>intrinsic motivations</em>.</li>
<li><strong>Representation</strong>, e.g., defining upon which conditions agents select actions (e.g., <em>history</em> of past states and actions).</li>
<li><strong>Value beliefs</strong> (value functions), capturing what is <em>good</em> for the agent regarding its <em>goal</em> in the long run.</li>
<li><strong>Behavioral rule</strong> (policy, strategy), defining how to select actions.</li>
<li><strong>Learning rule</strong>, describing how value beliefs are updated in light of new information.</li>
<li>(optionally), a <strong>model</strong> of the environment and rewards. Models are used for <em>planning</em>, i.e., deciding on a <em>behavioral rule</em> by considering possible future situations before they are actually experienced.</li>
</ul>
<div id="7200a2b5-869f-4150-b3ce-09fc98a967be" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomAgent:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, policy):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.policy_Xoa <span class="op">=</span> policy<span class="op">/</span>policy.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ActionIxs <span class="op">=</span> <span class="bu">range</span>(<span class="va">self</span>.policy_Xoa.shape[<span class="dv">1</span>])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, obs):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.choice(<span class="va">self</span>.ActionIxs, p<span class="op">=</span><span class="va">self</span>.policy_Xoa[obs])</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="example-risk-reward-dilemma" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="example-risk-reward-dilemma"><span class="header-section-number">10.3</span> Example | Risk Reward Dilemma</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/03.01-RiskRewardDilemma.dio.png" class="img-fluid figure-img"></p>
<figcaption>Risk Reward Dilemma</figcaption>
</figure>
</div>
<div id="adf4cf46-15cf-46db-9914-6296a657ce9f" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RiskRewardDilemma(Environment):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A simple risk-reward dilemma environment."""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_StateSet(<span class="va">self</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="st">'p'</span>, <span class="st">'d'</span>]  <span class="co"># prosperous, degraded</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_ActionSet(<span class="va">self</span>):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="st">'c'</span>, <span class="st">'r'</span>]  <span class="co"># cautious, risky</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="transitions-environmental-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="transitions-environmental-dynamics">Transitions | Environmental dynamics</h3>
<p>The <strong>environmental dynamics</strong>, i.e., the transitions between environmental state contexts are modeled by two parameters: a collapse probability, <span class="math inline">\(p_c\)</span>, and a recovery probability, <span class="math inline">\(p_r\)</span>.</p>
<div id="5e727975-b2c4-4c16-9f67-64ee4f2e664c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>pc, pr <span class="op">=</span> sp.symbols(<span class="st">'p_c p_r'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f30d3e1a-7220-4bcd-800b-cf58062564d2" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> RiskRewardDilemma().obtain_StateSet().index(<span class="st">'p'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> RiskRewardDilemma().obtain_StateSet().index(<span class="st">'d'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> RiskRewardDilemma().obtain_ActionSet().index(<span class="st">'c'</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> RiskRewardDilemma().obtain_ActionSet().index(<span class="st">'r'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>p,d,c,r    </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>(0, 1, 0, 1)</code></pre>
</div>
</div>
<p>We implement the transitions as a three-dimensional array or <strong>tensors</strong>, with dimensions <span class="math inline">\(Z \times M \times Z\)</span>, where <span class="math inline">\(Z\)</span> is the number of states and <span class="math inline">\(M\)</span> is the number of actions.</p>
<div id="e1c3ffca-b60d-40ab-85d7-fa8eeea791d0" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.zeros((<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">object</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The cautious action guarantees to remain in the prosperous state, <span class="math inline">\(T(\mathsf{p,c,p})=1\)</span>. Thus, the agent can avoid the risk of environmental collapse by choosing the cautious action, <span class="math inline">\(T(\mathsf{p,c,d})=0\)</span>.</p>
<div id="4407af08-fa26-411d-9e27-a0afe09172cd" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>T[p,c,d] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>T[p,c,p] <span class="op">=</span> <span class="dv">1</span>   </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The risky action risks the collapse to the degraded state, <span class="math inline">\(T(\mathsf{p,r,d}) = p_c\)</span>, with a collapse probability <span class="math inline">\(p_c\)</span>. Thus, with probability <span class="math inline">\(1-p_c\)</span>, the environment remains prosperous under the risky action, <span class="math inline">\(T(\mathsf{p,r,p}) = 1-p_c\)</span>.</p>
<div id="426b6cd2-e046-46e2-9aea-92ffee8cae5b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>T[p,r,d] <span class="op">=</span> pc</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>T[p,r,p] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>pc</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>At the degraded state, recovery is only possible through the cautious action, <span class="math inline">\(T(\mathsf{d,c,p})=p_r\)</span>, with recovery probability <span class="math inline">\(p_r\)</span>. Thus, with probability <span class="math inline">\(1-p_r\)</span>, the environment remains degraded under the cautious action, <span class="math inline">\(T(\mathsf{d,c,d})=1-p_r\)</span>.</p>
<div id="4ba77d28-eddb-462a-97e6-848b755a236f" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>T[d,c,p] <span class="op">=</span> pr</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>T[d,c,d] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>pr</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Finally, the risky action at the degraded state guarantees a lock-in in the degraded state, <span class="math inline">\(T(\mathsf{d,r,d})=1\)</span>. Thus, the environment cannot recover from the degraded state under the risky action, <span class="math inline">\(T(\mathsf{d,r,p})=0\)</span>.</p>
<div id="ad9fc88d-e947-4e98-b193-d9bdc54cea27" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>T[d,r,p] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>T[d,r,d] <span class="op">=</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Last, we make sure that our transition tensor is normalized, i.e., the sum of all transition probabilities from a state-action pair to all possible next states equals one, <span class="math inline">\(\sum_{s'} T(s, a, s') = 1\)</span>.</p>
<div id="b6e9c9bc-949c-4e8f-894f-d0260b2c027a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>T.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[1, 1],
       [1, 1]], dtype=object)</code></pre>
</div>
</div>
<p>All together, the transition tensor looks as follows,</p>
<div id="b9c0ea9f-cbb5-4fb0-b711-8080f27c5627" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sp.Array(T)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}\left[\begin{matrix}1 &amp; 0\\1 - p_{c} &amp; p_{c}\end{matrix}\right] &amp; \left[\begin{matrix}p_{r} &amp; 1 - p_{r}\\0 &amp; 1\end{matrix}\right]\end{matrix}\right]\)</span></p>
</div>
</div>
<p>We give the risk-reward dilemma class its environmental dynamics by substituting the numeric parameter values in the <code>sympy</code> array and converting it to a numeric <code>numpy</code> array, à la</p>
<div id="6f909083-5dbe-405b-887f-619a360d34d1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>(np.array(sp.Array(T)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    .subs({pc: <span class="fl">0.1</span>, pr: <span class="fl">0.05</span>}))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    .astype(<span class="bu">float</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[[1.  , 0.  ],
        [0.9 , 0.1 ]],

       [[0.05, 0.95],
        [0.  , 1.  ]]])</code></pre>
</div>
</div>
<div id="cfee6178-7dd8-4fc5-af23-7e0d33505f97" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_TransitionTensor(<span class="va">self</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create the transition tensor."""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (np.array(sp.Array(T)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>            .subs({pc: <span class="va">self</span>.pc, pr: <span class="va">self</span>.pr}))</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>            .astype(<span class="bu">float</span>))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>RiskRewardDilemma.create_TransitionTensor <span class="op">=</span> create_TransitionTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="rewards-short-term-welfare" class="level3">
<h3 class="anchored" data-anchor-id="rewards-short-term-welfare">Rewards | Short-term welfare</h3>
<p>The rewards or welfare the agent receives represent the ecosystem services the environment provides. It is modeled by three parameters: a safe reward <span class="math inline">\(r_s\)</span>, a risky reward <span class="math inline">\(r_r&gt;r_s\)</span>, and a degraded reward <span class="math inline">\(r_d&lt;r_s\)</span>. We assume the following default values,</p>
<div id="e139d38a-a741-4a41-8877-c89a25d8791d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>rs, rr, rd <span class="op">=</span> sp.symbols(<span class="st">'r_s r_r r_d'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We implement the rewards as a four-dimensional array or <strong>tensor</strong>, with dimensions <span class="math inline">\(N \times Z \times M \times Z\)</span>, where <span class="math inline">\(N=1\)</span> is the number of agents, <span class="math inline">\(Z\)</span> is the number of states and <span class="math inline">\(M\)</span> is the number of actions. The additional agent dimension is necessary to accommodate multi-agent environments.</p>
<div id="96517c82-6fe9-4909-9ddc-96f0857e11e3" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.zeros((<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">object</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The cautious action at the prosperous state guarantees the safe reward, <span class="math inline">\(R(\mathsf{p,c,p}) = r_s\)</span>,</p>
<div id="6cd4e91d-0d67-4888-832f-82a5912fea12" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>R[p,c,p] <span class="op">=</span> rs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The risky action at the prosperous leads to the risky reward if the environment does not collapse, <span class="math inline">\(R(\mathsf{p,r,p}) = r_r\)</span>,</p>
<div id="0c3102cb-7f77-4342-9ea2-cb039afa28aa" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>R[p,r,p] <span class="op">=</span> rr</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Yet, whenever the environment enters, remains, or leaves the degraded state, it provides only the degraded reward <span class="math inline">\(R(\mathsf{d,:,:}) = R(\mathsf{:,:,d}) = r_d\)</span>, where <span class="math inline">\(:\)</span> denotes all possible states and actions.</p>
<div id="36b85c98-1b56-4567-8277-b7809a828685" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>R[d,:,:] <span class="op">=</span> R[:,:,d] <span class="op">=</span> rd</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Together, the reward tensor looks as follows:</p>
<div id="9b87915a-3c24-48f8-9598-c02dcaa86af0" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>sp.Array(R)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}\left[\begin{matrix}r_{s} &amp; r_{d}\\r_{r} &amp; r_{d}\end{matrix}\right] &amp; \left[\begin{matrix}r_{d} &amp; r_{d}\\r_{d} &amp; r_{d}\end{matrix}\right]\end{matrix}\right]\)</span></p>
</div>
</div>
<p>Again, we give the risk-reward dilemma class its reward function by substituting the numeric parameter values in the <code>sympy</code> array and converting it to a numeric <code>numpy</code> array, à la</p>
<div id="65accdde-1bd7-4167-bda7-69521d17afaf" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>(np.array(sp.Array(R)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    .subs({rr: <span class="fl">1.0</span>, rs: <span class="fl">0.8</span>, rd: <span class="fl">0.0</span>}))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    .astype(<span class="bu">float</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[[0.8, 0. ],
        [1. , 0. ]],

       [[0. , 0. ],
        [0. , 0. ]]])</code></pre>
</div>
</div>
<div id="dedc243f-976f-4f16-8948-fbcbcd703ea3" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_RewardTensor(<span class="va">self</span>):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create the reward tensor."""</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (np.array(sp.Array(R)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>            .subs({rr: <span class="va">self</span>.rr, rs: <span class="va">self</span>.rs, rd: <span class="va">self</span>.rd}))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>            .astype(<span class="bu">float</span>))</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>RiskRewardDilemma.create_RewardTensor <span class="op">=</span> create_RewardTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="init-method" class="level3">
<h3 class="anchored" data-anchor-id="init-method">Init method</h3>
<p>When initializing an agent, the following method is executed.</p>
<div id="c9c60ae0-b81d-41f9-b08e-f1992cd1478d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, CollapseProbability, RecoveryProbability, </span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>             RiskyReward, SafeReward, DegradedReward, state<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.N <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> <span class="va">self</span>.M <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> <span class="va">self</span>.Z <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.pc <span class="op">=</span> CollapseProbability</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.pr <span class="op">=</span> RecoveryProbability</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.rr <span class="op">=</span> RiskyReward</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.rs <span class="op">=</span> SafeReward</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.rd <span class="op">=</span> DegradedReward</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.StateSet <span class="op">=</span> <span class="va">self</span>.obtain_StateSet()</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ActionSet <span class="op">=</span> <span class="va">self</span>.obtain_ActionSet()</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.TransitionTensor <span class="op">=</span> <span class="va">self</span>.create_TransitionTensor()</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.RewardTensor <span class="op">=</span> <span class="va">self</span>.create_RewardTensor()</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.state <span class="op">=</span> state</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>RiskRewardDilemma.<span class="fu">__init__</span> <span class="op">=</span> <span class="fu">__init__</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="basic-testing" class="level3">
<h3 class="anchored" data-anchor-id="basic-testing">Basic testing</h3>
<p>Performing some basic tests,</p>
<div id="7422f8f1-e177-4c86-8a86-e86edac94206" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>                        RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>                        RiskyReward<span class="op">=</span><span class="fl">1.0</span>, </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>                        DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>env.TransitionTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[[1. , 0. ],
        [0.8, 0.2]],

       [[0.1, 0.9],
        [0. , 1. ]]])</code></pre>
</div>
</div>
<div id="45b893ef-ba65-439d-b086-60df80a99587" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>env.RewardTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[[0.8, 0. ],
        [1. , 0. ]],

       [[0. , 0. ],
        [0. , 0. ]]])</code></pre>
</div>
</div>
<div id="26006326-4033-4b4b-889e-0a8b164c9f3d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>🎬 The environment is in state,</p>
<div id="2deaaa55-a313-4751-a0b1-8bf161983821" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>env.StateSet[env.observe()]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>'p'</code></pre>
</div>
</div>
<p>Using the risky action, the agent transitions to state</p>
<div id="e24ddde0-2a2d-4251-8e20-828e8b4977b8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>next_state, reward <span class="op">=</span> env.step(env.ActionSet.index(<span class="st">'r'</span>))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>env.StateSet[next_state]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>'p'</code></pre>
</div>
</div>
<p>and receives are reward of</p>
<div id="502941ee-57e4-47c3-b551-f343a9331fdc" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>reward</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>1.0</code></pre>
</div>
</div>
<p><strong>🫳 Hands-on</strong>: Reexecute the cells from 🎬 a few times to see when the environment collapses.</p>
<p>Testing the interface,</p>
<div id="e5a55bcc-1831-4b33-aebd-b3d92e81f928" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> RandomAgent(policy<span class="op">=</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)))</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(<span class="fl">0.2</span>, <span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>interface_run(agent, env, <span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Obviously, this is not very insightful. We need to track the learning process.</p>
</section>
<section id="tracking-the-learning-process" class="level3">
<h3 class="anchored" data-anchor-id="tracking-the-learning-process">Tracking the learning process</h3>
<p>We need to track the learning process. We can do this by storing the actions, observations, and rewards in a <code>pandas</code> DataFrame. Pandas is a powerful data manipulation library in Python that provides data structures and functions to work with structured data. We will store the data of each time step into a row and its attributes into a set of respective columns of the DataFrame.</p>
<div id="77e375fb-aa13-4f1b-b809-9d9167fcac2c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interface_run(agent, env, NrOfTimesteps):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run the multi-agent environment for several time steps."""</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> [<span class="st">"observation"</span>, <span class="st">"action"</span>, <span class="st">"reward"</span>]</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(index<span class="op">=</span><span class="bu">range</span>(NrOfTimesteps), columns<span class="op">=</span>columns)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    observation <span class="op">=</span> env.observe()</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(NrOfTimesteps):</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> agent.act(observation)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>        next_observation, reward <span class="op">=</span> env.step(action)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        agent.update(observation, action, reward, next_observation)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        df.loc[t] <span class="op">=</span> (observation, action, reward)</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>        observation <span class="op">=</span> next_observation</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Testing the interface,</p>
<div id="72849efd-5d8d-4cc2-ba4b-b30ad1cdf43e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(agent, env, <span class="dv">25</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>df.tail()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">observation</th>
<th data-quarto-table-cell-role="th">action</th>
<th data-quarto-table-cell-role="th">reward</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">20</th>
<td>0</td>
<td>0</td>
<td>0.8</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">21</th>
<td>0</td>
<td>0</td>
<td>0.8</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">22</th>
<td>0</td>
<td>1</td>
<td>1.0</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">23</th>
<td>0</td>
<td>1</td>
<td>1.0</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">24</th>
<td>0</td>
<td>1</td>
<td>1.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="104ced3d-4079-4cd8-b2f8-e15ba81dc619" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_ActionsRewardsObservations(df):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].plot(df.observation, <span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Agent 0'</span>)<span class="op">;</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Observation'</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_yticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_yticklabels([env.StateSet[<span class="dv">0</span>], env.StateSet[<span class="dv">1</span>]])<span class="op">;</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].plot(df.action, <span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Agent 0'</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Action'</span>)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_yticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_yticklabels([env.ActionSet[<span class="dv">0</span>], env.ActionSet[<span class="dv">1</span>]])</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].plot(df.reward, <span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Agent 0'</span>)<span class="op">;</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_ylabel(<span class="st">'Reward'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="9528af0f-2d04-40d1-b7e6-f3702b5e4d9d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="8768d2b5-0717-4f3c-b3be-d9176279ce43" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>env.state<span class="op">=</span><span class="dv">0</span><span class="op">;</span> df <span class="op">=</span> interface_run(agent, env, <span class="dv">25</span>)<span class="op">;</span> plot_ActionsRewardsObservations(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-38-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Action-Reward-Observation Dynamics</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="reinforcement-learning-agent" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="reinforcement-learning-agent"><span class="header-section-number">10.4</span> Reinforcement learning agent</h2>
<p>Agents choose <strong>actions</strong> based on their <strong>observations</strong> to achieve a goal (e.g., <strong>maximize reward</strong>). We need to specify their</p>
<ul>
<li><strong>Goal</strong>, describing what an agent wants (in the long run). They may contain <em>intrinsic motivations</em>.</li>
<li><strong>Representation</strong>, e.g., defining upon which conditions agents select actions (e.g., <em>history</em> of past states and actions).</li>
<li><strong>Value beliefs</strong> (value functions), capturing what is <em>good</em> for the agent regarding its <em>goal</em> in the long run.</li>
<li><strong>Behavioral rule</strong> (policy, strategy), defining how to select actions.</li>
<li><strong>Learning rule</strong>, describing how value beliefs are updated in light of new information.</li>
<li>(optionally), a <strong>model</strong> of the environment and rewards. Models are used for <em>planning</em>, i.e., deciding on a <em>behavioral rule</em> by considering possible future situations before they are actually experienced.</li>
</ul>
<section id="goal" class="level3">
<h3 class="anchored" data-anchor-id="goal">Goal</h3>
<p>As in Lecture <a href="https://wbarfuss.github.io/csm-of-hei/03.01-SequentialDecisions.html">03.01-SequentialDecision</a>, the agent aims to maximize the discounted sum of future rewards,</p>
<p><span class="math display">\[ G_t = (1-\gamma) \sum_{\tau=0}^\infty \gamma^\tau R_{t+\tau},\]</span></p>
<p>where <span class="math inline">\(1-\gamma\)</span> is a normalizing factor and <span class="math inline">\(R_{t+\tau}\)</span> is the reward received at time step <span class="math inline">\(t+\tau\)</span>.</p>
<p>However, in contrast to Lecture <a href="./03.01-SequentialDecisions.html">03.01-SequentialDecision</a>, we assume that the agent does <strong>not know the environment’s dynamics and rewards</strong>. Instead, the agent <strong>learns</strong> about the environment <strong>while interacting</strong> with it.</p>
<p>The challenge is that actions may have <strong>delayed</strong> and <strong>uncertain</strong> consequences.</p>
<p><strong>Delayed</strong> consequences mean that an action may influence the environmental state, which, in turn, influences the reward the agent receives at a later time step. For example, in our risk-reward dilemma, opting for a sustainable policy may initially reduce the agent’s immediate reward but ensures a comparably higher long-term welfare. <strong>Uncertain</strong> consequences refer to the <strong>stochasticity</strong> in the environmental transitions (and possibly the reward signals themselves). For example, in our risk-reward dilemma, the risky action in the prosperous state may lead to a high reward but may also cause a transition to the degraded state. Moreover, uncertainty may also refer to the fact that the environmental transition dynamics may change over time.</p>
<p>Thus, the agent can’t just try each action in each state once and then immediately know which course of action is best. It <strong>must learn</strong> the best course of action <strong>over successive trials</strong>, each of which gives possible noisy data.</p>
</section>
<section id="representation" class="level3">
<h3 class="anchored" data-anchor-id="representation">Representation</h3>
<p>As the <code>RandomAgent</code> above, we here consider agents that learn to associate an action with each observation. To do so, the agent learns how valuable each action <span class="math inline">\(a\)</span> is to play when observing each observation <span class="math inline">\(o\)</span>. It represents its current beliefs of these values in a <code>ValueBeliefs_Qoa</code> array.</p>
<p>We start implementing the learning class by defining <code>__init__</code> method.</p>
<div id="7646eacf-82c3-4d61-8acf-e832463b1638" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Learner():</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A simple reinforcement learning agent."""</span>   </span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ValueBeliefs_Qoa,</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>                 DiscountFactor, LearningRate, ChoiceIntensity):</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.DiscountFactor <span class="op">=</span> <span class="va">self</span>.df <span class="op">=</span> DiscountFactor</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LearningRate <span class="op">=</span> <span class="va">self</span>.lr <span class="op">=</span> LearningRate</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ChoiceIntensity <span class="op">=</span> <span class="va">self</span>.ci <span class="op">=</span> ChoiceIntensity</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ValueBeliefs_Qoa <span class="op">=</span> ValueBeliefs_Qoa</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ActionIxs <span class="op">=</span> <span class="bu">range</span>(ValueBeliefs_Qoa.shape[<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The agent receives the following parameters: the initial value beliefs <code>ValueBeliefs_Qoa</code>, the discount factor <code>DiscountFactor</code>, the learning rate <code>LearningRate</code>, and the choice intensity <code>ChoiceIntensity</code>. Furthermore, we give the agent an attribute <code>ActionIxs</code> that stores the indices of the possible actions. This will be helpful when selecting actions.</p>
</section>
<section id="value-beliefs" class="level3">
<h3 class="anchored" data-anchor-id="value-beliefs">Value beliefs</h3>
<p>The general strategy we use to address the challenges of delayed and uncertain consequences is to let the agent <strong>learn value beliefs</strong>. Value beliefs are the agent’s estimates of the long-term value of each action <span class="math inline">\(a\)</span> under each observation <span class="math inline">\(o\)</span>. The agent then uses these value beliefs to select actions. These estimates are also often referred to as Q values. You may think of the <em>quality</em> of an action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, which tells the agents which action to select in which state.</p>
<p>For example, in the risk-reward dilemma, we can represent the agent’s value beliefs by</p>
<div id="2a96ac39-4927-422f-8412-5be4bcd45faa" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">10</span> <span class="op">*</span> np.random.rand(<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>ValueBeliefs_Qoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[3.74540119, 9.50714306],
       [7.31993942, 5.98658484]])</code></pre>
</div>
</div>
<p>The challenge of uncertain consequences in then solved by an appropriate <strong>behavioral rule</strong> which handles the so-called <strong>exploration-exploitation trade-off</strong></p>
<p>The challenge of delayed consequences is solved by the <strong>learning rule</strong>, which updates the value beliefs in light of new information using the Bellman equation, as in Lecture <a href="https://wbarfuss.github.io/csm-of-hei/03.01-SequentialDecisions.html">03.01-SequentialDecisions</a>.</p>
</section>
<section id="behavioral-rule" class="level3">
<h3 class="anchored" data-anchor-id="behavioral-rule">Behavioral rule</h3>
<p>A behavioral rule must deal with the <strong>exploration-exploitation trade-off</strong>, which poses <strong>a fundamental problem for decision-making under uncertainty</strong>.</p>
<p><strong>Under too much exploitation</strong>, the agent may pick an action that is not optimal, as it has not yet sufficiently explored all possible actions. It acts under the false belief that its current value beliefs are already correct or optimal. Thus, it <em>loses out</em> on possible rewards it would have gotten if it had explored more and discovered that a different course of action is better.</p>
<p><strong>Under too much exploration</strong>, the agent may continue to try all actions to gain as much information about the transitions and reward distributions as possible. It is <em>losing out</em> because it never settles on the best course of action, continuing to pick all actions until the end.</p>
<p>What is needed is a <strong>behavioral rule</strong> that <strong>balances exploitation and exploration</strong> to explore enough to find the best option but not too much so that the best option is exploited as much as possible.</p>
<p>We use the so-called <em>softmax</em> function,</p>
<p><span class="math display">\[x(o, a) = \frac{\exp \beta Q(o, a)}{\sum_{b \in \mathcal A}\exp \beta Q(o, b)},\]</span></p>
<p>which converts any set of value beliefs into probabilities that sum to one.</p>
<p>The higher the relative value belief, the higher the relative probability.</p>
<div id="6d1f4c61-8c1b-4699-9d1b-ab57adc39662" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obtain_softmax_probabilities(ChoiceIntensity, ValueBeliefs):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    expValueBeliefs <span class="op">=</span> np.exp(ChoiceIntensity<span class="op">*</span>np.array(ValueBeliefs))</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> expValueBeliefs <span class="op">/</span> expValueBeliefs.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The softmax function contains a parameter <span class="math inline">\(\beta\)</span>, denoting the <strong>choice intensity</strong> (sometimes called <em>inverse temperature</em>, that determines how <strong>exploitative</strong> (or <em>greedy</em>) the agent is.</p>
<p>When <span class="math inline">\(\beta = 0\)</span>, arms are chosen entirely at random with no influence of the Q values. This is super exploratory, as the agent continues to choose all arms irrespective of observed rewards.</p>
<div id="eeb255b6-2d92-4fb0-9c37-a40604e5890b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>obtain_softmax_probabilities(<span class="dv">0</span>, ValueBeliefs_Qoa)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[0.5, 0.5],
       [0.5, 0.5]])</code></pre>
</div>
</div>
<p>As <span class="math inline">\(\beta\)</span> increases, there is a higher probability of picking the arm with the highest Q value. This is increasingly exploitative (or ‘greedy’).</p>
<div id="93b31875-4789-41fe-9739-31f5e0bae4e1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>obtain_softmax_probabilities(<span class="dv">1</span>, ValueBeliefs_Qoa)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[0.00313576, 0.99686424],
       [0.79139498, 0.20860502]])</code></pre>
</div>
</div>
<p>When <span class="math inline">\(\beta\)</span> is very large, then only the arm that currently has the highest Q value will be chosen, even if other arms might actually be better.</p>
<div id="1789e7d3-c6b9-4622-ab58-c8df207c1725" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>obtain_softmax_probabilities(<span class="dv">50</span>, ValueBeliefs_Qoa).<span class="bu">round</span>(<span class="dv">9</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[0., 1.],
       [1., 0.]])</code></pre>
</div>
</div>
<p>For example, assuming we are in state zero and using <span class="math inline">\(\beta=1\)</span>, we can select an action by</p>
<div id="91a14375-db5a-4668-b6d6-2787614d777f" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>obs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>Xoa <span class="op">=</span> obtain_softmax_probabilities(<span class="dv">1</span>, ValueBeliefs_Qoa)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>np.random.choice([<span class="dv">0</span>,<span class="dv">1</span>], p<span class="op">=</span>Xoa[obs])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>1</code></pre>
</div>
</div>
<p>We summarize this logic in the agent’s <code>act</code> method:</p>
<div id="3e06bb11-b30c-4a5e-994a-75b557e14d26" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> act(<span class="va">self</span>, obs):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    Xoa <span class="op">=</span> <span class="va">self</span>.obtain_policy_Xoa()</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.choice(<span class="va">self</span>.ActionIxs, p<span class="op">=</span>Xoa[obs])</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>Learner.act <span class="op">=</span> act</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>where we define the <code>ActionIxs</code> as <code>range(self.NrActions)</code> in the <code>__init__</code> method of the agent. We also define the <code>obtain_policy_Xoa</code> method in the agent class:</p>
<div id="dc77c6cb-624d-4dc4-9431-72c429b8f5a9" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obtain_policy_Xoa(<span class="va">self</span>):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> obtain_softmax_probabilities(<span class="va">self</span>.ChoiceIntensity, <span class="va">self</span>.ValueBeliefs_Qoa)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>Learner.obtain_policy_Xoa <span class="op">=</span> obtain_policy_Xoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Testing</strong> the <code>act</code> and <code>obtain_policy_Xoa</code> methods:</p>
<div id="d2e1edac-9e3a-46d6-bc96-5a5726eb3082" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), <span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="fd668d73-6f25-40d9-a552-32cc5befa661" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>learner.obtain_policy_Xoa()[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([0.5, 0.5])</code></pre>
</div>
</div>
<p>Selecting action uniformly at random for 1000 times should give a mean index of approx. 0.5:</p>
<div id="d3cb4ee2-f6f3-43a5-bb5a-f361beaadcd2" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>np.mean([learner.act(<span class="dv">0</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>0.498</code></pre>
</div>
</div>
</section>
<section id="learning-rule" class="level3">
<h3 class="anchored" data-anchor-id="learning-rule">Learning rule</h3>
<p>The learning rule solves the challenge of delayed consequences. The value beliefs are updated using the Bellman equation in light of new information. As the Bellman equation describes how state(-action) values relate at different timesteps, this reinforcement learning update class is called <strong>temporal-difference learning</strong>.</p>
<p>Given an observed state <span class="math inline">\(s\)</span>, the agent selects an action <span class="math inline">\(a\)</span> and receives a reward <span class="math inline">\(r\)</span>. Then, the agent updates its value beliefs (for state-action pair <span class="math inline">\(s\)</span>-<span class="math inline">\(a\)</span>) according to</p>
<p><span id="eq-RLupdate"><span class="math display">\[Q_{t+1}(s, a) = Q_{t}(s, a) + \alpha\left((1-\gamma)r + \gamma \sum_b x_t(s', b) Q_t(s',b) - Q_{t}(s, a)
\right). \tag{10.1}\]</span></span></p>
<p><strong>DeepDive</strong> | There is some freedom into designing the specifics of the temporal-difference update, especially regarding estimating the value of the next state or observation. The specific update used above is called <strong>Expected SARSA</strong>. It is beyond the scope of this course to discuss the different temporal-difference learning algorithms. The interested reader is referred to excellent material on (multi-agent) reinforcement learning, e.g., <span class="citation" data-cites="SuttonBarto2018 AlbrechtEtAl2024">(<a href="References.html#ref-AlbrechtEtAl2024" role="doc-biblioref">Albrecht et al., 2024</a>; <a href="References.html#ref-SuttonBarto2018" role="doc-biblioref">Sutton &amp; Barto, 2018</a>)</span>.</p>
<p>The extent to which the value beliefs are updated is controlled by a second parameter, <span class="math inline">\(\alpha \in (0, 1)\)</span>, called the <strong>learning rate</strong>.</p>
<p>When <span class="math inline">\(\alpha=0\)</span>, there is no updating, and the reward does not affect value beliefs,</p>
<p><span class="math display">\[Q_{t+1}(s, a) =\!\!|_{\alpha=0} \ \, Q_{t}(s, a) =: \text{old estimate}.\]</span></p>
<p>The value belief update always remains the <em>old estimate</em> of the value beliefs.</p>
<p>When <span class="math inline">\(\alpha = 1\)</span>, the value belief for the state-action pair (<span class="math inline">\(s, a\)</span>) becomes a discount-factor weighted average between the current reward <span class="math inline">\(r\)</span> and the expected value of the next state <span class="math inline">\(\sum_b x_t(s', b) Q_t(s',b)\)</span>,</p>
<p><span class="math display">\[Q_{t+1}(s, a) =\!\!|_{\alpha=1} \ \, (1-\gamma)r + \gamma \sum_b x_t(s', b) Q_t(s',b) =: \text{new estimate}.\]</span></p>
<p>The value belief update is entirely determined by the <em>new estimate</em> of the value beliefs, which is the current reward received <span class="math inline">\(r\)</span> plus the discount factor <span class="math inline">\(\gamma\)</span> multiplied by the expected value of the following state <span class="math inline">\(\sum_b x_t(s', b) Q_t(s',b)\)</span>, and adequately normalized with the prefactor <span class="math inline">\((1-\gamma)\)</span>.</p>
<p>When <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>, the new value belief for the rewarded arm is a <em>weighted average</em> between old value belief and new reward information,</p>
<p><span class="math display">\[\begin{align}
Q_{t+1}(s, a) &amp;= (1-\alpha) \ \text{old estimate} &amp;+ &amp;\alpha \ \text{new estimate}\\
&amp;= (1-\alpha) \ Q_{t}(s, a) &amp;+ &amp;\alpha \left((1-\gamma)r + \gamma \sum_b x_t(s', b) Q_t(s',b)\right).
\end{align}\]</span></p>
<p><strong>Once more, we face a trade-off</strong>. Clearly, setting <span class="math inline">\(\alpha = 0\)</span> is ineffective, as the agent does not acquire knowledge. Yet, if <span class="math inline">\(\alpha\)</span> is excessively high, the agent tends to <em>forget</em> previously learned observation-action associations.</p>
<p><strong>Temporal-difference reward-prediction error</strong>. Another way to think about the update equation (<a href="#eq-RLupdate" class="quarto-xref">Equation&nbsp;<span>10.1</span></a>) is as follows: The value beliefs are updated by the <em>temporal-difference reward-prediction error</em> (TDRP error) times the learning rate. The TDRP error equals the difference between the <em>new estimate</em> and the <em>old estimate</em> of the value beliefs. If the TDRP error is zero, the agent correctly predicted the next reward, and thus, no further adjustments in the value beliefs are necessary.</p>
<p><span class="math display">\[\begin{align}
Q_{t+1}(s, a) &amp;= Q_{t}(s, a) + \alpha \qquad\qquad\qquad\qquad\qquad \text{TDRP-error}, \\
&amp;= Q_{t}(s, a) + \alpha \Big( \qquad \qquad   \text{new estimate} \qquad \qquad \ - \text{old estimate} \Big), \\
&amp;= Q_{t}(s, a) + \alpha \Big( (1-\gamma)r + \gamma \sum_b x_t(s', b) Q_t(s',b) - \quad Q_{t}(s, a) \quad \Big)
\end{align}\]</span></p>
<p><strong>DeepDive</strong> | The exact terminology <em>temporal-difference reward-prediction error</em> is used rather rarely. We use it here to express the interdisciplinary nature of temporal-difference reward-prediction learning. In machine learning, the term <em>temporal-difference error</em> is more common. It describes the difference between the predicted and the observed reward. In psychology and neuroscience, the term <em>reward-prediction error</em> is used in the context of the brain’s dopamine system, where it is thought to signal the difference between the expected and the observed reward. The term <em>temporal-difference reward-prediction error</em> combines both terms, expressing the idea that the agent learns by predicting future rewards.</p>
<p>Executing the value belief update in Python may look like</p>
<div id="fd578128-94b6-478f-883e-58bf5d6fe775" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(<span class="va">self</span>, </span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>           obs: <span class="bu">int</span>,</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>           action: <span class="bu">int</span>,</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>           reward: <span class="bu">float</span>,</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>           next_obs: <span class="bu">int</span>,</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>          ):</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Updates the value beliefs / Q-value of an action."""</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>    temporal_difference <span class="op">=</span><span class="va">self</span>.obtain_temporal_difference(</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>        obs, action, reward, next_obs)</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ValueBeliefs_Qoa[obs, action] <span class="op">=</span> (</span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ValueBeliefs_Qoa[obs, action]</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> <span class="va">self</span>.LearningRate <span class="op">*</span> temporal_difference</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>Learner.update <span class="op">=</span> update</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We program the <code>update</code> method highly modular. It calls the <code>obtain_temporal_difference</code> method to compute the temporal-difference error and then updates the value beliefs accordingly. The <code>obtain_temporal_difference</code> method is defined as follows:</p>
<div id="6712f914-c619-4dc4-86d6-f8e2a15ba341" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obtain_temporal_difference(<span class="va">self</span>,</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>                               obs: <span class="bu">int</span>,</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>                               action: <span class="bu">int</span>,</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>                               reward: <span class="bu">float</span>,</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>                               next_obs: <span class="bu">int</span>,</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>                              ):</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute temporal-difference eorror"""</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    next_Qoa <span class="op">=</span> <span class="va">self</span>.obtain_nextQoa(next_obs)</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    new_estimate <span class="op">=</span> ((<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.DiscountFactor) <span class="op">*</span> reward </span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> <span class="va">self</span>.DiscountFactor <span class="op">*</span> next_Qoa)</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    old_estimate <span class="op">=</span> <span class="va">self</span>.ValueBeliefs_Qoa[obs][action]</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_estimate <span class="op">-</span> old_estimate</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>Learner.obtain_temporal_difference <span class="op">=</span> obtain_temporal_difference</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In here, we call the <code>obtain_nextQoa</code> method to compute the expected value of the next state. The <code>obtain_nextQoa</code> method is defined as follows:</p>
<div id="fe97ba2f-854d-4dee-875a-2e11eeeb8adf" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obtain_nextQoa(<span class="va">self</span>, next_obs: <span class="bu">int</span>):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    policy_Xoa <span class="op">=</span> <span class="va">self</span>.obtain_policy_Xoa()</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(policy_Xoa[next_obs] <span class="op">*</span> <span class="va">self</span>.ValueBeliefs_Qoa[next_obs])</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>Learner.obtain_nextQoa <span class="op">=</span> obtain_nextQoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Testing the update method</strong>: First, let’s assume the agent does not care about future rewards at all and has a discount factor of zero</p>
<div id="d6331c65-278e-457d-9be7-006ec5564b28" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.0</span>,</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="e46f0627-c6eb-4756-a70d-83bee7a24860" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[1., 1.],
       [1., 1.]])</code></pre>
</div>
</div>
<p>Let’s assume the agent selected the action with index <code>0</code> after observing the state with index <code>0</code>, received a reward of zero, and observed the next state with index <code>1</code>.</p>
<div id="5ba1754f-1e80-463d-8f05-b17c80abe452" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>learner.update(obs<span class="op">=</span><span class="dv">0</span>, action<span class="op">=</span><span class="dv">0</span>, reward<span class="op">=</span><span class="dv">0</span>, next_obs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa.<span class="bu">round</span>(<span class="dv">4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[0.9, 1. ],
       [1. , 1. ]])</code></pre>
</div>
</div>
<p>The value belief for the action <code>0</code> in state <code>0</code> is updated exactly as a learning rate weighted average: <span class="math inline">\(\alpha \cdot \text{new estimate} + (1-\alpha) \cdot \text{old estimate}\)</span> <span class="math inline">\(= \alpha 0 + (1-\alpha) 1\)</span> <span class="math inline">\(= 0.1 \cdot 0 + 0.9 \cdot 1\)</span>.</p>
<p>Repeating this update a hundred more time steps updates the value beliefs for the action <code>0</code> in state <code>0</code> to the expected value of zero.</p>
<div id="bcb52359-f172-45ed-9508-e33d63623135" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): learner.update(obs<span class="op">=</span><span class="dv">0</span>, action<span class="op">=</span><span class="dv">0</span>, reward<span class="op">=</span><span class="dv">0</span>, next_obs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa.<span class="bu">round</span>(<span class="dv">4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[0., 1.],
       [1., 1.]])</code></pre>
</div>
</div>
<p>Now, we repeat that test, but with an agent with a discount factor of <span class="math inline">\(\gamma=0.8\)</span>.</p>
<div id="dad85c87-aba5-4ab8-b757-f0ef3103b040" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">1</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.8</span>,</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="2cc50e0b-c475-408a-bed3-1a72cbb86f42" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[1., 1.],
       [1., 1.]])</code></pre>
</div>
</div>
<div id="af1e976c-58a5-4b2f-a478-e4ef6b216b2e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): learner.update(obs<span class="op">=</span><span class="dv">0</span>, action<span class="op">=</span><span class="dv">0</span>, reward<span class="op">=</span><span class="dv">0</span>, next_obs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa.<span class="bu">round</span>(<span class="dv">4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>array([[0.8, 1. ],
       [1. , 1. ]])</code></pre>
</div>
</div>
<p>Now, the value belief for the action <code>0</code> in state <code>0</code> is updated to <span class="math inline">\(0.8\)</span>. Can you explain why?</p>
<p>We have convinced ourselves that the learner’s <code>update</code> methods works as we expect.</p>
<p>Now, we are ready to let it learn in the risk-reward dilemma environment.</p>
</section>
<section id="testing-the-interface" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-interface">Testing the interface</h3>
<div id="f77d911b-884e-483a-b156-437b66bf2091" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(learner.obtain_policy_Xoa())</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" - - - - "</span>)</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" - - - - "</span>)</span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(learner.obtain_policy_Xoa())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.5 0.5]
 [0.5 0.5]]
 - - - - 
 - - - - 
[[0.52328547 0.47671453]
 [0.50631145 0.49368855]]</code></pre>
</div>
</div>
<p>The learning agent’s policy changed. However, not too much. We know from Lecture <a href="./03.01-SequentialDecisions.html">03.01-SequentialDecisions</a> that the agent should learn to prefer the cautious action in both states under these parameter settings.</p>
<p><strong>🫳 Hands-on</strong>: Try re-executing the above cell while making some changes to the parameters. Can you get an intuition of what is important for a successful learning process?</p>
<p>The process of finding the right parameters for the agent is called <strong>hyperparameter tuning</strong>. It is a crucial step in machine learning and often requires a lot of trial and error.</p>
<p><strong>From a modeling point of view</strong>, we aim to go beyond finding the <em>right</em> parameter. We aim to <strong>understand</strong> how the parameters influence the learning process.</p>
<p>Comparing the initial with the final policy is not the best way to facilitate both aims. We need a more refined way to keep track of the learning process.</p>
</section>
</section>
<section id="investigating-the-learning-process" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="investigating-the-learning-process"><span class="header-section-number">10.5</span> Investigating the learning process</h2>
<p>To keep track of the learning process, we store the value beliefs and the policy in a <code>pandas</code> DataFrame. Aditionally, we also record the learning rate and the choice intensity.</p>
<div id="1f4e3e5d-671e-40a2-bf07-32224adc0645" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interface_run(agent, env, NrOfTimesteps):</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run the multi-agent environment for several time steps."""</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> [<span class="st">"action"</span>, <span class="st">"observation"</span>, <span class="st">"reward"</span>, <span class="st">"beliefs"</span>, <span class="st">"policy"</span>,</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>               <span class="st">"ChoiceIntensity"</span>, <span class="st">"LearningRate"</span>]</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(index<span class="op">=</span><span class="bu">range</span>(NrOfTimesteps), columns<span class="op">=</span>columns)</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    observation <span class="op">=</span> env.observe()</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(NrOfTimesteps):</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> agent.act(observation)</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>        next_observation, reward <span class="op">=</span> env.step(action)</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>        agent.update(observation, action, reward, next_observation)</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>        df.loc[t] <span class="op">=</span> (action, next_observation, reward,</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a>                     deepcopy(agent.ValueBeliefs_Qoa), </span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>                     deepcopy(agent.obtain_policy_Xoa()),</span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>                     deepcopy(agent.ChoiceIntensity), </span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>                     deepcopy(agent.LearningRate))</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>        observation <span class="op">=</span> next_observation</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Running the interface,</p>
<div id="922a327a-845c-4d92-a098-0e083b322b1d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">8.0</span>)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>As we stored the value beliefs and policies as two-dimensional <code>numpy</code> arrays, we convert them into three-dimensional <code>numpy</code> with time running on the first dimension:</p>
<div id="70f6e4a4-afbd-4b29-8247-2864c1304aae" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>beliefs_Qtoa <span class="op">=</span> np.array(df.beliefs.values.tolist())</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>policy_Xtoa <span class="op">=</span> np.array(df.policy.values.tolist())</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>beliefs_Qtoa.shape, policy_Xtoa.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>((10000, 2, 2), (10000, 2, 2))</code></pre>
</div>
</div>
<p>We include these conversions into a plotting function that visualizes the learning process</p>
<div id="e66c62d5-b1a6-4caa-b913-b1810c0a457c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_learning_process(df, plot_varying_parameters<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    beliefs_Qtoa <span class="op">=</span> np.array(df.beliefs.values.tolist())</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    policy_Xtoa <span class="op">=</span> np.array(df.policy.values.tolist())</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    beliefs_Qtoa.shape, policy_Xtoa.shape</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>,<span class="dv">6</span>))</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>    ax0 <span class="op">=</span> fig.add_subplot(<span class="dv">311</span>)</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs_Qtoa[:,p,c], label<span class="op">=</span><span class="st">'Q(p,c)'</span>, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs_Qtoa[:,p,r], label<span class="op">=</span><span class="st">'Q(p,r)'</span>, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs_Qtoa[:,d,c], label<span class="op">=</span><span class="st">'Q(d,c)'</span>, color<span class="op">=</span><span class="st">'darkblue'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs_Qtoa[:,d,r], label<span class="op">=</span><span class="st">'Q(d,r)'</span>, color<span class="op">=</span><span class="st">'darkred'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>    ax0.set_ylabel(<span class="st">'Value beliefs'</span>)<span class="op">;</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>    ax0.legend(loc<span class="op">=</span><span class="st">'center right'</span>)<span class="op">;</span> ax0.set_xlim(<span class="op">-</span><span class="dv">10</span>, <span class="bu">len</span>(df)<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>    ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">312</span>, sharex<span class="op">=</span>ax0)</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy_Xtoa[:,p,c], label<span class="op">=</span><span class="st">'X(p,c)'</span>, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy_Xtoa[:,p,r], label<span class="op">=</span><span class="st">'X(p,r)'</span>, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy_Xtoa[:,d,c], label<span class="op">=</span><span class="st">'X(d,c)'</span>, color<span class="op">=</span><span class="st">'darkblue'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy_Xtoa[:,d,r], label<span class="op">=</span><span class="st">'X(d,r)'</span>, color<span class="op">=</span><span class="st">'darkred'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">'Policy'</span>)<span class="op">;</span> ax1.set_xlabel(<span class="st">'Time steps'</span>)</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>    ax1.legend(loc<span class="op">=</span><span class="st">'center right'</span>)</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> plot_varying_parameters:</span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>        ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">615</span>, sharex<span class="op">=</span>ax0)</span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a>        ax2.plot(df.LearningRate, label<span class="op">=</span><span class="st">'LearningRate'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a>        ax2.set_ylabel(<span class="st">"Learning</span><span class="ch">\n</span><span class="st">Rate"</span>)</span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>        ax3 <span class="op">=</span> fig.add_subplot(<span class="dv">616</span>, sharex<span class="op">=</span>ax0)</span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>        ax3.plot(df.ChoiceIntensity, label<span class="op">=</span><span class="st">'ChoiceIntensity'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a>        ax3.set_ylabel(<span class="st">"Choice</span><span class="ch">\n</span><span class="st">Intensity"</span>), ax3.set_xlabel(<span class="st">'Time steps'</span>)</span>
<span id="cb87-32"><a href="#cb87-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb87-33"><a href="#cb87-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plt.tight_layout()</span></span>
<span id="cb87-34"><a href="#cb87-34" aria-hidden="true" tabindex="-1"></a>    plt.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.35</span>)</span>
<span id="cb87-35"><a href="#cb87-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-36"><a href="#cb87-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plt.legend()</span></span>
<span id="cb87-37"><a href="#cb87-37" aria-hidden="true" tabindex="-1"></a>    ax0.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="too-much-exploration" class="level3">
<h3 class="anchored" data-anchor-id="too-much-exploration">Too much exploration</h3>
<p>What happens if the agent explores too much? Can the agent learn the optimal policy, in this case, the cautious policy?</p>
<div id="346d1814-83d1-48b7-a6b5-95dd2e73da81" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="dead569c-05cb-4a80-8448-7199266d4312" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="3f4d6a51-02fa-403a-bfcd-ce6cca4d90d0" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>plot_learning_process(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-68-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning the risk-reward dilemma with to much exploration.</figcaption>
</figure>
</div>
</div>
</div>
<p>The order of the <strong>value beliefs</strong> seems roughly <strong>consistent with</strong> the <strong>optimal policy</strong>, that prefers the cautious action over the risky on in both states. However, the <strong>agents policy is fluctuating around</strong>. The action choice probabilities are fluctuating around their uniformly random value of 0.5. This is a sign that the agent explores too much and exploits too little.</p>
<p><strong>🫳 Hands-on</strong>: Convince yourself of the learning process under high exploration by re-executing the above cell multiple times.</p>
</section>
<section id="too-little-exploration" class="level3">
<h3 class="anchored" data-anchor-id="too-little-exploration">Too little exploration</h3>
<p>Can the agent learn the optimal (cautious) policy under little exploration?</p>
<div id="34aa1079-8bf8-4840-bd13-981865b474a0" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="376f0b88-e10e-4946-9aa4-cf7447b4049a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">250.0</span>)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="16c7137a-7e98-4962-a2cf-d5e422b70127" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>plot_learning_process(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-71-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning the risk-reward dilemma with to much exploitation.</figcaption>
</figure>
</div>
</div>
</div>
<p>Increasing the choice intensity leads to a more exploitative policy. However, what the agent learns depends on the stochasticity of the learning process. In other words, the agent may not learn the optimal policy. Too little exploration harms the learning.</p>
<p><strong>🫳 Hands-on</strong>: Convince yourself of the learning process under high exploitation by re-executing the above cell multiple times.</p>
</section>
<section id="decaying-exploration" class="level3">
<h3 class="anchored" data-anchor-id="decaying-exploration">Decaying exploration</h3>
<p>To mitigate the negative effects of too much exploration towards the end of the learning process and the negative effects of too much exploitation towards the beginning of the learning process, we could let the choice intensity increase over time. This is called <strong>decaying exploration</strong>.</p>
<p>We implement this idea by creating a new agent class <code>AdjustingLearner</code> that inherits from the <code>Learner</code> class. This shows the power of object-oriented programming. We overwrite the <code>update</code> method to include an increasing choice intensity. We also make the learning rate decay over time.</p>
<div id="7b8936fd-11f4-42e2-899a-353e597529a6" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdjustingLearner(Learner):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>                 ValueBeliefs_Qoa, </span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>                 DiscountFactor, </span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>                 LearningRate, MinLearningRate, LearningRateDecayFactor,</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>                 ChoiceIntensity, MaxChoiceIntensity, ChoiceIntensityGrowthFactor,</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>                ):</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.DiscountFactor <span class="op">=</span> DiscountFactor</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LearningRate <span class="op">=</span> LearningRate</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.MinLearningRate <span class="op">=</span> MinLearningRate</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LearningRateDecayFactor <span class="op">=</span> LearningRateDecayFactor</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ChoiceIntensity <span class="op">=</span> ChoiceIntensity</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.MaxChoiceIntensity <span class="op">=</span> MaxChoiceIntensity</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ChoiceIntensityGrowthFactor <span class="op">=</span> ChoiceIntensityGrowthFactor</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ValueBeliefs_Qoa <span class="op">=</span> ValueBeliefs_Qoa</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ActionIxs <span class="op">=</span> <span class="bu">range</span>(ValueBeliefs_Qoa.shape[<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="2c5d522e-a494-4c7d-a15e-6b6c178ddc58" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(<span class="va">self</span>, </span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>           obs: <span class="bu">int</span>,</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>           action: <span class="bu">int</span>,</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>           reward: <span class="bu">float</span>,</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>           next_obs: <span class="bu">int</span>,</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>          ):</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Updates the value beliefs / Q-value of an action."""</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>    temporal_difference <span class="op">=</span><span class="va">self</span>.obtain_temporal_difference(</span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>        obs, action, reward, next_obs)</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ValueBeliefs_Qoa[obs, action] <span class="op">=</span> (</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ValueBeliefs_Qoa[obs, action] <span class="op">+</span> <span class="va">self</span>.LearningRate <span class="op">*</span> temporal_difference</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.LearningRate <span class="op">=</span> <span class="bu">max</span>(<span class="va">self</span>.MinLearningRate, </span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>                            <span class="va">self</span>.LearningRate <span class="op">*</span> <span class="va">self</span>.LearningRateDecayFactor)</span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ChoiceIntensity <span class="op">=</span> <span class="bu">min</span>(<span class="va">self</span>.MaxChoiceIntensity, </span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>                               <span class="va">self</span>.ChoiceIntensity <span class="op">*</span> <span class="va">self</span>.ChoiceIntensityGrowthFactor)</span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a>AdjustingLearner.update <span class="op">=</span> update</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now, we are ready to perform a new learning simulation.</p>
<div id="16d045d9-e71b-497e-8659-bbb59fb837c7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="0c8542ba-57be-4941-86e8-06acf4389330" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> AdjustingLearner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>                           DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>                           LearningRate <span class="op">=</span> <span class="fl">0.1</span>, MinLearningRate <span class="op">=</span> <span class="fl">0.01</span>, LearningRateDecayFactor <span class="op">=</span> <span class="fl">0.999</span>,</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>                           ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>, MaxChoiceIntensity <span class="op">=</span> <span class="fl">50.0</span>, ChoiceIntensityGrowthFactor <span class="op">=</span> <span class="fl">1.001</span>)</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="57b685f4-93ea-4a24-8eae-45e32d8c1f05" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>plot_learning_process(df, plot_varying_parameters<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-76-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning the risk-reward dilemma with an increasing choice intensity.</figcaption>
</figure>
</div>
</div>
</div>
<p>We find that the <code>AdjustingLearner</code> is able to consistently learn the optimal policy.</p>
<p><strong>🫳 Hands-on</strong>: Try to convince yourself that this is true by re-executing the simulation above multiple times.</p>
<p>Obviously, the <code>AdjustingLearner</code> is a more complex agent than the <code>Learner</code>. There is also a prominent <em>trick</em> to give our simpler <code>Learner</code> agent an initial exploration bonus.</p>
</section>
<section id="initial-exploration-bonus" class="level3">
<h3 class="anchored" data-anchor-id="initial-exploration-bonus">Initial exploration bonus</h3>
<p>We can give the agent an initial exploration bonus by setting the initial value beliefs to a high value. This is called <strong>optimistic initialization</strong>.</p>
<div id="216e0f4b-eb29-4681-95fe-b74de627ca15" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="8cfc0bed-93c2-48ed-8fd6-f85af99a5ff4" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">8</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">60.0</span>)</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f631e444-7574-45ab-83d3-83b940b817f7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;subslide&quot;}}" data-tags="[]">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>plot_learning_process(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-79-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning the risk-reward dilemma with an initial exploration bonus.</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>🫳 Hands-on</strong>: Convince yourself that optimistic initialization works by re-executing the simulation above multiple times.</p>
</section>
</section>
<section id="learning-goals-revisited" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="learning-goals-revisited"><span class="header-section-number">10.6</span> Learning goals revisited</h2>
<ul>
<li>Reinforcement learning is <strong>valuable in models of human-environment interactions</strong> as a principled take to integrate individual cognition in dynamic environments and emerging collective behavior</li>
<li>We implemented the different elements of the multi-agent environment framework (interface, environment, agents).
<ul>
<li>We implemented and applied a <strong>basic temporal-different learning</strong> agent.</li>
<li>We implemented and applied the <strong>risk-reward dilemma</strong> (Lecture <a href="https://wbarfuss.github.io/csm-of-hei/03.01-SequentialDecisions.html">03.01</a>)</li>
<li>We visualized the learning process</li>
</ul></li>
<li>We introduced and studied the <strong>exploration-exploitation trade-off</strong>, a general challenge for decision-making under uncertainty.</li>
<li>We made all of this possible by using the Python library <code>pandas</code> to manage data and refining our skills in object-oriented programming.</li>
</ul>
<p>The <a href="https://wbarfuss.github.io/csm-of-hei/04.02ex-IndividualLearning.html">exercises for this lecture</a> will explore the learning process in a risk-reward dilemma where the risky policy is the optimal one.</p>
<section id="key-advantages-of-an-rl-framework" class="level3">
<h3 class="anchored" data-anchor-id="key-advantages-of-an-rl-framework">Key advantages of an RL framework</h3>
<ul>
<li>Cognitive mechanisms are more integrated / less fragmented than behavioral theories</li>
<li>Cognitive mechanisms (as in RL) are more formalized than behavioral theories</li>
<li>The RL frame provides a natural dynamic extension to some economic equilibrium decision models.</li>
<li>The RL frame allows for the study of behavior changes (e.g., after experimental policy interventions or environmental catastrophes)</li>
</ul>
<p>So far, we have investigated the learning process in a single environment.</p>
<p>Next, we will explore learning in games to illustrate the modularity of the agent-environment interface, utilizing the same learning agents as previously discussed.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-AlbrechtEtAl2024" class="csl-entry" role="listitem">
Albrecht, S. V., Christianos, F., &amp; Schäfer, L. (2024). <em>Multi-<span>Agent Reinforcement Learning</span>: <span>Foundations</span> and <span>Modern Approaches</span></em>.
</div>
<div id="ref-BotvinickEtAl2020" class="csl-entry" role="listitem">
Botvinick, M., Wang, J. X., Dabney, W., Miller, K. J., &amp; Kurth-Nelson, Z. (2020). Deep <span>Reinforcement Learning</span> and <span>Its Neuroscientific Implications</span>. <em>Neuron</em>, <em>107</em>(4), 603–616. <a href="https://doi.org/10.1016/j.neuron.2020.06.014">https://doi.org/10.1016/j.neuron.2020.06.014</a>
</div>
<div id="ref-ConstantinoEtAl2021" class="csl-entry" role="listitem">
Constantino, S. M., Schlüter, M., Weber, E. U., &amp; Wijermans, N. (2021). Cognition and behavior in context: A framework and theories to explain natural resource use decisions in social-ecological systems. <em>Sustainability Science</em>, <em>16</em>(5), 1651–1671. <a href="https://doi.org/10.1007/s11625-021-00989-w">https://doi.org/10.1007/s11625-021-00989-w</a>
</div>
<div id="ref-SchluterEtAl2017" class="csl-entry" role="listitem">
Schlüter, M., Baeza, A., Dressler, G., Frank, K., Groeneveld, J., Jager, W., Janssen, M. A., McAllister, R. R. J., Müller, B., Orach, K., Schwarz, N., &amp; Wijermans, N. (2017). A framework for mapping and comparing behavioural theories in models of social-ecological systems. <em>Ecological Economics</em>, <em>131</em>, 21–35. <a href="https://doi.org/10.1016/j.ecolecon.2016.08.008">https://doi.org/10.1016/j.ecolecon.2016.08.008</a>
</div>
<div id="ref-SuttonBarto2018" class="csl-entry" role="listitem">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (Second edition). The MIT Press.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04.01-BehavioralAgency.html" class="pagination-link" aria-label="Behavioral agency">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Behavioral agency</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./References.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>