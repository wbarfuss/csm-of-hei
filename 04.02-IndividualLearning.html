<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Individual learning – Complex Systems Modeling of Human-Environment Interactions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04.03-LearningDynamics.html" rel="next">
<link href="./04.01-BehavioralAgency.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-dda8909163e0b4f3670ba323ebd66e56.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-TransformationAgency.html">Transformation Agency</a></li><li class="breadcrumb-item"><a href="./04.02-IndividualLearning.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/CSMofHEI_Logo.drawio.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="./images/CSMofHEI_Logo.drawio.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Complex Systems Modeling of Human-Environment Interactions</a> 
        <div class="sidebar-tools-main">
    <a href="./Complex-Systems-Modeling-of-Human-Environment-Interactions.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.01-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./02-DynamicSystems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dynamic Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.01-Nonlinearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Nonlinearity</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.02-TippingElements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tipping elements</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.03-Resilience.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Resilience</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.04-StateTransitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">State transitions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./03-TargetEquilibria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Target Equilibria</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.01-SequentialDecisions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Sequential Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.02-StrategicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Strategic Interactions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.03-DynamicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dynamic Interactions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./04-TransformationAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformation Agency</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.01-BehavioralAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Behavioral agency</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.02-IndividualLearning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.03-LearningDynamics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Learning dynamics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Exercises</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.02ex-IntroToPython.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Introduction to Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.01ex-Nonlinearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Nonlinearity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.02ex-TippingElements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Tipping elements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.03ex-Resilience.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Resilience</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.04ex-StateTransitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | State transitions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.01ex-SequentialDecisions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Sequential Decisions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.02ex-StrategicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Strategic Interactions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.03ex-DynamicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Dynamic Interactions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.01ex-BehavioralAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Behavioral Agency</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.02ex-IndividualLearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Individual Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.03ex-LearningDynamics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Learning Dynamics</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation"><span class="header-section-number">10.1</span> Motivation</a>
  <ul class="collapse">
  <li><a href="#using-behavioral-theories-in-abms-is-challenging" id="toc-using-behavioral-theories-in-abms-is-challenging" class="nav-link" data-scroll-target="#using-behavioral-theories-in-abms-is-challenging">Using behavioral theories in ABMs is challenging</a></li>
  <li><a href="#reinforcement-learning-offers-a-principled-take" id="toc-reinforcement-learning-offers-a-principled-take" class="nav-link" data-scroll-target="#reinforcement-learning-offers-a-principled-take">Reinforcement learning offers a principled take</a></li>
  <li><a href="#an-integrating-platform-for-cognitive-mechanisms" id="toc-an-integrating-platform-for-cognitive-mechanisms" class="nav-link" data-scroll-target="#an-integrating-platform-for-cognitive-mechanisms">An integrating platform for cognitive mechanisms</a></li>
  <li><a href="#learning-goals" id="toc-learning-goals" class="nav-link" data-scroll-target="#learning-goals">Learning goals</a></li>
  </ul></li>
  <li><a href="#elements-of-the-multi-agent-environment-interface" id="toc-elements-of-the-multi-agent-environment-interface" class="nav-link" data-scroll-target="#elements-of-the-multi-agent-environment-interface"><span class="header-section-number">10.2</span> Elements of the multi-agent environment interface</a></li>
  <li><a href="#example-risk-reward-dilemma" id="toc-example-risk-reward-dilemma" class="nav-link" data-scroll-target="#example-risk-reward-dilemma"><span class="header-section-number">10.3</span> Example | Risk Reward Dilemma</a>
  <ul class="collapse">
  <li><a href="#transitions-environmental-dynamics" id="toc-transitions-environmental-dynamics" class="nav-link" data-scroll-target="#transitions-environmental-dynamics">Transitions | Environmental dynamics</a></li>
  <li><a href="#rewards-short-term-welfare" id="toc-rewards-short-term-welfare" class="nav-link" data-scroll-target="#rewards-short-term-welfare">Rewards | Short-term welfare</a></li>
  <li><a href="#init-method" id="toc-init-method" class="nav-link" data-scroll-target="#init-method">Init method</a></li>
  <li><a href="#testing-the-interface" id="toc-testing-the-interface" class="nav-link" data-scroll-target="#testing-the-interface">Testing the interface</a></li>
  </ul></li>
  <li><a href="#reinforcement-learning-agent" id="toc-reinforcement-learning-agent" class="nav-link" data-scroll-target="#reinforcement-learning-agent"><span class="header-section-number">10.4</span> Reinforcement learning agent</a>
  <ul class="collapse">
  <li><a href="#value-beliefs" id="toc-value-beliefs" class="nav-link" data-scroll-target="#value-beliefs">Value beliefs</a></li>
  <li><a href="#behavioral-rule-exploration-exploitation-trade-off" id="toc-behavioral-rule-exploration-exploitation-trade-off" class="nav-link" data-scroll-target="#behavioral-rule-exploration-exploitation-trade-off">Behavioral rule | Exploration-exploitation trade-off</a></li>
  <li><a href="#learning-rule-temporal-difference-learning" id="toc-learning-rule-temporal-difference-learning" class="nav-link" data-scroll-target="#learning-rule-temporal-difference-learning">Learning rule | Temporal-difference learning</a></li>
  <li><a href="#testing-the-interface-1" id="toc-testing-the-interface-1" class="nav-link" data-scroll-target="#testing-the-interface-1">Testing the interface</a></li>
  </ul></li>
  <li><a href="#investigating-the-learning-process" id="toc-investigating-the-learning-process" class="nav-link" data-scroll-target="#investigating-the-learning-process"><span class="header-section-number">10.5</span> Investigating the learning process</a>
  <ul class="collapse">
  <li><a href="#to-little-exploitation-to-much-exploration" id="toc-to-little-exploitation-to-much-exploration" class="nav-link" data-scroll-target="#to-little-exploitation-to-much-exploration">To little exploitation | To much exploration</a></li>
  <li><a href="#to-much-exploitation-to-little-exploration" id="toc-to-much-exploitation-to-little-exploration" class="nav-link" data-scroll-target="#to-much-exploitation-to-little-exploration">To much exploitation | To little exploration</a></li>
  <li><a href="#decaying-exploration-increasing-exploitation" id="toc-decaying-exploration-increasing-exploitation" class="nav-link" data-scroll-target="#decaying-exploration-increasing-exploitation">Decaying exploration | Increasing exploitation</a></li>
  <li><a href="#initial-exploration-bonus" id="toc-initial-exploration-bonus" class="nav-link" data-scroll-target="#initial-exploration-bonus">Initial exploration bonus</a></li>
  </ul></li>
  <li><a href="#multi-agent-environments-games" id="toc-multi-agent-environments-games" class="nav-link" data-scroll-target="#multi-agent-environments-games"><span class="header-section-number">10.6</span> Multi-agent environments | Games</a>
  <ul class="collapse">
  <li><a href="#interface-1" id="toc-interface-1" class="nav-link" data-scroll-target="#interface-1">Interface</a></li>
  <li><a href="#social-dilemmas-environment" id="toc-social-dilemmas-environment" class="nav-link" data-scroll-target="#social-dilemmas-environment">Social dilemmas environment</a></li>
  <li><a href="#testing-the-implementation" id="toc-testing-the-implementation" class="nav-link" data-scroll-target="#testing-the-implementation">Testing the implementation:</a></li>
  <li><a href="#transient-cooperation" id="toc-transient-cooperation" class="nav-link" data-scroll-target="#transient-cooperation">Transient cooperation</a></li>
  </ul></li>
  <li><a href="#learning-goals-revisited" id="toc-learning-goals-revisited" class="nav-link" data-scroll-target="#learning-goals-revisited"><span class="header-section-number">10.7</span> Learning goals revisited</a>
  <ul class="collapse">
  <li><a href="#key-advantages-of-an-rl-framework" id="toc-key-advantages-of-an-rl-framework" class="nav-link" data-scroll-target="#key-advantages-of-an-rl-framework">Key advantages of an RL framework</a></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges">Challenges</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-TransformationAgency.html">Transformation Agency</a></li><li class="breadcrumb-item"><a href="./04.02-IndividualLearning.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="https://wbarfuss.github.io">Wolfram Barfuss</a> | <a href="https://www.uni-bonn.de">University of Bonn</a> | 2024/2025 <br> ▶ <strong>Complex Systems Modeling of Human-Environment Interactions</strong></p>
<section id="motivation" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">10.1</span> Motivation</h2>
<blockquote class="blockquote">
<p>Give a man a fish, and he’ll eat for a day</p>
</blockquote>
<blockquote class="blockquote">
<p>Teach a man to fish, and he’ll eat for a lifetime</p>
</blockquote>
<blockquote class="blockquote">
<p>Give a man a taste for fish, and he’ll eat even if conditions change. [<a href="https://www.coursera.org/lecture/fundamentals-of-reinforcement-learning/michael-littman-the-reward-hypothesis-q6x0e">source</a>]</p>
</blockquote>
<p>In this chapter, we will introduce the basics of temporal-difference reward-prediction reinforcement learning.</p>
<section id="using-behavioral-theories-in-abms-is-challenging" class="level3">
<h3 class="anchored" data-anchor-id="using-behavioral-theories-in-abms-is-challenging">Using behavioral theories in ABMs is challenging</h3>
<p>General <strong>agent-based modeling</strong> is a flexible tool for studying different theories of human behavior. However, the social and behavioral sciences are not known for their tendency to integrate. <strong>Knowledge about human behavior is fragmented into many different, context-specific, and often not formalized theories</strong>. For example, in an attempt to order and use this knowledge for sustainability science, Constantino and colleagues presented a selection of 32 behavioral theories <span class="citation" data-cites="ConstantinoEtAl2021">(<a href="References.html#ref-ConstantinoEtAl2021" role="doc-biblioref">Constantino et al., 2021</a>)</span>.</p>
<p><img src="images/04.02-BehavioralTheories.dio.png" class="img-fluid"></p>
<p>The many behavioral theories pose a significant <strong>challenge for general agent-based modeling</strong> when it comes to incorporating human decision-making into models of Nature-society systems <span class="citation" data-cites="SchluterEtAl2017">(<a href="References.html#ref-SchluterEtAl2017" role="doc-biblioref">Schlüter et al., 2017</a>)</span>:</p>
<ol type="1">
<li><strong>Fragmentation of theories</strong>: A vast array of theories on human decision-making is scattered across different disciplines, making it difficult to navigate and select relevant theories. Each theory often focuses on specific aspects of decision-making, leading to <strong>fragmented knowledge</strong>.</li>
<li><strong>Incomplete theories</strong>: The degree of formalization varies across theories. Many decision-making theories are incomplete or not fully formalized, requiring modelers to fill logical gaps with assumptions to make simulations work. This step introduces more degrees of freedom and possibly arbitrariness into the modeling process.</li>
<li><strong>Correlation-based theories</strong>: Many theories focus on correlations rather than causal mechanisms, essential for dynamic modeling. This requires modelers to make explicit assumptions about causal relationships - introducing more degrees of freedom and possibly arbitrariness into the modeling process.</li>
<li><strong>Context-dependent theories</strong>: The applicability of theories can vary greatly depending on the context, which adds complexity to their integration into models.</li>
</ol>
</section>
<section id="reinforcement-learning-offers-a-principled-take" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-learning-offers-a-principled-take">Reinforcement learning offers a principled take</h3>
<p>Reinforcement learning (RL) offers a general prototype model for intelligent &amp; adaptive decision-making in agent-based models.</p>
<p><strong>Principle</strong></p>
<blockquote class="blockquote">
<p>“Do more of what makes you happy.”</p>
</blockquote>
<p><strong>We do not need to specify the behavior of an agent</strong> directly.</p>
<p><strong>Instead, we specify what an agent wants</strong> and how it learns. Crucially, how it learns does not depend on the details of the environment model. It is a more general process, applicable across different environments.</p>
<p><strong>Then, it learns for itself what to do</strong> and we study the learning process and the learned behaviors.</p>
<p>This approach is <strong>particularly valuable</strong> for studying human-environment systems <strong>when the decision environment changes</strong> through a policy intervention or a global change process, like climate change or biodiversity loss. If we had specified the agent’s behavior directly, the agent’s behavior could not change when the environment changes. In contrast, if we specify the underlying agent’s goal, we can study how the agent’s behavior changes when the environment changes. Reinforcement learning agents can <strong>learn while interacting</strong> with the environment.</p>
</section>
<section id="an-integrating-platform-for-cognitive-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="an-integrating-platform-for-cognitive-mechanisms">An integrating platform for cognitive mechanisms</h3>
<p>RL, broadly understood, offers an interdisciplinary platform for integrating <strong>cognitive mechanisms</strong> into ABMs. It offers a comprehensive framework for studying the interplay among <strong>learning</strong> (adaptive behavior), <strong>representation</strong> (beliefs), and <strong>decision-making</strong> (actions) <span class="citation" data-cites="BotvinickEtAl2020">(<a href="References.html#ref-BotvinickEtAl2020" role="doc-biblioref">Botvinick et al., 2020</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/04.02-CognitiveFrameworks.dio.png" class="img-fluid figure-img"></p>
<figcaption>RL-based frameworks with cognitive mechanisms</figcaption>
</figure>
</div>
<p>Collective or multi-agent reinforcement learning is a natural extension of RL to study the emerging <strong>collective behavior</strong> of multiple agents in dynamic environments. It allows for formulating hypotheses on how different <strong>cognitive mechanisms</strong> affect <strong>collective behavior</strong> in <strong>dynamic environments</strong> <span class="citation" data-cites="BarfussEtAl2024a">(<a href="References.html#ref-BarfussEtAl2024a" role="doc-biblioref">Barfuss et al., 2024</a>)</span>.</p>
<p>RL is also an <strong>interdisciplinary endeavor</strong>, studied in Psychology, Neuroscience, Behavioral economics, Complexity science, and Machine learning.</p>
<div id="fig-RLinBrain" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-RLinBrain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/04.02-DopamineRewardPredictionError.dio.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-RLinBrain-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Reinforcement learning in the brain
</figcaption>
</figure>
</div>
<p><a href="#fig-RLinBrain" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> shows the <strong>remarkable analogy between the firing patterns of dopamine neurons in the brain and the prediction errors in a reinforcement learning simulation</strong>.</p>
<p><a href="#fig-RLinBrain" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> (a-c) shows prediction errors in a Pavlovian RL conditioning task simulation. A conditional stimulus (CS) is presented randomly, followed 2 seconds later by a reward (Unconditional Stimulus - US). (a) In the early training phase, the reward is not anticipated, leading to prediction errors when the reward is presented. As learning occurs, these prediction errors begin to affect prior events in the trial (examples from trials 5 and 10) because predictive values are learned. (b) After learning, the previously unexpected reward no longer creates a prediction error. Instead, the conditional stimulus now causes a prediction error when it occurs unexpectedly. (c) When the reward is omitted when expected, it results in a negative prediction error, signaling that what happened was worse than anticipated.</p>
<p><a href="#fig-RLinBrain" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> (d–f) Firing patterns of dopamine neurons in monkeys engaged in a similar instrumental conditioning task [SchultzEtAl1997]. Each raster plot shows action potentials (dots) with different rows for different trials aligned with the cue (or reward) timing. Histograms show combined activity across the trials below. (d) When a reward is unexpectedly received, dopamine neurons fire rapidly. (e) After conditioning with a visual cue (which predicted a food reward if the animal performed correctly), the reward no longer triggers a burst of activity; now, the burst happens at the cue’s presentation. (f) If the food reward is omitted unexpectedly, dopamine neurons exhibit a distinct pause in firing, falling below their typical rate.</p>
<p><strong>Source of confusion.</strong> Because of its broad scope and interdisciplinary nature, simply the phrase “reinforcement learning” can mean different things to different people. To mitigate this possible source of confusion, it is good to acknowledge that RL can refer to a <strong>model of human learning</strong>, an <strong>optimization method</strong>, a <strong>problem description</strong>, and a <strong>field of research</strong>.</p>
</section>
<section id="learning-goals" class="level3">
<h3 class="anchored" data-anchor-id="learning-goals">Learning goals</h3>
<p>After this chapter, students will be able to:</p>
<ul>
<li>Explain why reinforcement learning is valuable in models of human-environment interactions</li>
<li>Implement and apply the different elements of the multi-agent environment framework, including a temporal-different learning agent.</li>
<li>Explain and manage the trade-off between exploration and exploitation.</li>
<li>Visualize the learning process</li>
<li>Use the Python library <code>pandas</code> to manage data</li>
<li>Refine their skills in object-oriented programming</li>
</ul>
<div id="3b20cb0f-e4e1-41af-b0fd-44e514dcf80f" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np  </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact, interactive</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> deepcopy</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.style <span class="im">as</span> style<span class="op">;</span> style.use(<span class="st">'seaborn-v0_8'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">4</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>color <span class="op">=</span> plt.rcParams[<span class="st">'axes.prop_cycle'</span>].by_key()[<span class="st">'color'</span>][<span class="dv">0</span>]  <span class="co"># get the first color of the default color cycle</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.facecolor'</span>] <span class="op">=</span> <span class="st">'white'</span><span class="op">;</span> plt.rcParams[<span class="st">'grid.color'</span>] <span class="op">=</span> <span class="st">'gray'</span><span class="op">;</span> </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'grid.linewidth'</span>] <span class="op">=</span> <span class="fl">0.25</span><span class="op">;</span> </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.dpi'</span>] <span class="op">=</span> <span class="dv">140</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="elements-of-the-multi-agent-environment-interface" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="elements-of-the-multi-agent-environment-interface"><span class="header-section-number">10.2</span> Elements of the multi-agent environment interface</h2>
<p>Generally, making sense of an agent without its environment is difficult, and vice versa.</p>
<div id="fig-MAEiRLFrame" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-MAEiRLFrame-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/04.02-MAEiRLFrame.dio.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MAEiRLFrame-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2: Reinforcement learning in the multi-agent environment interface
</figcaption>
</figure>
</div>
<section id="interface" class="level4">
<h4 class="anchored" data-anchor-id="interface">Interface</h4>
<p>At the interface between agents and the environment are</p>
<ul>
<li>each agent’s <strong>set of</strong> (conceivable) <strong>actions</strong> - from agents to environment,</li>
<li>extrinsic <strong>reward signals</strong> - a single number from environment to each agent,</li>
<li>possibly <strong>observation signals</strong> - from environment to agents.</li>
</ul>
<p>Note: In general, the environment is composed of the <em>natural</em> and the <em>social</em> environment.</p>
<div id="93e7193c-ab16-43c8-9830-e2fb42cd153a" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interface_run(agents, env, NrOfTimesteps):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run the multi-agent environment for several time steps."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    observations <span class="op">=</span> env.observe()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(NrOfTimesteps):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        actions <span class="op">=</span> [agent.act(observations[i])</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, agent <span class="kw">in</span> <span class="bu">enumerate</span>(agents)]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        next_observations, rewards, info <span class="op">=</span> env.step(actions)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, agent <span class="kw">in</span> <span class="bu">enumerate</span>(agents):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            agent.update(observations[i], actions[i], rewards[i], next_observations[i])</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        observations <span class="op">=</span> next_observations                </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="environment" class="level4">
<h4 class="anchored" data-anchor-id="environment">Environment</h4>
<p>The environment delivers <strong>extrinsic rewards</strong> (motivations) to the agents based on the <strong>agents’ chosen actions</strong> (choices). It may contain environmental <strong>states</strong>, which may not be fully <strong>observable</strong> to the agents</p>
<p>The most common <strong>environment classes</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Agents</th>
<th>Environment</th>
<th>Observation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Multi-armed bandit</strong></td>
<td>one</td>
<td>no states</td>
<td>-</td>
</tr>
<tr class="even">
<td><strong>Normal-form game</strong></td>
<td>multiple</td>
<td>no states</td>
<td>-</td>
</tr>
<tr class="odd">
<td><strong>Markov decision process</strong></td>
<td>one</td>
<td>multiple states</td>
<td>full</td>
</tr>
<tr class="even">
<td><strong>Stochastic/Markov games</strong></td>
<td>multiple</td>
<td>multiple states</td>
<td>full</td>
</tr>
<tr class="odd">
<td><strong>Partially observable Markov decision process</strong></td>
<td>one</td>
<td>multiple states</td>
<td>partial</td>
</tr>
<tr class="even">
<td><strong>Partially observable stochastic games</strong></td>
<td>multiple</td>
<td>multiple states</td>
<td>partial</td>
</tr>
</tbody>
</table>
<p>In all cases, <strong>reward signals</strong> may be <strong>stochastic</strong> and or <strong>multi-dimensional</strong>.</p>
<div id="9e5ac398-939a-4169-9a6f-e0a47151e8a6" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Environment:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Abstract environment class."""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_StateSet(<span class="va">self</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Default state set representation `state_s`."""</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="bu">str</span>(s) <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.Z)]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_ActionSets(<span class="va">self</span>):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Default action set representation `action_a`."""</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="bu">str</span>(a) <span class="cf">for</span> a <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.M)]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>             jA <span class="co"># joint actions</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            ) <span class="op">-&gt;</span> <span class="bu">tuple</span>:  <span class="co"># (observations_Oi, rewards_Ri, info)</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Iterate the environment one step forward.</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># choose a next state according to transition tensor T</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        tps <span class="op">=</span> <span class="va">self</span>.TransitionTensor[<span class="bu">tuple</span>([<span class="va">self</span>.state]<span class="op">+</span><span class="bu">list</span>(jA))].astype(<span class="bu">float</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(tps)), p<span class="op">=</span>tps)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># obtain the current rewards</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        rewards <span class="op">=</span> <span class="va">self</span>.RewardTensor[<span class="bu">tuple</span>([<span class="bu">slice</span>(<span class="va">self</span>.N),<span class="va">self</span>.state]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>                                          <span class="op">+</span><span class="bu">list</span>(jA)<span class="op">+</span>[next_state])]</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># advance the state and collect info</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state <span class="op">=</span> next_state</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        obs <span class="op">=</span> <span class="va">self</span>.observe()     </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># report the true state in the info dict</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        info <span class="op">=</span> {<span class="st">'state'</span>: <span class="va">self</span>.state}</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> obs, rewards.astype(<span class="bu">float</span>), info</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> observe(<span class="va">self</span>):</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Observe the environment."""</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.state <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.N)]</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="agents" class="level4">
<h4 class="anchored" data-anchor-id="agents">Agents</h4>
<p>Agents act (oftentimes to reach a goal). We need to specify their</p>
<ul>
<li><strong>Actions</strong>, describing which choices are available to the agent.</li>
<li><strong>Goal</strong>, describing what an agent wants (in the long run). They may contain <em>intrinsic motivations</em>.</li>
<li><strong>Representation</strong>, e.g., defining upon which conditions agents select actions (e.g., <em>history</em> of past actions in multi-agent situations).</li>
<li><strong>Value beliefs</strong> (value functions), capturing what is <em>good</em> for the agent regarding its <em>goal</em> in the long run.</li>
<li><strong>Behavioral rule</strong> (policy, strategy), defining how to select actions.</li>
<li><strong>Learning rule</strong>, describing how value beliefs are updated in light of new information.</li>
<li>(optionally), a <strong>model</strong> of the environment and rewards. Models are used for <em>planning</em>, i.e., deciding on a <em>behavioral rule</em> by considering possible future situations before they are actually experienced.</li>
</ul>
<div id="7200a2b5-869f-4150-b3ce-09fc98a967be" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BehaviorAgent:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, policy):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.policy_Xoa <span class="op">=</span> policy <span class="op">/</span> policy.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ActionIxs <span class="op">=</span> <span class="bu">range</span>(<span class="va">self</span>.policy_Xoa.shape[<span class="dv">1</span>])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, obs):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.choice(<span class="va">self</span>.ActionIxs, p<span class="op">=</span><span class="va">self</span>.policy_Xoa[obs])</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="example-risk-reward-dilemma" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="example-risk-reward-dilemma"><span class="header-section-number">10.3</span> Example | Risk Reward Dilemma</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/03.01-RiskRewardDilemma.dio.png" class="img-fluid figure-img"></p>
<figcaption>Risk Reward Dilemma</figcaption>
</figure>
</div>
<div id="adf4cf46-15cf-46db-9914-6296a657ce9f" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RiskRewardDilemma(Environment):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A simple risk-reward dilemma environment."""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_StateSet(<span class="va">self</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="st">'p'</span>, <span class="st">'d'</span>]  <span class="co"># prosperous, degraded</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_ActionSets(<span class="va">self</span>):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [[<span class="st">'c'</span>, <span class="st">'r'</span>]]  <span class="co"># cautious, risky</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="transitions-environmental-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="transitions-environmental-dynamics">Transitions | Environmental dynamics</h3>
<p>The <strong>environmental dynamics</strong>, i.e., the transitions between environmental state contexts are modeled by two parameters: a collapse probability, <span class="math inline">\(p_c\)</span>, and a recovery probability, <span class="math inline">\(p_r\)</span>.</p>
<div id="5e727975-b2c4-4c16-9f67-64ee4f2e664c" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>pc, pr <span class="op">=</span> sp.symbols(<span class="st">'p_c p_r'</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>pc</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="11">
<p><span class="math inline">\(\displaystyle p_{c}\)</span></p>
</div>
</div>
<div id="f30d3e1a-7220-4bcd-800b-cf58062564d2" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> RiskRewardDilemma().obtain_StateSet().index(<span class="st">'p'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> RiskRewardDilemma().obtain_StateSet().index(<span class="st">'d'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> RiskRewardDilemma().obtain_ActionSets()[<span class="dv">0</span>].index(<span class="st">'c'</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> RiskRewardDilemma().obtain_ActionSets()[<span class="dv">0</span>].index(<span class="st">'r'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>p,d,c,r    </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>(0, 1, 0, 1)</code></pre>
</div>
</div>
<p>We implement the transitions as a three-dimensional array or <strong>tensors</strong>, with dimensions <span class="math inline">\(Z \times M \times Z\)</span>, where <span class="math inline">\(Z\)</span> is the number of states and <span class="math inline">\(M\)</span> is the number of actions.</p>
<div id="e1c3ffca-b60d-40ab-85d7-fa8eeea791d0" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.zeros((<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">object</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The cautious action guarantees to remain in the prosperous state, <span class="math inline">\(T(\mathsf{p,c,p})=1\)</span>. Thus, the agent can avoid the risk of environmental collapse by choosing the cautious action, <span class="math inline">\(T(\mathsf{p,c,d})=0\)</span>.</p>
<div id="4407af08-fa26-411d-9e27-a0afe09172cd" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>T[p,c,d] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>T[p,c,p] <span class="op">=</span> <span class="dv">1</span>   </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The risky action risks the collapse to the degraded state, <span class="math inline">\(T(\mathsf{p,r,d}) = p_c\)</span>, with a collapse probability <span class="math inline">\(p_c\)</span>. Thus, with probability <span class="math inline">\(1-p_c\)</span>, the environment remains prosperous under the risky action, <span class="math inline">\(T(\mathsf{p,r,p}) = 1-p_c\)</span>.</p>
<div id="426b6cd2-e046-46e2-9aea-92ffee8cae5b" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>T[p,r,d] <span class="op">=</span> pc</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>T[p,r,p] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>pc</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>At the degraded state, recovery is only possible through the cautious action, <span class="math inline">\(T(\mathsf{d,c,p})=p_r\)</span>, with recovery probability <span class="math inline">\(p_r\)</span>. Thus, with probability <span class="math inline">\(1-p_r\)</span>, the environment remains degraded under the cautious action, <span class="math inline">\(T(\mathsf{d,c,d})=1-p_r\)</span>.</p>
<div id="4ba77d28-eddb-462a-97e6-848b755a236f" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>T[d,c,p] <span class="op">=</span> pr</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>T[d,c,d] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>pr</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Finally, the risky action at the degraded state guarantees a lock-in in the degraded state, <span class="math inline">\(T(\mathsf{d,r,d})=1\)</span>. Thus, the environment cannot recover from the degraded state under the risky action, <span class="math inline">\(T(\mathsf{d,r,p})=0\)</span>.</p>
<div id="ad9fc88d-e947-4e98-b193-d9bdc54cea27" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>T[d,r,p] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>T[d,r,d] <span class="op">=</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Last, we make sure that our transition tensor is normalized, i.e., the sum of all transition probabilities from a state-action pair to all possible next states equals one, <span class="math inline">\(\sum_{s'} T(s, a, s') = 1\)</span>.</p>
<div id="b6e9c9bc-949c-4e8f-894f-d0260b2c027a" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>T.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>array([[1, 1],
       [1, 1]], dtype=object)</code></pre>
</div>
</div>
<p>All together, the transition tensor looks as follows:</p>
<div id="b9c0ea9f-cbb5-4fb0-b711-8080f27c5627" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sp.Array(T)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="19">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}\left[\begin{matrix}1 &amp; 0\\1 - p_{c} &amp; p_{c}\end{matrix}\right] &amp; \left[\begin{matrix}p_{r} &amp; 1 - p_{r}\\0 &amp; 1\end{matrix}\right]\end{matrix}\right]\)</span></p>
</div>
</div>
<p>Recap | <strong>Substituting parameter values</strong>. In this chapter, we defined the transition and reward tensors as general <code>numpy</code> arrays with data types <code>object</code>, which we filled with symbolic expressions from <code>sympy</code>. To manipulate and substitute these expressions, we can use the <code>sympy.subs</code> method, however, not directly on the <code>numpy</code> array. Instead, we define a helper function <code>substitute_in_array</code> that takes a <code>numpy</code> array and a dictionary of substitutions and returns a new array with the substitutions applied.</p>
<div id="fbeaab24-3a40-4cc1-9fbc-f5f8cad9403f" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> substitute_in_array(array, subs_dict):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> array.copy()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index,_ <span class="kw">in</span> np.ndenumerate(array):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(array[index], sp.Basic):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>            result[index] <span class="op">=</span> array[index].subs(subs_dict)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>To make this work, it seems to be of critical importance that the subsitution dictionary is given as a dictionary in the form of <code>{&lt;symbol_variable&gt;: &lt;subsitution&gt;, ...}</code> and <em>not</em> as <code>dict(&lt;symbol_variable&gt;=&lt;subsitution&gt;, ...)</code>. For example,</p>
<div id="2df5b5a9-1dae-4710-89af-2d9bf08a4a8c" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>substitute_in_array(T, {pc: <span class="fl">0.1</span>, pr: <span class="fl">0.05</span>}).astype(<span class="bu">float</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>array([[[1.  , 0.  ],
        [0.9 , 0.1 ]],

       [[0.05, 0.95],
        [0.  , 1.  ]]])</code></pre>
</div>
</div>
<p>With the help of the <code>substitue_in_array</code> function we give the risk-reward dilemma class its environmental dynamics:</p>
<div id="cfee6178-7dd8-4fc5-af23-7e0d33505f97" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_TransitionTensor(<span class="va">self</span>):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create the transition tensor."""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> substitute_in_array(T, {pc: <span class="va">self</span>.pc, pr: <span class="va">self</span>.pr}).astype(<span class="bu">float</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>RiskRewardDilemma.create_TransitionTensor <span class="op">=</span> create_TransitionTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="rewards-short-term-welfare" class="level3">
<h3 class="anchored" data-anchor-id="rewards-short-term-welfare">Rewards | Short-term welfare</h3>
<p>The rewards or welfare the agent receives represent the ecosystem services the environment provides. It is modeled by three parameters: a safe reward <span class="math inline">\(r_s\)</span>, a risky reward <span class="math inline">\(r_r&gt;r_s\)</span>, and a degraded reward <span class="math inline">\(r_d&lt;r_s\)</span>. We assume the following default values,</p>
<div id="e139d38a-a741-4a41-8877-c89a25d8791d" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>rs, rr, rd <span class="op">=</span> sp.symbols(<span class="st">'r_s r_r r_d'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We implement the rewards as a four-dimensional array or <strong>tensor</strong>, with dimensions <span class="math inline">\(N \times Z \times M \times Z\)</span>, where <span class="math inline">\(N=1\)</span> is the number of agents, <span class="math inline">\(Z\)</span> is the number of states and <span class="math inline">\(M\)</span> is the number of actions. The additional agent dimension is necessary to accommodate multi-agent environments.</p>
<div id="96517c82-6fe9-4909-9ddc-96f0857e11e3" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.zeros((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">object</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The cautious action at the prosperous state guarantees the safe reward, <span class="math inline">\(R(\mathsf{p,c,p}) = r_s\)</span>,</p>
<div id="6cd4e91d-0d67-4888-832f-82a5912fea12" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">0</span>,p,c,p] <span class="op">=</span> rs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The risky action at the prosperous leads to the risky reward if the environment does not collapse, <span class="math inline">\(R(\mathsf{p,r,p}) = r_r\)</span>,</p>
<div id="0c3102cb-7f77-4342-9ea2-cb039afa28aa" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">0</span>,p,r,p] <span class="op">=</span> rr</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Yet, whenever the environment enters, remains, or leaves the degraded state, it provides only the degraded reward <span class="math inline">\(R(\mathsf{d,:,:}) = R(\mathsf{:,:,d}) = r_d\)</span>, where <span class="math inline">\(:\)</span> denotes all possible states and actions.</p>
<div id="36b85c98-1b56-4567-8277-b7809a828685" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">0</span>,d,:,:] <span class="op">=</span> R[<span class="dv">0</span>,:,:,d] <span class="op">=</span> rd</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Together, the reward tensor looks as follows:</p>
<div id="9b87915a-3c24-48f8-9598-c02dcaa86af0" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>sp.Array(R)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="28">
<p><span class="math inline">\(\displaystyle \left[\left[\begin{matrix}\left[\begin{matrix}r_{s} &amp; r_{d}\\r_{r} &amp; r_{d}\end{matrix}\right] &amp; \left[\begin{matrix}r_{d} &amp; r_{d}\\r_{d} &amp; r_{d}\end{matrix}\right]\end{matrix}\right]\right]\)</span></p>
</div>
</div>
<p>Again, we use the <code>substitute_in_array</code> function to give the risk-reward dilemma class its reward function:</p>
<div id="dedc243f-976f-4f16-8948-fbcbcd703ea3" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_RewardTensor(<span class="va">self</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create the reward tensor."""</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> substitute_in_array(</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        R, {rr: <span class="va">self</span>.rr, rs: <span class="va">self</span>.rs, rd: <span class="va">self</span>.rd}).astype(<span class="bu">float</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>RiskRewardDilemma.create_RewardTensor <span class="op">=</span> create_RewardTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="init-method" class="level3">
<h3 class="anchored" data-anchor-id="init-method">Init method</h3>
<div id="c9c60ae0-b81d-41f9-b08e-f1992cd1478d" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="30">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, CollapseProbability, RecoveryProbability, </span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>             RiskyReward, SafeReward, DegradedReward, state<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.N <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> <span class="va">self</span>.M <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> <span class="va">self</span>.Z <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.pc <span class="op">=</span> CollapseProbability</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.pr <span class="op">=</span> RecoveryProbability</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.rr <span class="op">=</span> RiskyReward</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.rs <span class="op">=</span> SafeReward</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.rd <span class="op">=</span> DegradedReward</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.StateSet <span class="op">=</span> <span class="va">self</span>.obtain_StateSet()</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ActionSets <span class="op">=</span> <span class="va">self</span>.obtain_ActionSets()</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.TransitionTensor <span class="op">=</span> <span class="va">self</span>.create_TransitionTensor()</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.RewardTensor <span class="op">=</span> <span class="va">self</span>.create_RewardTensor()</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.state <span class="op">=</span> state</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>RiskRewardDilemma.<span class="fu">__init__</span> <span class="op">=</span> <span class="fu">__init__</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Basic testing</p>
<div id="7422f8f1-e177-4c86-8a86-e86edac94206" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="31">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(<span class="fl">0.11</span>, <span class="fl">0.4</span>, <span class="fl">1.0</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>env.TransitionTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>array([[[1.  , 0.  ],
        [0.89, 0.11]],

       [[0.4 , 0.6 ],
        [0.  , 1.  ]]])</code></pre>
</div>
</div>
<div id="45b893ef-ba65-439d-b086-60df80a99587" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]" data-execution_count="32">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>env.RewardTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>array([[[[0.8, 0. ],
         [1. , 0. ]],

        [[0. , 0. ],
         [0. , 0. ]]]])</code></pre>
</div>
</div>
<div id="02e1ad57-b7c4-4fa3-936d-47eca132ccd1" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]" data-execution_count="33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>env.step([<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>([0], array([1.]), {'state': 0})</code></pre>
</div>
</div>
</section>
<section id="testing-the-interface" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-interface">Testing the interface</h3>
<div id="e5a55bcc-1831-4b33-aebd-b3d92e81f928" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> BehaviorAgent(policy<span class="op">=</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)))</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(<span class="fl">0.2</span>, <span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="fl">0.8</span>, <span class="fl">0.0</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>interface_run([agent], env, <span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Obviously, this is not very insightful. We need to track the learning process.</p>
<p>We need to track the learning process. We can do this by storing the actions, observations, and rewards in a <code>pandas</code> DataFrame. Pandas is a powerful data manipulation library in Python that provides data structures and functions to work with structured data. We will store the data of each time step into a row and its attributes into a set of respective columns of the DataFrame.</p>
<div id="77e375fb-aa13-4f1b-b809-9d9167fcac2c" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interface_run(agent, env, NrOfTimesteps):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run the multi-agent environment for several time steps."""</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> [<span class="st">"action"</span>, <span class="st">"observation"</span>, <span class="st">"reward"</span>]</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(index<span class="op">=</span><span class="bu">range</span>(NrOfTimesteps), columns<span class="op">=</span>columns)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    observations <span class="op">=</span> env.observe()</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(NrOfTimesteps):</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> agent.act(observations[<span class="dv">0</span>])</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>        next_observations, rewards, info <span class="op">=</span> env.step([action])</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        agent.update(observations[<span class="dv">0</span>], action, </span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>                     rewards[<span class="dv">0</span>], next_observations[<span class="dv">0</span>])</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>        df.loc[t] <span class="op">=</span> (action, observations[<span class="dv">0</span>], rewards[<span class="dv">0</span>])</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>        observations <span class="op">=</span> next_observations</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="72849efd-5d8d-4cc2-ba4b-b30ad1cdf43e" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(agent, env, <span class="dv">25</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>df.tail()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">action</th>
<th data-quarto-table-cell-role="th">observation</th>
<th data-quarto-table-cell-role="th">reward</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">20</th>
<td>0</td>
<td>0</td>
<td>0.8</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">21</th>
<td>0</td>
<td>0</td>
<td>0.8</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">22</th>
<td>0</td>
<td>0</td>
<td>0.8</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">23</th>
<td>1</td>
<td>0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">24</th>
<td>1</td>
<td>0</td>
<td>0.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="104ced3d-4079-4cd8-b2f8-e15ba81dc619" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_ActionsRewardsObservations(df):</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].plot(df.action, <span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Agent 0'</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Action'</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_yticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].set_yticklabels([env.ActionSets[<span class="dv">0</span>][<span class="dv">0</span>], env.ActionSets[<span class="dv">0</span>][<span class="dv">1</span>]])</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].plot(df.reward, <span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Agent 0'</span>)<span class="op">;</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Reward'</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].plot(df.observation, <span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Agent 0'</span>)<span class="op">;</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_ylabel(<span class="st">'Observation'</span>)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_yticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].set_yticklabels([env.StateSet[<span class="dv">0</span>], env.StateSet[<span class="dv">1</span>]])<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="8768d2b5-0717-4f3c-b3be-d9176279ce43" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(agent, env, <span class="dv">25</span>)<span class="op">;</span> plot_ActionsRewardsObservations(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-34-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Action-Reward-Observation Dynamics</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="reinforcement-learning-agent" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="reinforcement-learning-agent"><span class="header-section-number">10.4</span> Reinforcement learning agent</h2>
<p>Agents act (oftentimes to reach a goal). We need to specify their</p>
<ul>
<li><strong>Actions</strong>, describing which choices are available to the agent.</li>
<li><strong>Goal</strong>, describing what an agent wants (in the long run). They may contain <em>intrinsic motivations</em>.</li>
<li><strong>Representation</strong>, e.g., defining upon which conditions agents select actions (e.g., <em>history</em> of past actions in multi-agent situations).</li>
<li><strong>Value beliefs</strong> (value functions), capturing what is <em>good</em> for the agent regarding its <em>goal</em> in the long run.</li>
<li><strong>Behavioral rule</strong> (policy, strategy), defining how to select actions.</li>
<li><strong>Learning rule</strong>, describing how value beliefs are updated in light of new information.</li>
<li>(optionally), a <strong>model</strong> of the environment and rewards. Models are used for <em>planning</em>, i.e., deciding on a <em>behavioral rule</em> by considering possible future situations before they are actually experienced.</li>
</ul>
<p>Agents act (oftentimes to reach a goal). As in Lecture <a href="./03.01-SequentialDecisions.html">03.01-SequentialDecision</a>, the agent aims to maximize the discounted sum of future rewards,</p>
<p><span class="math display">\[ G_t = (1-\gamma) \sum_{\tau=0}^\infty \gamma^\tau R_{t+\tau},\]</span></p>
<p>where <span class="math inline">\(1-\gamma\)</span> is a normalizing factor and <span class="math inline">\(R_{t+\tau+1}\)</span> is the reward received at time step <span class="math inline">\(t+\tau+1\)</span>.</p>
<p>However, in contrast to Lecture <a href="./03.01-SequentialDecisions.html">03.01-SequentialDecision</a>, we assume that the agent does <strong>not know the environment’s dynamics and rewards</strong>. Instead, the agent <strong>learns</strong> about the environment <strong>while interacting</strong> with it.</p>
<p>The challenge is that actions may have <strong>delayed</strong> and <strong>uncertain</strong> consequences.</p>
<p><strong>Delayed</strong> consequences mean that an action may influence the environmental state, which, in turn, influences the reward the agent receives at a later time step. For example, in our risk-reward dilemma, opting for a sustainable policy may initially reduce the agent’s immediate reward but ensures a comparably higher long-term welfare. <strong>Uncertain</strong> consequences refer to the <strong>stochasticity</strong> in the environmental transitions (and possibly the reward signals themselves). For example, in our risk-reward dilemma, the risky action in the prosperous state may lead to a high reward but may also cause a transition to the degraded state. Moreover, uncertainty may also refer to the fact that the environmental transition dynamics may change over time.</p>
<p>Thus, the agent can’t just try each action in each state once and then immediately know which course of action is best. It <strong>must learn</strong> the best course of action <strong>over successive trials</strong>, each of which gives possible noisy data.</p>
<p>We start implement the learning class by defining <code>__init__</code> method.</p>
<div id="7646eacf-82c3-4d61-8acf-e832463b1638" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Learner():</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A simple reinforcement learning agent."""</span>   </span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ValueBeliefs_Qoa,</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>                 DiscountFactor, LearningRate, ChoiceIntensity):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.DiscountFactor <span class="op">=</span> <span class="va">self</span>.df <span class="op">=</span> DiscountFactor</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LearningRate <span class="op">=</span> <span class="va">self</span>.lr <span class="op">=</span> LearningRate</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ChoiceIntensity <span class="op">=</span> <span class="va">self</span>.ci <span class="op">=</span> ChoiceIntensity</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ValueBeliefs_Qoa <span class="op">=</span> ValueBeliefs_Qoa</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ActionIxs <span class="op">=</span> <span class="bu">range</span>(ValueBeliefs_Qoa.shape[<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The agent receives the following parameters: the initial value beliefs <code>ValueBeliefs_Qoa</code>, the discount factor <code>DiscountFactor</code>, the learning rate <code>LearningRate</code>, and the choice intensity <code>ChoiceIntensity</code>. Furthermore, we give the agent an attribute <code>ActionIxs</code> that stores the indices of the possible actions. This will be helpful when selecting actions.</p>
<section id="value-beliefs" class="level3">
<h3 class="anchored" data-anchor-id="value-beliefs">Value beliefs</h3>
<p>The general strategy we focus on to solve the challenges of delayed and uncertain consequences is to let the agent <strong>learn value beliefs</strong>. Value beliefs are the agent’s estimates of the long-term value of each action <span class="math inline">\(a\)</span> in each state <span class="math inline">\(s\)</span>. The agent then uses these value beliefs to select actions. These estimates are also often called Q values. You may think of the <em>quality</em> of an action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> which tells the agents Which action to select in which state.</p>
<p>For example, in our risk-reward dilemma, we can represent the agent’s value beliefs by</p>
<div id="2a96ac39-4927-422f-8412-5be4bcd45faa" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">10</span> <span class="op">*</span> np.random.rand(<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>ValueBeliefs_Qoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>array([[8.83140215, 5.58254363],
       [8.59330674, 1.20765508]])</code></pre>
</div>
</div>
<p>The challenge of uncertain consequences in then solved by an appropriate <strong>behavioral rule</strong> which handles the so-called <strong>exploration-exploitation trade-off</strong></p>
<p>The challenge of delayed consequences is solved by the <strong>learning rule</strong>, which updates the value beliefs in light of new information using the Bellman equation, as in Lecture <a href="./03.01-SequentialDecisions.html">03.01-SequentialDecisions</a>.</p>
</section>
<section id="behavioral-rule-exploration-exploitation-trade-off" class="level3">
<h3 class="anchored" data-anchor-id="behavioral-rule-exploration-exploitation-trade-off">Behavioral rule | Exploration-exploitation trade-off</h3>
<p>The <strong>exploration-exploitation trade-off</strong> poses <strong>a fundamental problem for decision-making under uncertainty</strong>.</p>
<p><strong>Under too much exploitation</strong>, the agent may pick an action that is not optimal, as it has not yet sufficiently explored all possible actions. It acts under the false belief that its current value beliefs are already correct or optimal. Thus, it <em>loses out</em> on possible rewards it would have gotten if it had explored more and discovered that a different course of action is better.</p>
<p><strong>Under too much exploration</strong>, the agent may continue to try all actions to gain as much information about the transitions and reward distributions as possible. It is <em>losing out</em> because it never settles on the best course of action, continuing to pick all actions until the end.</p>
<p>What is needed is a <strong>behavioral rule</strong> that <strong>balances exploitation and exploration</strong> to explore enough to find the best option but not too much so that the best option is exploited as much as possible.</p>
<p>We use the so-called <em>softmax</em> function,</p>
<p><span class="math display">\[x(s, a) = \frac{\exp \beta Q(s, a)}{\sum_{b \in \mathcal A}\exp \beta Q(s, b)},\]</span></p>
<p>which converts any set of value beliefs into probabilities that sum to one.</p>
<p>The higher the relative value belief, the higher the relative probability.</p>
<div id="6d1f4c61-8c1b-4699-9d1b-ab57adc39662" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obtain_softmax_probabilities(ChoiceIntensity, ValueBeliefs):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    expValueBeliefs <span class="op">=</span> np.exp(ChoiceIntensity<span class="op">*</span>np.array(ValueBeliefs))</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> expValueBeliefs <span class="op">/</span> expValueBeliefs.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The softmax function contains a parameter <span class="math inline">\(\beta\)</span>, denoting the <strong>choice intensity</strong> (sometimes called <em>inverse temperature</em>, that determines how <strong>exploitative</strong> (or <em>greedy</em>) the agent is.</p>
<p>When <span class="math inline">\(\beta = 0\)</span>, arms are chosen entirely at random with no influence of the Q values. This is super exploratory, as the agent continues to choose all arms irrespective of observed rewards.</p>
<div id="eeb255b6-2d92-4fb0-9c37-a40604e5890b" class="cell" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>obtain_softmax_probabilities(<span class="dv">0</span>, ValueBeliefs_Qoa)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>array([[0.5, 0.5],
       [0.5, 0.5]])</code></pre>
</div>
</div>
<p>As <span class="math inline">\(\beta\)</span> increases, there is a higher probability of picking the arm with the highest Q value. This is increasingly exploitative (or ‘greedy’).</p>
<div id="93b31875-4789-41fe-9739-31f5e0bae4e1" class="cell" data-execution_count="43">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>obtain_softmax_probabilities(<span class="dv">1</span>, ValueBeliefs_Qoa)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>array([[9.62632074e-01, 3.73679265e-02],
       [9.99380298e-01, 6.19702179e-04]])</code></pre>
</div>
</div>
<p>When <span class="math inline">\(\beta\)</span> is very large, then only the arm that currently has the highest Q value will be chosen, even if other arms might actually be better.</p>
<div id="1789e7d3-c6b9-4622-ab58-c8df207c1725" class="cell" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>obtain_softmax_probabilities(<span class="dv">50</span>, ValueBeliefs_Qoa).<span class="bu">round</span>(<span class="dv">9</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>array([[1., 0.],
       [1., 0.]])</code></pre>
</div>
</div>
<p>For example, assuming we are in state zero and using <span class="math inline">\(\beta=1\)</span>, we can select an action by</p>
<div id="91a14375-db5a-4668-b6d6-2787614d777f" class="cell" data-execution_count="45">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>obs <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>Xoa <span class="op">=</span> obtain_softmax_probabilities(<span class="dv">1</span>, ValueBeliefs_Qoa)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>np.random.choice([<span class="dv">0</span>,<span class="dv">1</span>], p<span class="op">=</span>Xoa[obs])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>0</code></pre>
</div>
</div>
<p>We summarize this logic in the agent’s <code>act</code> method:</p>
<div id="3e06bb11-b30c-4a5e-994a-75b557e14d26" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> act(<span class="va">self</span>, obs):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    Xoa <span class="op">=</span> <span class="va">self</span>.obtain_policy_Xoa()</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.choice(<span class="va">self</span>.ActionIxs, p<span class="op">=</span>Xoa[obs])</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>Learner.act <span class="op">=</span> act</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>where we define the <code>ActionIxs</code> as <code>range(self.NrActions)</code> in the <code>__init__</code> method of the agent. We also define the <code>obtain_policy_Xoa</code> method in the agent class:</p>
<div id="dc77c6cb-624d-4dc4-9431-72c429b8f5a9" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obtain_policy_Xoa(<span class="va">self</span>):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> obtain_softmax_probabilities(<span class="va">self</span>.ChoiceIntensity, <span class="va">self</span>.ValueBeliefs_Qoa)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>Learner.obtain_policy_Xoa <span class="op">=</span> obtain_policy_Xoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Testing</strong> the <code>act</code> and <code>obtain_policy_Xoa</code> methods:</p>
<div id="d2e1edac-9e3a-46d6-bc96-5a5726eb3082" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), <span class="fl">0.9</span>, <span class="fl">0.1</span>, <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="fd668d73-6f25-40d9-a552-32cc5befa661" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>learner.obtain_policy_Xoa()[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>array([0.5, 0.5])</code></pre>
</div>
</div>
<p>Selecting action uniformly at random for 1000 times should give a mean index of approx. 0.5:</p>
<div id="d3cb4ee2-f6f3-43a5-bb5a-f361beaadcd2" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>np.mean([learner.act(<span class="dv">0</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>0.479</code></pre>
</div>
</div>
</section>
<section id="learning-rule-temporal-difference-learning" class="level3">
<h3 class="anchored" data-anchor-id="learning-rule-temporal-difference-learning">Learning rule | Temporal-difference learning</h3>
<p>The learning rule solves the challenge of delayed consequences. The value beliefs are updated using the Bellman equation in light of new information. As the Bellman equation describes how state(-action) values relate at different timesteps, this reinforcement learning update class is called <strong>temporal-difference learning</strong>.</p>
<p>Given an observed state <span class="math inline">\(s\)</span>, the agent selects an action <span class="math inline">\(a\)</span> and receives a reward <span class="math inline">\(r\)</span>. Then, the agent updates its value beliefs (for state-action pair <span class="math inline">\(s\)</span>-<span class="math inline">\(a\)</span>) according to</p>
<p><span id="eq-RLupdate"><span class="math display">\[Q_{t+1}(s, a) = Q_{t}(s, a) + \alpha\left((1-\gamma)r + \gamma \sum_b x_t(s', b) Q_t(s',b) - Q_{t}(s, a)
\right). \tag{10.1}\]</span></span></p>
<p><strong>DeepDive</strong> | There is some freedom into designing the specifics of the temporal-difference update, especially regarding estimating the value of the next state or observation. The specific update used above is called <strong>Expected SARSA</strong>. It is beyond the scope of this course to discuss the different temporal-difference learning algorithms. The interested reader is referred to excellent material on (multi-agent) reinforcement learning, e.g., <span class="citation" data-cites="SuttonBarto2018 AlbrechtEtAl2024">(<a href="References.html#ref-AlbrechtEtAl2024" role="doc-biblioref">Albrecht et al., 2024</a>; <a href="References.html#ref-SuttonBarto2018" role="doc-biblioref">Sutton &amp; Barto, 2018</a>)</span>.</p>
<p>The extent to which the value beliefs are updated is controlled by a second parameter, <span class="math inline">\(\alpha \in (0, 1)\)</span>, called the <strong>learning rate</strong>.</p>
<p>When <span class="math inline">\(\alpha=0\)</span>, there is no updating, and the reward does not affect value beliefs,</p>
<p><span class="math display">\[Q_{t+1}(s, a) = Q_{t}(s, a) =: \text{old estimate}.\]</span></p>
<p>The value belief update always remains the <em>old estimate</em> of the value beliefs.</p>
<p>When <span class="math inline">\(\alpha = 1\)</span>, the value belief for the state-action pair (<span class="math inline">\(s, a\)</span>) becomes a discount-factor weighted average between the current reward <span class="math inline">\(r\)</span> and the expected value of the next state <span class="math inline">\(\sum_b x_t(s', b) Q_t(s',b)\)</span>,</p>
<p><span class="math display">\[Q_{t+1}(s, a) = (1-\gamma)r + \gamma \sum_b x_t(s', b) Q_t(s',b) =: \text{new estimate}.\]</span></p>
<p>The value belief update is entirely determined by the <em>new estimate</em> of the value beliefs, which is the current reward received <span class="math inline">\(r\)</span> plus the discount factor <span class="math inline">\(\gamma\)</span> multiplied by the expected value of the following state <span class="math inline">\(\sum_b x_t(s', b) Q_t(s',b)\)</span>, and adequately normalized with the prefactor <span class="math inline">\((1-\gamma)\)</span>.</p>
<p>When <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>, the new value belief for the rewarded arm is a <em>weighted average</em> between old value belief and new reward information,</p>
<p><span class="math display">\[\begin{align}
Q_{t+1}(s, a) &amp;= (1-\alpha) \ \text{old estimate} &amp;+ &amp;\alpha \ \text{new estimate}\\
&amp;= (1-\alpha) \ Q_{t}(s, a) &amp;+ &amp;\alpha \left((1-\gamma)r + \gamma \sum_b x_t(s', b) Q_t(s',b)\right).
\end{align}\]</span></p>
<p>Once more, we face a trade-off. Clearly, setting <span class="math inline">\(\alpha = 0\)</span> is ineffective since the agent does not acquire knowledge. Yet, if <span class="math inline">\(\alpha\)</span> is excessively high; the agent tends to <em>forget</em> previously learned state-action information.</p>
<p><strong>Temporal-difference reward-prediction error</strong>. Another way to think about the update equation (<a href="#eq-RLupdate" class="quarto-xref">Equation&nbsp;<span>10.1</span></a>) is as follows: The value beliefs are updated by the <em>temporal-difference reward-prediction error</em> (TDRP error) times the learning rate. The TDRP error equals the difference between the <em>new estimate</em> and the <em>old estimate</em> of the value beliefs. If the TDRP error is zero, the agent correctly predicted the next reward, and thus, no further adjustments in the value beliefs are necessary.</p>
<p><span class="math display">\[\begin{align}
Q_{t+1}(s, a) &amp;= Q_{t}(s, a) + \alpha \qquad\qquad\qquad\qquad\qquad \text{TDRP-error}, \\
&amp;= Q_{t}(s, a) + \alpha \Big( \qquad \qquad   \text{new estimate} \qquad \qquad \ - \text{old estimate} \Big), \\
&amp;= Q_{t}(s, a) + \alpha \Big( (1-\gamma)r + \gamma \sum_b x_t(s', b) Q_t(s',b) - \quad Q_{t}(s, a) \quad \Big)
\end{align}\]</span></p>
<p><strong>DeepDive</strong> | The exact terminology <em>temporal-difference reward-prediction error</em> is used rather rarely. We use it here to express the interdisciplinary nature of temporal-difference reward-prediction learning. In machine learning, the term <em>temporal-difference error</em> is more common. It describes the difference between the predicted and the observed reward. In psychology and neuroscience, the term <em>reward-prediction error</em> is used in the context of the brain’s dopamine system, where it is thought to signal the difference between the expected and the observed reward. The term <em>temporal-difference reward-prediction error</em> combines both terms, expressing the idea that the agent learns by predicting future rewards.</p>
<p>Executing the value belief update in Python may look like</p>
<div id="fd578128-94b6-478f-883e-58bf5d6fe775" class="cell" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(<span class="va">self</span>, </span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>           obs: <span class="bu">int</span>,</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>           action: <span class="bu">int</span>,</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>           reward: <span class="bu">float</span>,</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>           next_obs: <span class="bu">int</span>,</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>          ):</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Updates the value beliefs / Q-value of an action."""</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    temporal_difference <span class="op">=</span><span class="va">self</span>.obtain_temporal_difference(</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        obs, action, reward, next_obs)</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ValueBeliefs_Qoa[obs, action] <span class="op">=</span> (</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ValueBeliefs_Qoa[obs, action] <span class="op">+</span> <span class="va">self</span>.LearningRate <span class="op">*</span> temporal_difference</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>Learner.update <span class="op">=</span> update</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We program the <code>update</code> method highly modular. It calls the <code>obtain_temporal_difference</code> method to compute the temporal-difference error and then updates the value beliefs accordingly. The <code>obtain_temporal_difference</code> method is defined as follows:</p>
<div id="6712f914-c619-4dc4-86d6-f8e2a15ba341" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obtain_temporal_difference(<span class="va">self</span>,</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>                               obs: <span class="bu">int</span>,</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>                               action: <span class="bu">int</span>,</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>                               reward: <span class="bu">float</span>,</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>                               next_obs: <span class="bu">int</span>,</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>                              ):</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute temporal-difference eorror"""</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    next_Qoa <span class="op">=</span> <span class="va">self</span>.obtain_nextQoa(next_obs)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    new_estimate <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.DiscountFactor) <span class="op">*</span> reward <span class="op">+</span> <span class="va">self</span>.DiscountFactor <span class="op">*</span> next_Qoa</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    old_estimate <span class="op">=</span> <span class="va">self</span>.ValueBeliefs_Qoa[obs][action]</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_estimate <span class="op">-</span> old_estimate</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>Learner.obtain_temporal_difference <span class="op">=</span> obtain_temporal_difference</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In here, we call the <code>obtain_nextQoa</code> method to compute the expected value of the next state. The <code>obtain_nextQoa</code> method is defined as follows:</p>
<div id="fe97ba2f-854d-4dee-875a-2e11eeeb8adf" class="cell" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obtain_nextQoa(<span class="va">self</span>, next_obs: <span class="bu">int</span>):</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    policy_Xoa <span class="op">=</span> <span class="va">self</span>.obtain_policy_Xoa()</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(policy_Xoa[next_obs] <span class="op">*</span> <span class="va">self</span>.ValueBeliefs_Qoa[next_obs])</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>Learner.obtain_nextQoa <span class="op">=</span> obtain_nextQoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Testing the update method</strong>: First, let’s assume the agent does not care about future rewards at all and has a discount factor of zero</p>
<div id="d6331c65-278e-457d-9be7-006ec5564b28" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.0</span>,</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="e46f0627-c6eb-4756-a70d-83bee7a24860" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="55">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>array([[1., 1.],
       [1., 1.]])</code></pre>
</div>
</div>
<p>Let’s assume the agent selected the action with index <code>0</code> after observing the state with index <code>0</code>, received a reward of zero, and observed the next state with index <code>1</code>.</p>
<div id="5ba1754f-1e80-463d-8f05-b17c80abe452" class="cell" data-execution_count="56">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>learner.update(obs<span class="op">=</span><span class="dv">0</span>, action<span class="op">=</span><span class="dv">0</span>, reward<span class="op">=</span><span class="dv">0</span>, next_obs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa.<span class="bu">round</span>(<span class="dv">4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>array([[0.9, 1. ],
       [1. , 1. ]])</code></pre>
</div>
</div>
<p>The value belief for the action <code>0</code> in state <code>0</code> is updated exactly as a learning rate weighted average: <span class="math inline">\(\alpha \cdot \text{new estimate} + (1-\alpha) \cdot \text{old estimate}\)</span> <span class="math inline">\(= \alpha 0 + (1-\alpha) 1\)</span> <span class="math inline">\(= 0.1 \cdot 0 + 0.9 \cdot 1\)</span>.</p>
<p>Repeating this update a hundred more time steps updates the value beliefs for the action <code>0</code> in state <code>0</code> to the expected value of zero.</p>
<div id="bcb52359-f172-45ed-9508-e33d63623135" class="cell" data-execution_count="57">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): learner.update(obs<span class="op">=</span><span class="dv">0</span>, action<span class="op">=</span><span class="dv">0</span>, reward<span class="op">=</span><span class="dv">0</span>, next_obs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa.<span class="bu">round</span>(<span class="dv">4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>array([[0., 1.],
       [1., 1.]])</code></pre>
</div>
</div>
<p>Now, we repeat that test, but with an agent with a discount factor of <span class="math inline">\(\gamma=0.8\)</span>.</p>
<div id="dad85c87-aba5-4ab8-b757-f0ef3103b040" class="cell" data-execution_count="58">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">1</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.8</span>,</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="2cc50e0b-c475-408a-bed3-1a72cbb86f42" class="cell" data-execution_count="59">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>array([[1., 1.],
       [1., 1.]])</code></pre>
</div>
</div>
<div id="af1e976c-58a5-4b2f-a478-e4ef6b216b2e" class="cell" data-execution_count="60">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): learner.update(obs<span class="op">=</span><span class="dv">0</span>, action<span class="op">=</span><span class="dv">0</span>, reward<span class="op">=</span><span class="dv">0</span>, next_obs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>learner.ValueBeliefs_Qoa.<span class="bu">round</span>(<span class="dv">4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>array([[0.8, 1. ],
       [1. , 1. ]])</code></pre>
</div>
</div>
<p>Now, the value belief for the action <code>0</code> in state <code>0</code> is updated to <span class="math inline">\(0.8\)</span>. Can you explain why?</p>
<p>We have convince ourselves that the learner’s <code>update</code> methods works as we expect.</p>
<p>Now, we are ready to let it learn in the risk-reward dilemma environment.</p>
</section>
<section id="testing-the-interface-1" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-interface-1">Testing the interface</h3>
<div id="f77d911b-884e-483a-b156-437b66bf2091" class="cell" data-execution_count="61">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(learner.obtain_policy_Xoa())</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" - - - - "</span>)</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" - - - - "</span>)</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(learner.obtain_policy_Xoa())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.5 0.5]
 [0.5 0.5]]
 - - - - 
 - - - - 
[[0.5091271  0.4908729 ]
 [0.50860671 0.49139329]]</code></pre>
</div>
</div>
<p>The learning agent’s policy changed. However, not too much. We know from Lecture <a href="./03.01-SequentialDecisions.html">03.01-SequentialDecisions</a> that the agent should learn to prefer the cautious action in both states under these parameter settings.</p>
<p><strong>Try re-executing the above cell while make some changes to the parameters. Can you get an intuition what is important for a successful learning process?</strong></p>
<p>The process of finding the right parameters for the agent is called <strong>hyperparameter tuning</strong>. It is a crucial step in machine learning and often requires a lot of trial and error.</p>
<p><strong>From a modeling point of view</strong>, we aim to go beyond finding the <em>right</em> parameter. We aim to <strong>understand</strong> how the parameters influence the learning process.</p>
<p>Comparing the initial with the final policy is not the best way to facilitate both aims. We need a more refined way to keep track of the learning process.</p>
</section>
</section>
<section id="investigating-the-learning-process" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="investigating-the-learning-process"><span class="header-section-number">10.5</span> Investigating the learning process</h2>
<p>To keep track of the learning process, we store the value beliefs and the policy in a <code>pandas</code> DataFrame. Aditionally, we also record the learning rate and the choice intensity.</p>
<div id="1f4e3e5d-671e-40a2-bf07-32224adc0645" class="cell" data-execution_count="62">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interface_run(agent, env, NrOfTimesteps):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run the multi-agent environment for several time steps."""</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> [<span class="st">"action"</span>, <span class="st">"observation"</span>, <span class="st">"reward"</span>, <span class="st">"beliefs"</span>, <span class="st">"policy"</span>,</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>               <span class="st">"ChoiceIntensity"</span>, <span class="st">"LearningRate"</span>]</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(index<span class="op">=</span><span class="bu">range</span>(NrOfTimesteps), columns<span class="op">=</span>columns)</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>    observations <span class="op">=</span> env.observe()</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(NrOfTimesteps):</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> agent.act(observations[<span class="dv">0</span>])</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>        next_observations, rewards, info <span class="op">=</span> env.step([action])</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>        agent.update(observations[<span class="dv">0</span>], action, </span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>                     rewards[<span class="dv">0</span>], next_observations[<span class="dv">0</span>])</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a>        df.loc[t] <span class="op">=</span> (action, next_observations[<span class="dv">0</span>], rewards[<span class="dv">0</span>],</span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a>                     deepcopy(agent.ValueBeliefs_Qoa), </span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a>                     deepcopy(agent.obtain_policy_Xoa()),</span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a>                     deepcopy(agent.ChoiceIntensity), </span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>                     deepcopy(agent.LearningRate))</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>        observations <span class="op">=</span> next_observations</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="922a327a-845c-4d92-a098-0e083b322b1d" class="cell" data-execution_count="63">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">8.0</span>)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(learner.obtain_policy_Xoa())</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.5 0.5]
 [0.5 0.5]]</code></pre>
</div>
</div>
<p>As we stored the value beliefs and policies as two-dimensional <code>numpy</code> arrays, we convert them into three-dimensional <code>numpy</code> with time running on the first dimension:</p>
<div id="70f6e4a4-afbd-4b29-8247-2864c1304aae" class="cell" data-execution_count="64">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>beliefs_Qtoa <span class="op">=</span> np.array(df.beliefs.values.tolist())</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>policy_Xtoa <span class="op">=</span> np.array(df.policy.values.tolist())</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>beliefs_Qtoa.shape, policy_Xtoa.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>((10000, 2, 2), (10000, 2, 2))</code></pre>
</div>
</div>
<p>We include these conversions into a plotting function that visualizes the learning process</p>
<div id="e66c62d5-b1a6-4caa-b913-b1810c0a457c" class="cell" data-execution_count="65">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_learning_process(df, plot_varying_parameters<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    beliefs_Qtoa <span class="op">=</span> np.array(df.beliefs.values.tolist())</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    policy_Xtoa <span class="op">=</span> np.array(df.policy.values.tolist())</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    beliefs_Qtoa.shape, policy_Xtoa.shape</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>,<span class="dv">6</span>))</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>    ax0 <span class="op">=</span> fig.add_subplot(<span class="dv">311</span>)</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs_Qtoa[:,p,c], label<span class="op">=</span><span class="st">'Q(p,c)'</span>, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs_Qtoa[:,p,r], label<span class="op">=</span><span class="st">'Q(p,r)'</span>, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs_Qtoa[:,d,c], label<span class="op">=</span><span class="st">'Q(d,c)'</span>, color<span class="op">=</span><span class="st">'darkblue'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs_Qtoa[:,d,r], label<span class="op">=</span><span class="st">'Q(d,r)'</span>, color<span class="op">=</span><span class="st">'darkred'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>    ax0.set_ylabel(<span class="st">'Value beliefs'</span>)<span class="op">;</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>    ax0.legend(loc<span class="op">=</span><span class="st">'center right'</span>)<span class="op">;</span> ax0.set_xlim(<span class="op">-</span><span class="dv">10</span>, <span class="bu">len</span>(df)<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>    ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">312</span>, sharex<span class="op">=</span>ax0)</span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy_Xtoa[:,p,c], label<span class="op">=</span><span class="st">'X(p,c)'</span>, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy_Xtoa[:,p,r], label<span class="op">=</span><span class="st">'X(p,r)'</span>, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy_Xtoa[:,d,c], label<span class="op">=</span><span class="st">'X(d,c)'</span>, color<span class="op">=</span><span class="st">'darkblue'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy_Xtoa[:,d,r], label<span class="op">=</span><span class="st">'X(d,r)'</span>, color<span class="op">=</span><span class="st">'darkred'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">'Policy'</span>)<span class="op">;</span> ax1.set_xlabel(<span class="st">'Time steps'</span>)</span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>    ax1.legend(loc<span class="op">=</span><span class="st">'center right'</span>)</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> plot_varying_parameters:</span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>        ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">615</span>, sharex<span class="op">=</span>ax0)</span>
<span id="cb81-26"><a href="#cb81-26" aria-hidden="true" tabindex="-1"></a>        ax2.plot(df.LearningRate, label<span class="op">=</span><span class="st">'LearningRate'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb81-27"><a href="#cb81-27" aria-hidden="true" tabindex="-1"></a>        ax2.set_ylabel(<span class="st">"Learning</span><span class="ch">\n</span><span class="st">Rate"</span>)</span>
<span id="cb81-28"><a href="#cb81-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-29"><a href="#cb81-29" aria-hidden="true" tabindex="-1"></a>        ax3 <span class="op">=</span> fig.add_subplot(<span class="dv">616</span>, sharex<span class="op">=</span>ax0)</span>
<span id="cb81-30"><a href="#cb81-30" aria-hidden="true" tabindex="-1"></a>        ax3.plot(df.ChoiceIntensity, label<span class="op">=</span><span class="st">'ChoiceIntensity'</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb81-31"><a href="#cb81-31" aria-hidden="true" tabindex="-1"></a>        ax3.set_ylabel(<span class="st">"Choice</span><span class="ch">\n</span><span class="st">Intensity"</span>), ax3.set_xlabel(<span class="st">'Time steps'</span>)</span>
<span id="cb81-32"><a href="#cb81-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb81-33"><a href="#cb81-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plt.tight_layout()</span></span>
<span id="cb81-34"><a href="#cb81-34" aria-hidden="true" tabindex="-1"></a>    plt.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.35</span>)</span>
<span id="cb81-35"><a href="#cb81-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb81-36"><a href="#cb81-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plt.legend()</span></span>
<span id="cb81-37"><a href="#cb81-37" aria-hidden="true" tabindex="-1"></a>    ax0.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="to-little-exploitation-to-much-exploration" class="level3">
<h3 class="anchored" data-anchor-id="to-little-exploitation-to-much-exploration">To little exploitation | To much exploration</h3>
<div id="dead569c-05cb-4a80-8448-7199266d4312" class="cell" data-execution_count="66">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>)</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="3f4d6a51-02fa-403a-bfcd-ce6cca4d90d0" class="cell" data-execution_count="67">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>plot_learning_process(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-63-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning the risk-reward dilemma with to much exploration.</figcaption>
</figure>
</div>
</div>
</div>
<p>The order of the <strong>value beliefs</strong> seems roughly <strong>consistent with</strong> the <strong>optimal policy</strong>, that prefers the cautious action over the risky on in both states. However, the <strong>agents policy is fluctuating around</strong>. The action choice probabilities are fluctuating around their uniformly random value of 0.5. This is a sign that the agent explores too much and exploits too little.</p>
</section>
<section id="to-much-exploitation-to-little-exploration" class="level3">
<h3 class="anchored" data-anchor-id="to-much-exploitation-to-little-exploration">To much exploitation | To little exploration</h3>
<div id="376f0b88-e10e-4946-9aa4-cf7447b4049a" class="cell" data-execution_count="68">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">100.0</span>)</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="16c7137a-7e98-4962-a2cf-d5e422b70127" class="cell" data-execution_count="69">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>plot_learning_process(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-65-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning the risk-reward dilemma with to much exploitation.</figcaption>
</figure>
</div>
</div>
</div>
<p>Increasing the choice intensity leads to a more exploitative policy. However, what the agent learns depends on the stochasticity of the learning process. <strong>Try to convince yourself of that fact by re-executing the above cell multiple times.</strong></p>
<p>In other words, the agent may not learn the optimal policy. To little exploration harms the learning.</p>
</section>
<section id="decaying-exploration-increasing-exploitation" class="level3">
<h3 class="anchored" data-anchor-id="decaying-exploration-increasing-exploitation">Decaying exploration | Increasing exploitation</h3>
<p>To mitigate the negative effects of too much exploration towards the end of the learning process and the negative effects of too much exploitation towards the beginning of the learning process, we could let the choice intensity increase over time. This is called <strong>decaying exploration</strong>.</p>
<p>We implement this idea by creating a new agent class <code>AdjustingLearner</code> that inherits from the <code>Learner</code> class. This shows the power of object-oriented programming. We overwrite the <code>update</code> method to include an increasing choice intensity. We also make the learning rate decay over time.</p>
<div id="7b8936fd-11f4-42e2-899a-353e597529a6" class="cell" data-execution_count="70">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AdjustingLearner(Learner):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>                 ValueBeliefs_Qoa, </span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>                 DiscountFactor, </span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>                 LearningRate, MinLearningRate, LearningRateDecayFactor,</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>                 ChoiceIntensity, MaxChoiceIntensity, ChoiceIntensityGrowthFactor,</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>                ):</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.DiscountFactor <span class="op">=</span> DiscountFactor</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LearningRate <span class="op">=</span> LearningRate</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.MinLearningRate <span class="op">=</span> MinLearningRate</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LearningRateDecayFactor <span class="op">=</span> LearningRateDecayFactor</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ChoiceIntensity <span class="op">=</span> ChoiceIntensity</span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.MaxChoiceIntensity <span class="op">=</span> MaxChoiceIntensity</span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ChoiceIntensityGrowthFactor <span class="op">=</span> ChoiceIntensityGrowthFactor</span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ValueBeliefs_Qoa <span class="op">=</span> ValueBeliefs_Qoa</span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ActionIxs <span class="op">=</span> <span class="bu">range</span>(ValueBeliefs_Qoa.shape[<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="2c5d522e-a494-4c7d-a15e-6b6c178ddc58" class="cell" data-execution_count="71">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(<span class="va">self</span>, </span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>           obs: <span class="bu">int</span>,</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>           action: <span class="bu">int</span>,</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>           reward: <span class="bu">float</span>,</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>           next_obs: <span class="bu">int</span>,</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>          ):</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Updates the value beliefs / Q-value of an action."""</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>    temporal_difference <span class="op">=</span><span class="va">self</span>.obtain_temporal_difference(</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>        obs, action, reward, next_obs)</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ValueBeliefs_Qoa[obs, action] <span class="op">=</span> (</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ValueBeliefs_Qoa[obs, action] <span class="op">+</span> <span class="va">self</span>.LearningRate <span class="op">*</span> temporal_difference</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.LearningRate <span class="op">=</span> <span class="bu">max</span>(<span class="va">self</span>.MinLearningRate, </span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>                            <span class="va">self</span>.LearningRate <span class="op">*</span> <span class="va">self</span>.LearningRateDecayFactor)</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ChoiceIntensity <span class="op">=</span> <span class="bu">min</span>(<span class="va">self</span>.MaxChoiceIntensity, </span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>                               <span class="va">self</span>.ChoiceIntensity <span class="op">*</span> <span class="va">self</span>.ChoiceIntensityGrowthFactor)</span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>AdjustingLearner.update <span class="op">=</span> update</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now, we are ready to perform a new learning simulation.</p>
<div id="0c8542ba-57be-4941-86e8-06acf4389330" class="cell" data-execution_count="72">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> AdjustingLearner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>                           DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>                           LearningRate <span class="op">=</span> <span class="fl">0.1</span>, MinLearningRate <span class="op">=</span> <span class="fl">0.01</span>, LearningRateDecayFactor <span class="op">=</span> <span class="fl">0.999</span>,</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>                           ChoiceIntensity <span class="op">=</span> <span class="fl">1.0</span>, MaxChoiceIntensity <span class="op">=</span> <span class="fl">50.0</span>, ChoiceIntensityGrowthFactor <span class="op">=</span> <span class="fl">1.001</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="57b685f4-93ea-4a24-8eae-45e32d8c1f05" class="cell" data-execution_count="73">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>plot_learning_process(df, plot_varying_parameters<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-69-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning the risk-reward dilemma with an increasing choice intensity.</figcaption>
</figure>
</div>
</div>
</div>
<p>We find the <code>AdjustingLearner</code> is able to consitently learn the optimal policy. <strong>Try to convince yourself that this is true by re-executing the simulation above multiple times.</strong> We also find that it learns the optimal policy in approx. less than 4000 time steps. <strong>How does the learning process depend on the additional parameters?</strong></p>
<p>Obviously, the <code>AdjustingLearner</code> is a more complex agent than the <code>Learner</code>. There is also a prominent <em>trick</em> to give our simpler <code>Learner</code> agent an initial exploration bonus.</p>
</section>
<section id="initial-exploration-bonus" class="level3">
<h3 class="anchored" data-anchor-id="initial-exploration-bonus">Initial exploration bonus</h3>
<p>We can give the agent an initial exploration bonus by setting the initial value beliefs to a high value. This is called <strong>optimistic initialization</strong>.</p>
<div id="8cfc0bed-93c2-48ed-8fd6-f85af99a5ff4" class="cell" data-execution_count="74">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">8</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">60.0</span>)</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run(learner, env, <span class="dv">10000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f631e444-7574-45ab-83d3-83b940b817f7" class="cell" data-execution_count="75">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>plot_learning_process(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-71-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning the risk-reward dilemma with an initial exploration bonus.</figcaption>
</figure>
</div>
</div>
</div>
<p>So far, we have investigated the learning process in a single environment.</p>
<p>Next, we will explore learning in normal-form games to illustrate the modularity of the agent-environment interface, utilizing the same learning agents as previously discussed.</p>
</section>
</section>
<section id="multi-agent-environments-games" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="multi-agent-environments-games"><span class="header-section-number">10.6</span> Multi-agent environments | Games</h2>
<section id="interface-1" class="level3">
<h3 class="anchored" data-anchor-id="interface-1">Interface</h3>
<p>First, we adjust the <code>interface_run</code> function to work with the multi-agent environment.</p>
<p>We can make the <em>columns</em> of a dataframe adaptive to the number of agents.</p>
<div id="7ebd4187-69e5-4ded-97d8-d8541d5cdb47" class="cell" data-execution_count="76">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>NrAgents <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> NrOfTimesteps <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_dataframe(NrAgents, NrOfTimesteps):</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> <span class="bu">list</span>(np.array([(<span class="ss">f"action</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, <span class="ss">f"observation</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, <span class="ss">f"reward</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>                              <span class="ss">f"beliefs</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, <span class="ss">f"policy</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, </span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>                              <span class="ss">f"ChoiceIntensity</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, <span class="ss">f"LearningRate</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>) </span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>                             <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(NrAgents)]).flatten())</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(index<span class="op">=</span><span class="bu">range</span>(NrOfTimesteps), columns<span class="op">=</span>columns)</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>create_dataframe(NrAgents, NrOfTimesteps)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="76">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">action0</th>
<th data-quarto-table-cell-role="th">observation0</th>
<th data-quarto-table-cell-role="th">reward0</th>
<th data-quarto-table-cell-role="th">beliefs0</th>
<th data-quarto-table-cell-role="th">policy0</th>
<th data-quarto-table-cell-role="th">ChoiceIntensity0</th>
<th data-quarto-table-cell-role="th">LearningRate0</th>
<th data-quarto-table-cell-role="th">action1</th>
<th data-quarto-table-cell-role="th">observation1</th>
<th data-quarto-table-cell-role="th">reward1</th>
<th data-quarto-table-cell-role="th">beliefs1</th>
<th data-quarto-table-cell-role="th">policy1</th>
<th data-quarto-table-cell-role="th">ChoiceIntensity1</th>
<th data-quarto-table-cell-role="th">LearningRate1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We also write a function to populate the DataFrame with the values of the learning process.</p>
<div id="c1c33cd3-9eb7-4548-82fa-e5ae0d7a689b" class="cell" data-execution_count="77">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fill_dataframe(actions, next_observations, rewards, agents):</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> []</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(agents)):</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>        data <span class="op">+=</span> [actions[i], next_observations[i], rewards[i],</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>                 deepcopy(agents[i].ValueBeliefs_Qoa), </span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>                 deepcopy(agents[i].obtain_policy_Xoa()),</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>                 deepcopy(agents[i].ChoiceIntensity), </span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>                 deepcopy(agents[i].LearningRate)]</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Adjusting the <code>interface_run</code> function to work with the multi-agent environment yields a clean and readable implementation.</p>
<div id="0cf6b4a2-7c04-4b87-902a-6eb7460c0e34" class="cell" data-execution_count="78">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interface_run(agents, env, NrOfTimesteps):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run the multi-agent environment for several time steps."""</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> create_dataframe(<span class="bu">len</span>(agents), NrOfTimesteps)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    observations <span class="op">=</span> env.observe()</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(NrOfTimesteps):</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>        actions <span class="op">=</span> [agent.act(observations[i])</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, agent <span class="kw">in</span> <span class="bu">enumerate</span>(agents)]</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>        next_observations, rewards, info <span class="op">=</span> env.step(actions)</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, agent <span class="kw">in</span> <span class="bu">enumerate</span>(agents):</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>            agent.update(observations[i], actions[i], rewards[i], next_observations[i])</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>        df.loc[t] <span class="op">=</span> fill_dataframe(actions, next_observations, rewards, agents)</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>        observations <span class="op">=</span> next_observations</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Testing whether it works in our previous environment, the risk-reward dilemma looks promising.</p>
<div id="05a9a7ec-77d2-4b6c-8a29-2113537a591e" class="cell" data-execution_count="79">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">8</span><span class="op">*</span>np.ones((<span class="dv">2</span>,<span class="dv">2</span>)), </span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">60.0</span>)</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> RiskRewardDilemma(CollapseProbability<span class="op">=</span><span class="fl">0.2</span>, RecoveryProbability<span class="op">=</span><span class="fl">0.1</span>, </span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>                        SafeReward<span class="op">=</span><span class="fl">0.8</span>, RiskyReward<span class="op">=</span><span class="fl">1.0</span>, DegradedReward<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: we need to pass the learner as a list of agents</span></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run([learner], env, <span class="dv">4</span>) </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c9de4df6-3cc3-472c-897b-ba1c1761b032" class="cell" data-execution_count="80">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="80">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">action0</th>
<th data-quarto-table-cell-role="th">observation0</th>
<th data-quarto-table-cell-role="th">reward0</th>
<th data-quarto-table-cell-role="th">beliefs0</th>
<th data-quarto-table-cell-role="th">policy0</th>
<th data-quarto-table-cell-role="th">ChoiceIntensity0</th>
<th data-quarto-table-cell-role="th">LearningRate0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>1</td>
<td>0</td>
<td>1.0</td>
<td>[[8.0, 7.93], [8.0, 8.0]]</td>
<td>[[0.9852259683067277, 0.014774031693272396], [...</td>
<td>60.0</td>
<td>0.1</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>0</td>
<td>0</td>
<td>0.8</td>
<td>[[7.927906923600332, 7.93], [8.0, 8.0]]</td>
<td>[[0.46864505269071266, 0.5313549473092875], [0...</td>
<td>60.0</td>
<td>0.1</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>0</td>
<td>0</td>
<td>0.8</td>
<td>[[7.8567279493493345, 7.93], [8.0, 8.0]]</td>
<td>[[0.012172569010807209, 0.9878274309891928], [...</td>
<td>60.0</td>
<td>0.1</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>1</td>
<td>0</td>
<td>1.0</td>
<td>[[7.8567279493493345, 7.86061972818162], [8.0,...</td>
<td>[[0.4418871301229423, 0.5581128698770577], [0....</td>
<td>60.0</td>
<td>0.1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The real test, however, comes with a true multi-agent environment.</p>
</section>
<section id="social-dilemmas-environment" class="level3">
<h3 class="anchored" data-anchor-id="social-dilemmas-environment">Social dilemmas environment</h3>
<p>Let’s extend our treatment of reinforcement learning to <strong>multiple agents</strong>. From the perspective of each individual agent, other agents make the environment <strong>non-stationary</strong>. This can complicate reinforcement learning significantly.</p>
<p>We here focus on normal-form games and use the generic model of a social dilemma, introduced in Lecture <a href="./03.02-StrategicInteractions.html">03.02-StrategicInteractions</a>.</p>
<p><span class="math display">\[
\begin{array}{c|cc}
\text{} &amp; \color{blue}{\mathsf{Abate}} &amp; \color{blue}{\mathsf{Pollute}} \\
\hline
\color{red}{\mathsf{Abate}} &amp; {\color{red}{1}} \ | \ {\color{blue}{1}} &amp; {\color{red}{-1-F}} \ | \ {\color{blue}{+1+G}} \\
\color{red}{\mathsf{Pollute}} &amp; {\color{red}{+1+G}} \ | \ {\color{blue}{-1-F}} &amp; {\color{red}{-1}} \ | \ {\color{blue}{-1}} \\
\end{array}
\]</span></p>
<p>Depending on whether the greed <span class="math inline">\(G\)</span> and fear <span class="math inline">\(F\)</span> are positive or negative, we can distinguish four types of games <a href="#fig-social-dilemma-dimensions" class="quarto-xref">Figure&nbsp;<span>10.3</span></a>.</p>
<div id="fig-social-dilemma-dimensions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-social-dilemma-dimensions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/03.02-SocialDilemmaDimensions.dio.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-social-dilemma-dimensions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.3: Dimensions of a social dilemma with ordinal payoffs and Nash equilibira shown in boxes.
</figcaption>
</figure>
</div>
<p>In <a href="#fig-social-dilemma-dimensions" class="quarto-xref">Figure&nbsp;<span>10.3</span></a>, the payoff values are ordinal, meaning that only their order, <span class="math inline">\(3&gt;2&gt;1&gt;0\)</span>, is considered of relevance.</p>
<p>We also implement it as a class using the same interface as before and letting it inherit from our base environment.</p>
<div id="4e592d73-5812-46e2-a9fc-81445803d2ed" class="cell" data-execution_count="81">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SocialDilemma(Environment):</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A simple social dilemma environment."""</span></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_StateSet(<span class="va">self</span>):</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="st">'.'</span>]  <span class="co"># a dummy state</span></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> obtain_ActionSets(<span class="va">self</span>):</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># abate, pollute for two agents</span></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [[<span class="st">'a'</span>, <span class="st">'p'</span>], [<span class="st">'a'</span>, <span class="st">'p'</span>]]  </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Due to the absence of environmental state transitions, the environment consistently exists in a single, effective dummy state. Consequently, the transition tensor is simplified significantly.</p>
<div id="71902475-6063-4bec-88bb-aa40a8ff6952" class="cell" data-execution_count="82">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_TransitionTensor(<span class="va">self</span>):</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create the transition tensor."""</span></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.ones((<span class="va">self</span>.Z, <span class="va">self</span>.M, <span class="va">self</span>.M, <span class="va">self</span>.Z))</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>SocialDilemma.create_TransitionTensor <span class="op">=</span> create_TransitionTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The reward tensor is slighlty more complicated. The two defining parameters of the social dilemma environemtn are the greed <span class="math inline">\(G\)</span> and the fear <span class="math inline">\(F\)</span>.</p>
<div id="0c4b8aaa-973a-4bb2-b341-ec057bb462f3" class="cell" data-execution_count="83">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>F, G <span class="op">=</span> sp.symbols(<span class="st">'F G'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We represent rewards using a five-dimensional tensor with dimensions <span class="math inline">\(N \times Z \times M \times M \times Z\)</span>. Here, <span class="math inline">\(N\)</span> denotes the number of agents, <span class="math inline">\(Z=1\)</span> signifies the state count, and <span class="math inline">\(M\)</span> indicates the number of actions. A uni-dimensional state dimension is essential for accommodating multi-state environments.</p>
<div id="3e3649f1-8ddc-4386-988e-9f8db934b580" class="cell" data-execution_count="84">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.zeros((<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>), dtype<span class="op">=</span><span class="bu">object</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Helper variable for the indices facilitate the construction of the reward tensor.</p>
<div id="ee603da4-b36c-40d4-9206-15a4fec2a673" class="cell" data-execution_count="85">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> SocialDilemma().obtain_ActionSets()[<span class="dv">0</span>].index(<span class="st">'a'</span>)</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> SocialDilemma().obtain_ActionSets()[<span class="dv">0</span>].index(<span class="st">'p'</span>)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>a,p</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>(0, 1)</code></pre>
</div>
</div>
<p>Mutual abatement yields a reward of one for both agents.</p>
<div id="b2b4a328-af74-423d-a4bd-58f84ec96ffb" class="cell" data-execution_count="86">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">0</span>, <span class="dv">0</span>, a, a, <span class="dv">0</span>] <span class="op">=</span> R[<span class="dv">1</span>, <span class="dv">0</span>, a, a, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Mutual pollution yields a reward of minus one for both agents.</p>
<div id="b2ed336a-213f-4554-a781-c42a905dbf0c" class="cell" data-execution_count="87">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">0</span>, <span class="dv">0</span>, p, p, <span class="dv">0</span>] <span class="op">=</span> R[<span class="dv">1</span>, <span class="dv">0</span>, p, p, <span class="dv">0</span>] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Pollution by one agent and abatement by the other agent yields a reward of one plus the greed for the polluting agent and minus the fear for the abating agent.</p>
<div id="31ed01a2-7699-462f-9c6f-5e95eb84e905" class="cell" data-execution_count="88">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">0</span>, <span class="dv">0</span>, p, a, <span class="dv">0</span>] <span class="op">=</span> R[<span class="dv">1</span>, <span class="dv">0</span>, a, p, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> G</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>R[<span class="dv">0</span>, <span class="dv">0</span>, a, p, <span class="dv">0</span>] <span class="op">=</span> R[<span class="dv">1</span>, <span class="dv">0</span>, p, a, <span class="dv">0</span>] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">-</span> F</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In sum, the reward tensor for agent zero reads,</p>
<div id="89b2e326-e34f-4799-b101-f218f978de85" class="cell" data-execution_count="89">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>sp.Array(R[<span class="dv">0</span>,<span class="dv">0</span>,:,:,<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="89">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}1 &amp; - F - 1\\G + 1 &amp; -1\end{matrix}\right]\)</span></p>
</div>
</div>
<p>and for agent one,</p>
<div id="ebba00b5-5df2-4d30-929c-3993c418b382" class="cell" data-execution_count="90">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>sp.Array(R[<span class="dv">1</span>,<span class="dv">0</span>,:,:,<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="90">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}1 &amp; G + 1\\- F - 1 &amp; -1\end{matrix}\right]\)</span></p>
</div>
</div>
<div id="ecc0943a-a141-468a-bc8a-2d8281980484" class="cell" data-execution_count="91">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_RewardTensor(<span class="va">self</span>):</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create the reward tensor."""</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> substitute_in_array(</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>        R, {F: <span class="va">self</span>.Fear, G: <span class="va">self</span>.Greed}).astype(<span class="bu">float</span>)</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>SocialDilemma.create_RewardTensor <span class="op">=</span> create_RewardTensor</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The two defining parameters of the social dilemma are the greed <span class="math inline">\(G\)</span> and the fear <span class="math inline">\(F\)</span>.</p>
<div id="91e55bbe-e814-4ef6-9ae1-262b4f3435cf" class="cell" data-execution_count="92">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, Greed, Fear):</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.N <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> <span class="va">self</span>.M <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> <span class="va">self</span>.Z <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.Greed <span class="op">=</span> Greed</span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.Fear <span class="op">=</span> Fear</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.StateSet <span class="op">=</span> <span class="va">self</span>.obtain_StateSet()</span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.ActionSets <span class="op">=</span> <span class="va">self</span>.obtain_ActionSets()</span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.TransitionTensor <span class="op">=</span> <span class="va">self</span>.create_TransitionTensor()</span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.RewardTensor <span class="op">=</span> <span class="va">self</span>.create_RewardTensor()</span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb109-12"><a href="#cb109-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.state <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb109-13"><a href="#cb109-13" aria-hidden="true" tabindex="-1"></a>SocialDilemma.<span class="fu">__init__</span> <span class="op">=</span> <span class="fu">__init__</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="testing-the-implementation" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-implementation">Testing the implementation:</h3>
<div id="671a38ed-2877-4742-afe9-35f9f6170bdd" class="cell" data-execution_count="93">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> SocialDilemma(Fear<span class="op">=</span><span class="fl">0.65</span>, Greed<span class="op">=</span><span class="fl">0.75</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Mutual cooperation by two abating agents:</p>
<div id="5598593d-25fd-4df4-821c-1b2194070034" class="cell" data-execution_count="94">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>env.step([a,a])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre><code>([0, 0], array([1., 1.]), {'state': 0})</code></pre>
</div>
</div>
<p>Mutual defection by two polluting agents:</p>
<div id="71751554-1599-473a-9b7d-2198f2613dfa" class="cell" data-execution_count="95">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>env.step([p,p])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>([0, 0], array([-1., -1.]), {'state': 0})</code></pre>
</div>
</div>
<p>Different actions:</p>
<div id="42f752a3-6de2-4087-8879-231e901f82c9" class="cell" data-execution_count="96">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>env.step([a,p])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>([0, 0], array([-1.65,  1.75]), {'state': 0})</code></pre>
</div>
</div>
<div id="eecb0112-d14e-4adf-a4ae-2f922fb050a4" class="cell" data-execution_count="97">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>env.step([p,a])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre><code>([0, 0], array([ 1.75, -1.65]), {'state': 0})</code></pre>
</div>
</div>
<p>Testing whether the implementation works,</p>
<div id="31779e7b-e8f3-4e78-b645-c33d58ce31cd" class="cell" data-execution_count="98">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>learner1 <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> <span class="dv">8</span><span class="op">*</span>np.ones((<span class="dv">1</span>,<span class="dv">2</span>)), </span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>                  DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>                  LearningRate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>                  ChoiceIntensity <span class="op">=</span> <span class="fl">60.0</span>)</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>learner2 <span class="op">=</span> deepcopy(learner1)</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> SocialDilemma(Fear<span class="op">=</span><span class="dv">1</span>, Greed<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run([learner1, learner2], env, <span class="dv">4</span>) </span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="98">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">action0</th>
<th data-quarto-table-cell-role="th">observation0</th>
<th data-quarto-table-cell-role="th">reward0</th>
<th data-quarto-table-cell-role="th">beliefs0</th>
<th data-quarto-table-cell-role="th">policy0</th>
<th data-quarto-table-cell-role="th">ChoiceIntensity0</th>
<th data-quarto-table-cell-role="th">LearningRate0</th>
<th data-quarto-table-cell-role="th">action1</th>
<th data-quarto-table-cell-role="th">observation1</th>
<th data-quarto-table-cell-role="th">reward1</th>
<th data-quarto-table-cell-role="th">beliefs1</th>
<th data-quarto-table-cell-role="th">policy1</th>
<th data-quarto-table-cell-role="th">ChoiceIntensity1</th>
<th data-quarto-table-cell-role="th">LearningRate1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>1</td>
<td>0</td>
<td>3.0</td>
<td>[[8.0, 7.95]]</td>
<td>[[0.9525741268224331, 0.047425873177566774]]</td>
<td>60.0</td>
<td>0.1</td>
<td>0</td>
<td>0</td>
<td>-2.0</td>
<td>[[7.9, 8.0]]</td>
<td>[[0.0024726231566347743, 0.9975273768433652]]</td>
<td>60.0</td>
<td>0.1</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>0</td>
<td>0</td>
<td>-2.0</td>
<td>[[7.899786583570701, 7.95]]</td>
<td>[[0.0468507276383191, 0.9531492723616809]]</td>
<td>60.0</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>3.0</td>
<td>[[7.9, 7.94997774639159]]</td>
<td>[[0.047486230263144295, 0.9525137697368558]]</td>
<td>60.0</td>
<td>0.1</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>1</td>
<td>0</td>
<td>-1.0</td>
<td>[[7.899786583570701, 7.860288271841277]]</td>
<td>[[0.9145029408174301, 0.08549705918256983]]</td>
<td>60.0</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>-1.0</td>
<td>[[7.9, 7.860264375998088]]</td>
<td>[[0.9156096786764698, 0.08439032132353015]]</td>
<td>60.0</td>
<td>0.1</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>0</td>
<td>0</td>
<td>1.0</td>
<td>[[7.830484788680395, 7.860288271841277]]</td>
<td>[[0.14329244697889626, 0.8567075530211037]]</td>
<td>60.0</td>
<td>0.1</td>
<td>0</td>
<td>0</td>
<td>1.0</td>
<td>[[7.830698202813024, 7.860264375998088]]</td>
<td>[[0.14504926660635398, 0.854950733393646]]</td>
<td>60.0</td>
<td>0.1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>throws no erros.</p>
</section>
<section id="transient-cooperation" class="level3">
<h3 class="anchored" data-anchor-id="transient-cooperation">Transient cooperation</h3>
<p>In this section, we show that reinforcement learning agents can learn to cooperate in a tragedy social dilemma environment. However, this cooperation is not stable. It is only a transient phenomenon <span class="citation" data-cites="GollEtAl2024">(<a href="References.html#ref-GollEtAl2024" role="doc-biblioref">Goll et al., 2024</a>)</span>.</p>
<p>We use a social dilemma with fear <span class="math inline">\(F=1\)</span> and greed <span class="math inline">\(G=2\)</span>.</p>
<div id="d0847189-fb37-433e-8666-d18e4a2ce3eb" class="cell" data-execution_count="99">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> SocialDilemma(Fear<span class="op">=</span><span class="dv">1</span>, Greed<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We want to give the agents an inital boost to cooperate or abate. Thus, we give them an inital higher value belief for abate than pollute.</p>
<div id="5cb7fc4b-17ff-47da-b7fd-c258ec07345a" class="cell" data-execution_count="100">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>learner1 <span class="op">=</span> Learner(ValueBeliefs_Qoa <span class="op">=</span> np.array([[<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>]]), </span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>                   DiscountFactor <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>                   LearningRate <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>                   ChoiceIntensity <span class="op">=</span> <span class="fl">5.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We assume the second agent to be identical to the first one.</p>
<div id="92ee5ca5-42a3-4f3d-97e9-52b2aaa89cae" class="cell" data-execution_count="101">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>learner2 <span class="op">=</span> deepcopy(learner1)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="d6be38e3-5021-43a5-b14b-2e65357d6206" class="cell" data-execution_count="102">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> interface_run([learner1, learner2], env, <span class="dv">20000</span>) </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="5b90248b-97c9-44e7-baf1-82a6dbf8bbe4" class="cell" data-execution_count="103">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_TwoAgentBeliefsPolicies(df):</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>    beliefs0_Qtoa <span class="op">=</span> np.array(df.beliefs0.values.tolist())</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>    policy0_Xtoa <span class="op">=</span> np.array(df.policy0.values.tolist())</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>    beliefs1_Qtoa <span class="op">=</span> np.array(df.beliefs1.values.tolist())</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>    policy1_Xtoa <span class="op">=</span> np.array(df.policy1.values.tolist())</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>,<span class="dv">6</span>))</span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a>    ax0 <span class="op">=</span> fig.add_subplot(<span class="dv">311</span>)<span class="op">;</span> ax0.set_ylabel(<span class="st">'Value beliefs'</span>)<span class="op">;</span></span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs0_Qtoa[:,<span class="dv">0</span>,a], label<span class="op">=</span><span class="st">'Q1(a)'</span>, color<span class="op">=</span><span class="st">'blue'</span>,lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb124-11"><a href="#cb124-11" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs0_Qtoa[:,<span class="dv">0</span>,p], label<span class="op">=</span><span class="st">'Q1(p)'</span>, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb124-12"><a href="#cb124-12" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs1_Qtoa[:,<span class="dv">0</span>,a], label<span class="op">=</span><span class="st">'Q2(a)'</span>, color<span class="op">=</span><span class="st">'darkblue'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb124-13"><a href="#cb124-13" aria-hidden="true" tabindex="-1"></a>    ax0.plot(beliefs1_Qtoa[:,<span class="dv">0</span>,p], label<span class="op">=</span><span class="st">'Q2(p)'</span>, color<span class="op">=</span><span class="st">'darkred'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb124-14"><a href="#cb124-14" aria-hidden="true" tabindex="-1"></a>    ax0.legend(loc<span class="op">=</span><span class="st">'center right'</span>)<span class="op">;</span> ax0.set_xlim(<span class="op">-</span><span class="dv">10</span>, <span class="bu">len</span>(df)<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb124-15"><a href="#cb124-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb124-16"><a href="#cb124-16" aria-hidden="true" tabindex="-1"></a>    ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">312</span>, sharex<span class="op">=</span>ax0)<span class="op">;</span> ax1.set_ylabel(<span class="st">'Policy'</span>)</span>
<span id="cb124-17"><a href="#cb124-17" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy0_Xtoa[:,<span class="dv">0</span>,a], label<span class="op">=</span><span class="st">'X1(a)'</span>, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb124-18"><a href="#cb124-18" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy0_Xtoa[:,<span class="dv">0</span>,p], label<span class="op">=</span><span class="st">'X1(p)'</span>, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb124-19"><a href="#cb124-19" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy1_Xtoa[:,<span class="dv">0</span>,a], label<span class="op">=</span><span class="st">'X2(a)'</span>, color<span class="op">=</span><span class="st">'darkblue'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb124-20"><a href="#cb124-20" aria-hidden="true" tabindex="-1"></a>    ax1.plot(policy1_Xtoa[:,<span class="dv">0</span>,p], label<span class="op">=</span><span class="st">'X2(p)'</span>, color<span class="op">=</span><span class="st">'darkred'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb124-21"><a href="#cb124-21" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">'Time steps'</span>)<span class="op">;</span> ax1.legend(loc<span class="op">=</span><span class="st">'center right'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="6de64307-de3e-4358-9da8-0ff85ea4b309" class="cell" data-execution_count="104">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>plot_TwoAgentBeliefsPolicies(df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04.02-IndividualLearning_files/figure-html/cell-100-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Transient cooperation in a tragedy social dilemma.</figcaption>
</figure>
</div>
</div>
</div>
<p>A propensity for cooperation coupled with excessive exploitation initially leads to transient cooperation within the stochastic learning dynamics. If one is unaware of this phenomenon, it may seem that the issue of cooperation in social dilemmas is resolved. However, as learning continues, this cooperation phase diminishes, resulting in increased defection (pollution) over cooperation (abate). During this phase, agents explore excessively, necessitating a higher choice intensity to establish more deterministic policies that align with the Nash equilibrium of full defection.</p>
<p>The timeing of when the breakdown of cooperation happens is stochastic. <strong>Re-run the simulation above with differnt random seeds to see that this is true</strong>. Beaware, that you must re-initalize the learners to begin from scratch.</p>
</section>
</section>
<section id="learning-goals-revisited" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="learning-goals-revisited"><span class="header-section-number">10.7</span> Learning goals revisited</h2>
<ul>
<li>Reinforcement learning is <strong>valuable in models of human-environment interactions</strong> as a principled take to integrate individual cognition in dynamic environments and emerging collective behavior</li>
<li>We implemented the different elements of the multi-agent environment framework (interface, environment, agents).
<ul>
<li>We implemented and applied a <strong>basic temporal-different learning</strong> agent.</li>
<li>We implemented and applied the <strong>risk-reward dilemma</strong> (Lecture <a href="./03.01-SequentialDecisions.html">03.01</a>) and <strong>social dilemma</strong> (Lecture <a href="./03.02-StrategicInteractions.html">03.02</a>)</li>
<li>We visualized the learning process</li>
</ul></li>
<li>We introduced and studied the <strong>exploration-exploitation trade-off</strong>, a general challenge for decision-making under uncertainty.</li>
<li>We made all of this possible by using the Python library <code>pandas</code> to manage data and refining our skills in object-oriented programming.</li>
</ul>
<section id="key-advantages-of-an-rl-framework" class="level3">
<h3 class="anchored" data-anchor-id="key-advantages-of-an-rl-framework">Key advantages of an RL framework</h3>
<ul>
<li>Cognitive mechanisms are more integrated / less fragmented than behavioral theories</li>
<li>Cognitive mechanisms (as in RL) are more formalized than behavioral theories</li>
<li>The RL frame provides a natural dynamic extension to some economic equilibrium decision models.</li>
<li>The Rl frame allows for the study of behavior changes (e.g., after experimental policy interventions or environmental catastrophes)</li>
</ul>
</section>
<section id="challenges" class="level3">
<h3 class="anchored" data-anchor-id="challenges">Challenges</h3>
<ul>
<li>The <strong>learning is inefficient</strong>. The agents require many interactions with the environment to learn what to do as they do not learn any model of the environment. This is a cognitive wasteful process.</li>
<li>Dealing with <strong>rare states/events</strong> is <strong>challenging</strong> when learning from only experience. Even more sample interactions are required to have enough experience of the raw events.</li>
<li>The <strong>stochasticity and hyperparameter tuning</strong> make it a <strong>cumbersome modeling tool</strong>. Both elements are invaluable for RL as an optimization method. For RL as a model of the cognitive processes underpinning human behavior, stochasticity and hyperparameter tuning complicate the modeling process considerably. They make studying the learning dynamics more difficult than necessary.</li>
</ul>
<p>Up next: Deterministic approximations</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-AlbrechtEtAl2024" class="csl-entry" role="listitem">
Albrecht, S. V., Christianos, F., &amp; Schäfer, L. (2024). <em>Multi-<span>Agent Reinforcement Learning</span>: <span>Foundations</span> and <span>Modern Approaches</span></em>.
</div>
<div id="ref-BarfussEtAl2024a" class="csl-entry" role="listitem">
Barfuss, W., Flack, J. C., Gokhale, C. S., Hammond, L., Hilbe, C., Hughes, E., Leibo, J. Z., Lenaerts, T., Levin, S. A., Madhushani Sehwag, U., McAvoy, A., Meylahn, J. M., &amp; Santos, F. P. (2024). Collective <span>Cooperative Intelligence</span>. <em>Forthcomming in the Proceedings of the National Academy of Sciences</em>.
</div>
<div id="ref-BotvinickEtAl2020" class="csl-entry" role="listitem">
Botvinick, M., Wang, J. X., Dabney, W., Miller, K. J., &amp; Kurth-Nelson, Z. (2020). Deep <span>Reinforcement Learning</span> and <span>Its Neuroscientific Implications</span>. <em>Neuron</em>, <em>107</em>(4), 603–616. <a href="https://doi.org/10.1016/j.neuron.2020.06.014">https://doi.org/10.1016/j.neuron.2020.06.014</a>
</div>
<div id="ref-ConstantinoEtAl2021" class="csl-entry" role="listitem">
Constantino, S. M., Schlüter, M., Weber, E. U., &amp; Wijermans, N. (2021). Cognition and behavior in context: A framework and theories to explain natural resource use decisions in social-ecological systems. <em>Sustainability Science</em>, <em>16</em>(5), 1651–1671. <a href="https://doi.org/10.1007/s11625-021-00989-w">https://doi.org/10.1007/s11625-021-00989-w</a>
</div>
<div id="ref-GollEtAl2024" class="csl-entry" role="listitem">
Goll, D., Heitzig, J., &amp; Barfuss, W. (2024). <em>Deterministic <span>Model</span> of <span>Incremental Multi-Agent Boltzmann Q-Learning</span>: <span>Transient Cooperation</span>, <span>Metastability</span>, and <span>Oscillations</span></em> (arXiv:2501.00160). arXiv. <a href="https://doi.org/10.48550/arXiv.2501.00160">https://doi.org/10.48550/arXiv.2501.00160</a>
</div>
<div id="ref-SchluterEtAl2017" class="csl-entry" role="listitem">
Schlüter, M., Baeza, A., Dressler, G., Frank, K., Groeneveld, J., Jager, W., Janssen, M. A., McAllister, R. R. J., Müller, B., Orach, K., Schwarz, N., &amp; Wijermans, N. (2017). A framework for mapping and comparing behavioural theories in models of social-ecological systems. <em>Ecological Economics</em>, <em>131</em>, 21–35. <a href="https://doi.org/10.1016/j.ecolecon.2016.08.008">https://doi.org/10.1016/j.ecolecon.2016.08.008</a>
</div>
<div id="ref-SuttonBarto2018" class="csl-entry" role="listitem">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (Second edition). The MIT Press.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04.01-BehavioralAgency.html" class="pagination-link" aria-label="Behavioral agency">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Behavioral agency</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04.03-LearningDynamics.html" class="pagination-link" aria-label="Learning dynamics">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Learning dynamics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>