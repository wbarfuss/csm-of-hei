{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28a63d7-d1c0-486d-8126-fe1831eb3ea6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Ex | Individual Learning {.unnumbered}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ea364-2170-4bab-bd8a-480a9362953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89f0a5-05c2-4e01-934a-f3fb1c279871",
   "metadata": {},
   "source": [
    "## Task 1 | Learning the risky policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827798d-1640-4082-adfe-bc6abb1568d1",
   "metadata": {},
   "source": [
    "In the lecture, we explored how the agent learns a cautious policy within the risk-reward dilemma. Investigate the learning process for parameter combinations that make the risky policy optimal (`DiscountFactor=0.6`, `CollapseProbability=0.1`, `RecoveryProbability=0.1`, `SafeReward=0.5`, `RiskyReward=1.0`, `DegradedReward=0.0`). What parameters of the learning process, such as learning rate and choice intensity, allow the agent to consistently learn the risky policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e734d17-9ab1-4708-90f6-55aaeb02ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0ea75-674a-4ff3-a13c-2b36394cd916",
   "metadata": {},
   "source": [
    "How does the learning process change if you change the transition probabilities to `CollapseProbability=0.05`, `RecoveryProbability=0.005`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7adb2-c4a9-427d-9b1a-8e60e860c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082a5b1-6fab-48ee-92a7-2733bc56b238",
   "metadata": {},
   "source": [
    "## Task 2 | Ecological public good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd4af7-b148-4fad-8f70-2fb506824dec",
   "metadata": {},
   "source": [
    "Implement the ecological public good from Lecture [03.03](03.02ex-StrategicInteractions.ipynb) as a reinforcement learning environment. Ensure your `EcologicalPublicGood` class inherits from the base Environment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5df67-dd17-47e1-b25e-0dfb7cb62c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8e2dd-12fc-4ca6-9035-58e7e4051b27",
   "metadata": {},
   "source": [
    "Let two agents learn in it and visualize the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd4848-886a-4783-81ae-3cd28c89792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3d8ca-bf07-4a9d-b684-88cdb4105ad9",
   "metadata": {},
   "source": [
    "Briefly discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a63a62-cda3-4578-be82-8ebc9eded904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
