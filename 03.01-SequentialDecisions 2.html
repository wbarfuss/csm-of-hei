<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Sequential Decisions – Complex Systems Modeling of Human-Environment Interactions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03.02-StrategicInteractions.html" rel="next">
<link href="./03-TargetEquilibria.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-dda8909163e0b4f3670ba323ebd66e56.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-TargetEquilibria.html">Target Equilibria</a></li><li class="breadcrumb-item"><a href="./03.01-SequentialDecisions.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Sequential Decisions</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/CSMofHEI_Logo.drawio.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="./images/CSMofHEI_Logo.drawio.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Complex Systems Modeling of Human-Environment Interactions</a> 
        <div class="sidebar-tools-main">
    <a href="./Complex-Systems-Modeling-of-Human-Environment-Interactions.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.01-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./02-DynamicSystems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dynamic Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.01-Nonlinearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Nonlinearity</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.02-TippingElements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tipping elements</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.03-Resilience.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Resilience</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.04-StateTransitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">State transitions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./03-TargetEquilibria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Target Equilibria</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.01-SequentialDecisions.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Sequential Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.02-StrategicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Strategic Interactions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.03-DynamicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dynamic Interactions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./04-TransformationAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformation Agency</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.01-BehavioralAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Behavioral agency</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.02-IndividualLearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Individual learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.03-LearningDynamics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Learning dynamics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Exercises</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.02ex-IntroToPython.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Introduction to Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.01ex-Nonlinearity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Nonlinearity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.02ex-TippingElements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Ex | Tipping elements</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.03ex-Resilience.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Resilience</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.04ex-StateTransitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | State transitions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.01ex-SequentialDecisions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Sequential Decisions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.02ex-StrategicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Strategic Interactions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.03ex-DynamicInteractions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Dynamic Interactions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.01ex-BehavioralAgency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Behavioral Agency</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.02ex-IndividualLearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Individual Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.03ex-LearningDynamics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ex | Learning Dynamics</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation-sequential-decision-making-under-uncertainty" id="toc-motivation-sequential-decision-making-under-uncertainty" class="nav-link active" data-scroll-target="#motivation-sequential-decision-making-under-uncertainty"><span class="header-section-number">6.1</span> Motivation | <strong>Sequential decision-making under uncertainty</strong></a>
  <ul class="collapse">
  <li><a href="#applications-in-human-environment-interactions" id="toc-applications-in-human-environment-interactions" class="nav-link" data-scroll-target="#applications-in-human-environment-interactions"><strong>Applications</strong> in human-environment interactions</a></li>
  <li><a href="#advantages-of-markov-decision-processes" id="toc-advantages-of-markov-decision-processes" class="nav-link" data-scroll-target="#advantages-of-markov-decision-processes"><strong>Advantages</strong> of Markov decision processes</a></li>
  <li><a href="#learning-goals" id="toc-learning-goals" class="nav-link" data-scroll-target="#learning-goals"><strong>Learning goals</strong></a></li>
  </ul></li>
  <li><a href="#markov-decision-processes-mdps" id="toc-markov-decision-processes-mdps" class="nav-link" data-scroll-target="#markov-decision-processes-mdps"><span class="header-section-number">6.2</span> <strong>Markov Decision Process</strong>es (MDPs)</a>
  <ul class="collapse">
  <li><a href="#example-model-overview" id="toc-example-model-overview" class="nav-link" data-scroll-target="#example-model-overview">Example model <strong>overview</strong></a></li>
  <li><a href="#states-and-actions" id="toc-states-and-actions" class="nav-link" data-scroll-target="#states-and-actions">States and actions</a></li>
  <li><a href="#transitions-environmental-dynamics" id="toc-transitions-environmental-dynamics" class="nav-link" data-scroll-target="#transitions-environmental-dynamics"><strong>Transitions</strong> | Environmental dynamics</a></li>
  <li><a href="#rewards-short-term-welfare" id="toc-rewards-short-term-welfare" class="nav-link" data-scroll-target="#rewards-short-term-welfare"><strong>Rewards</strong> | Short-term welfare</a></li>
  <li><a href="#policy" id="toc-policy" class="nav-link" data-scroll-target="#policy">Policy</a></li>
  </ul></li>
  <li><a href="#simulation" id="toc-simulation" class="nav-link" data-scroll-target="#simulation"><span class="header-section-number">6.3</span> Simulation</a>
  <ul class="collapse">
  <li><a href="#stochastic-simulation" id="toc-stochastic-simulation" class="nav-link" data-scroll-target="#stochastic-simulation">Stochastic simulation</a></li>
  <li><a href="#ensemble-simulation" id="toc-ensemble-simulation" class="nav-link" data-scroll-target="#ensemble-simulation">Ensemble simulation</a></li>
  <li><a href="#distribution-trajectory" id="toc-distribution-trajectory" class="nav-link" data-scroll-target="#distribution-trajectory">Distribution trajectory</a></li>
  </ul></li>
  <li><a href="#goals-and-values" id="toc-goals-and-values" class="nav-link" data-scroll-target="#goals-and-values"><span class="header-section-number">6.4</span> Goals and values</a>
  <ul class="collapse">
  <li><a href="#goal-functions" id="toc-goal-functions" class="nav-link" data-scroll-target="#goal-functions">Goal functions</a></li>
  <li><a href="#value-functions" id="toc-value-functions" class="nav-link" data-scroll-target="#value-functions">Value functions</a></li>
  <li><a href="#bellman-equation" id="toc-bellman-equation" class="nav-link" data-scroll-target="#bellman-equation">Bellman equation</a></li>
  </ul></li>
  <li><a href="#optimal-policies" id="toc-optimal-policies" class="nav-link" data-scroll-target="#optimal-policies"><span class="header-section-number">6.5</span> Optimal policies</a>
  <ul class="collapse">
  <li><a href="#numerical-computation" id="toc-numerical-computation" class="nav-link" data-scroll-target="#numerical-computation">Numerical computation</a></li>
  <li><a href="#symbolic-computation" id="toc-symbolic-computation" class="nav-link" data-scroll-target="#symbolic-computation">Symbolic computation</a></li>
  <li><a href="#efficient-computation" id="toc-efficient-computation" class="nav-link" data-scroll-target="#efficient-computation">Efficient computation</a></li>
  </ul></li>
  <li><a href="#learning-goals-revisited" id="toc-learning-goals-revisited" class="nav-link" data-scroll-target="#learning-goals-revisited"><span class="header-section-number">6.6</span> Learning goals <strong>revisited</strong></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-TargetEquilibria.html">Target Equilibria</a></li><li class="breadcrumb-item"><a href="./03.01-SequentialDecisions.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Sequential Decisions</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Sequential Decisions</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="https://wbarfuss.github.io">Wolfram Barfuss</a> | <a href="https://www.uni-bonn.de">University of Bonn</a> | 2024/2025 <br> ▶ <strong>Complex Systems Modeling of Human-Environment Interactions</strong></p>
<section id="motivation-sequential-decision-making-under-uncertainty" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="motivation-sequential-decision-making-under-uncertainty"><span class="header-section-number">6.1</span> Motivation | <strong>Sequential decision-making under uncertainty</strong></h2>
<p>We will introduce the model framework of <strong>Markov Decision Processes (MDPs)</strong> to model sequential decision-making under uncertainty. MDPs are a powerful tool to model decision-making processes in various applications, such as robotics, finance, and environmental management.</p>
<p><strong>MDPs highlight the trade-off between current and future wellbeing in the presence of uncertainty</strong>.</p>
<ul>
<li>Markov Decision Processes (MDPs) are models for sequential decision-making when outcomes are uncertain.</li>
<li>They extend <a href="./02.04-StateTransitions.html">Markov Chains</a> by a single <strong>agent</strong>, executing an <em>action</em> at each time step, trying to optimize its long-term wellbeing.</li>
</ul>
<section id="applications-in-human-environment-interactions" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-human-environment-interactions"><strong>Applications</strong> in human-environment interactions</h3>
<p>MDPs are widely used in environmental management and conservation biology to model the trade-off between current and future wellbeing in the presence of uncertainty <span class="citation" data-cites="Williams2009 MarescotEtAl2013">(<a href="References.html#ref-MarescotEtAl2013" role="doc-biblioref">Marescot et al., 2013</a>; <a href="References.html#ref-Williams2009" role="doc-biblioref">Williams, 2009</a>)</span>. Application areas cover the whole spectrum of <strong>natural resource ecology, management, and conservation</strong>, including</p>
<ul>
<li><strong>forestry and forest management</strong></li>
<li><strong>fisheries and aquatic management</strong></li>
<li><strong>wildlife and range management</strong></li>
<li><strong>weeds, pest, and disease control</strong></li>
</ul>
<p>In ecology, the term <em>stochastic dynamic programming</em> (SDP) is often used to refer to both the mathematical model (MDP) and its solution techniques (SDP per see).</p>
</section>
<section id="advantages-of-markov-decision-processes" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-markov-decision-processes"><strong>Advantages</strong> of Markov decision processes</h3>
<p>Using MDPs to model human-environment interactions has several advantages:</p>
<ul>
<li><strong>inherently stochastic</strong> - to account for uncertainty</li>
<li><strong>nonlinear</strong> - to account for structural changes</li>
<li><strong>agency</strong> - to account for <em>human behavior</em></li>
<li><strong>future-looking</strong> - to account for the trade-off between short-term and long-term</li>
<li><strong>feedback</strong> - between one agent and the environment</li>
</ul>
<p>In addition to these structural advantages, MDPs can also be solved in a computationally efficient way using a variety of algorithms, such as dynamic programming and reinforcement learning. This makes them also a <strong>scalable</strong> modeling framework. However, as <strong>our focus</strong> lies on <strong>transparent analysis and interpretation</strong>, we will focus on <strong>minimalistic models</strong> and won’t cover the computational aspects. But in principle, MDPs can be used to model high-dimensional systems with many states and actions.</p>
</section>
<section id="learning-goals" class="level3">
<h3 class="anchored" data-anchor-id="learning-goals"><strong>Learning goals</strong></h3>
<p>After this lecture, students will be able to:</p>
<ul>
<li><strong>Describe</strong> the elements of a Markov Decision Process (MDP) and how they relate to applications in human-environment interactions</li>
<li><strong>Simulate</strong> and <strong>visualize</strong> the time-evolution of an MDP</li>
<li><strong>Explain</strong> what value functions are, why they are useful, and how to relate to the agent’s goal and Bellman equation.</li>
<li><strong>Compute</strong> value functions and <strong>visualize</strong> the best policy in simple MDPs</li>
</ul>
</section>
</section>
<section id="markov-decision-processes-mdps" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="markov-decision-processes-mdps"><span class="header-section-number">6.2</span> <strong>Markov Decision Process</strong>es (MDPs)</h2>
<div id="2b9acba3" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np  </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact, interactive, fixed</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.style <span class="im">as</span> style<span class="op">;</span> style.use(<span class="st">'seaborn-v0_8'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="fl">7.8</span>, <span class="fl">2.5</span>)<span class="op">;</span> plt.rcParams[<span class="st">'figure.dpi'</span>] <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>color <span class="op">=</span> plt.rcParams[<span class="st">'axes.prop_cycle'</span>].by_key()[<span class="st">'color'</span>][<span class="dv">0</span>]  <span class="co"># get the first color of the default color cycle</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.facecolor'</span>] <span class="op">=</span> <span class="st">'white'</span><span class="op">;</span> plt.rcParams[<span class="st">'grid.color'</span>] <span class="op">=</span> <span class="st">'gray'</span><span class="op">;</span> plt.rcParams[<span class="st">'grid.linewidth'</span>] <span class="op">=</span> <span class="fl">0.25</span><span class="op">;</span> </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Graphically, a Markov decision process can be represented by the agent-environment interface in <a href="#fig-AEi" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>.</p>
<div id="fig-AEi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-AEi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/03.01-AgentEnvironment.dio.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-AEi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Agent-Environment Interface
</figcaption>
</figure>
</div>
<p>Formally, we define a Markov Decision process by the following elements:</p>
<ul>
<li>A discrete-time variable <span class="math inline">\(t=0,1,2,\dots\)</span></li>
<li>A discrete set of <strong>contexts</strong> or <strong>states</strong> <span class="math inline">\(\mathcal S = \{S_1, \dots, S_Z\}\)</span>.</li>
<li>A discrete set of <strong>options</strong> or <strong>actions</strong> <span class="math inline">\(\mathcal A = \{A_1, \dots, A_M\}\)</span>.</li>
<li>A transition function <span class="math inline">\(T: \mathcal S \times \mathcal A \times \mathcal S \rightarrow [0,1]\)</span>.
<ul>
<li><span class="math inline">\(T(s, a, s')\)</span> is the transition probability from current state <span class="math inline">\(s\)</span> to the next state <span class="math inline">\(s'\)</span> under action <span class="math inline">\(a\)</span>.</li>
</ul></li>
<li>A <strong>welfare</strong> or <strong>reward</strong> function <span class="math inline">\(R: \mathcal S \times \mathcal A \times \mathcal S \rightarrow \mathbb R\)</span>.
<ul>
<li><span class="math inline">\(R(s, a, s')\)</span> is the current/immediate/short-term reward the agent receives when executing action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and transitioning to state <span class="math inline">\(s'\)</span>.</li>
</ul></li>
<li>The agent’s <strong>goal</strong> or <strong>gain</strong> function <span class="math inline">\(G\)</span>, including a discount factor <span class="math inline">\(\gamma \in [0, 1)\)</span>, denoting how much the agent cares for future rewards</li>
<li>The agent’s <strong>policy</strong> or <strong>strategy</strong> <span class="math inline">\(x: \mathcal S \times \mathcal A \rightarrow [0,1]\)</span>.</li>
</ul>
<section id="example-model-overview" class="level3">
<h3 class="anchored" data-anchor-id="example-model-overview">Example model <strong>overview</strong></h3>
<p>We will illustrate the concept of MDPs using a simple example <span class="citation" data-cites="BarfussEtAl2018">(<a href="References.html#ref-BarfussEtAl2018" role="doc-biblioref">Barfuss et al., 2018</a>)</span>, modeling the trade-off between short-term gains and environmental collapse with long-term consequences for the decision-maker’s wellbeing.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/03.01-RiskRewardDilemma.dio.png" class="img-fluid figure-img"></p>
<figcaption>Risk-reward dilemma</figcaption>
</figure>
</div>
</section>
<section id="states-and-actions" class="level3">
<h3 class="anchored" data-anchor-id="states-and-actions">States and actions</h3>
<p>The environment consists of two states, <span class="math inline">\(\mathcal S = \{\textsf{p}, \textsf{d}\}\)</span>, representing a <strong>prosperous</strong> and a <strong>degraded</strong> state of the environment.</p>
<div id="d6f3b012" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>state_set <span class="op">=</span> [<span class="st">'prosperous'</span>, <span class="st">'degraded'</span>]<span class="op">;</span> p<span class="op">=</span><span class="dv">0</span><span class="op">;</span> d<span class="op">=</span><span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We also defined two Python variable <code>p=0</code> and <code>d=1</code> to serves as readable and memorable indices to represent the environmental contexts.</p>
<p>The agent can choose between two actions, <span class="math inline">\(\mathcal A = \{\textsf{f}, \textsf{r}\}\)</span>, representing a <strong>safe</strong> and a <strong>risky</strong> decision.</p>
<div id="aed10a53-b576-472f-a5d6-bdfbc234d520" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;fragment&quot;}}" data-tags="[]" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>action_set <span class="op">=</span> [<span class="st">'safe'</span>, <span class="st">'risky'</span>]<span class="op">;</span> f<span class="op">=</span><span class="dv">0</span><span class="op">;</span> r<span class="op">=</span><span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Likewise, we define two Python variables, <code>f=0</code> and <code>r=1</code>, to serve as readable and memorable indices to represent the agent’s actions. We represent the <em>safe</em> action with the <em>f</em> instead of the <em>s</em> to avoid confusion with the state <em>s</em>.</p>
</section>
<section id="transitions-environmental-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="transitions-environmental-dynamics"><strong>Transitions</strong> | Environmental dynamics</h3>
<p>The <strong>environmental dynamics</strong>, i.e., the transitions between environmental state contexts are modeled by two parameters, a collapse probability, <span class="math inline">\(p_c\)</span>, and a recovery probability, <span class="math inline">\(p_r\)</span>.</p>
<p>Let’s assume the following default values,</p>
<div id="074edea9" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>pc <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>pr <span class="op">=</span> <span class="fl">0.025</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We implement the transitions as a three-dimensional array or <strong>tensors</strong>, with dimensions <span class="math inline">\(Z \times M \times Z\)</span>, where <span class="math inline">\(Z\)</span> is the number of states and <span class="math inline">\(M\)</span> is the number of actions.</p>
<div id="b167f4c1" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.zeros((<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The cautious action guarantees to remain in the prosperous state, <span class="math inline">\(T(\mathsf{p,f,p})=1\)</span>. Thus, the agent can avoid the risk of environmental collapse by choosing the cautious action, <span class="math inline">\(T(\mathsf{p,f,d})=0\)</span>.</p>
<div id="63eff064" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>T[p,f,d] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>T[p,f,p] <span class="op">=</span> <span class="dv">1</span>   </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The risky action risks the collapse to the degraded state, <span class="math inline">\(T(\mathsf{p,r,d}) = p_c\)</span>, with a collapse probability <span class="math inline">\(p_c\)</span>. Thus, with probability <span class="math inline">\(1-p_c\)</span>, the environment remains prosperous under the risky action, <span class="math inline">\(T(\mathsf{p,r,p}) = 1-p_c\)</span>.</p>
<div id="7c1eb1b0" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>T[p,r,d] <span class="op">=</span> pc</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>T[p,r,p] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>pc</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>At the degraded state, recovery is only possible through the cautious action, <span class="math inline">\(T(\mathsf{d,f,p})=p_r\)</span>, with recovery probability <span class="math inline">\(p_r\)</span>. Thus, with probability <span class="math inline">\(1-p_r\)</span>, the environment remains degraded under the cautious action, <span class="math inline">\(T(\mathsf{d,f,d})=1-p_r\)</span>.</p>
<div id="726f3f8f" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>T[d,f,p] <span class="op">=</span> pr</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>T[d,f,d] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>pr</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Finally, the risky action at the degraded state guarantees a lock-in in the degraded state, <span class="math inline">\(T(\mathsf{d,r,d})=1\)</span>. Thus, the environment cannot recover from the degraded state under the risky action, <span class="math inline">\(T(\mathsf{d,r,p})=0\)</span>.</p>
<div id="3e303f30-d813-4a53-9133-e8fce0e871e4" class="cell" data-tags="[]" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>T[d,r,p] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>T[d,r,d] <span class="op">=</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Last, we make sure that our transition tensor is normalized, i.e., the sum of all transition probabilities from a state-action pair to all possible next states equals one, <span class="math inline">\(\sum_{s'} T(s, a, s') = 1\)</span>.</p>
<div id="b3d2ebff" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.allclose(T.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>), <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>All together, the transition tensor looks as follows:</p>
<div id="1d97f614-1e7f-46d9-929d-584f1c9c4cb4" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>T</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([[[1.   , 0.   ],
        [0.95 , 0.05 ]],

       [[0.025, 0.975],
        [0.   , 1.   ]]])</code></pre>
</div>
</div>
</section>
<section id="rewards-short-term-welfare" class="level3">
<h3 class="anchored" data-anchor-id="rewards-short-term-welfare"><strong>Rewards</strong> | Short-term welfare</h3>
<p>The rewards or welfare the agent receives represent the ecosystem services the environment provides. It is modeled by three parameters: a safe reward <span class="math inline">\(r_s\)</span>, a risky reward <span class="math inline">\(r_r&gt;r_s\)</span>, and a degraded reward <span class="math inline">\(r_d&lt;r_s\)</span>. We assume the following default values,</p>
<div id="436fc7b1" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>rs <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>rr <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>rd <span class="op">=</span> <span class="fl">0.0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>As the transition, we implement the rewards as a three-dimensional array or <strong>tensor</strong>, with dimensions <span class="math inline">\(Z \times M \times Z\)</span>, where <span class="math inline">\(Z\)</span> is the number of states and <span class="math inline">\(M\)</span> is the number of actions.</p>
<div id="470a909f" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.zeros((<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The cautious action at the prosperous state guarantees the safe reward, <span class="math inline">\(R(\mathsf{p,f,p}) = r_s\)</span>,</p>
<div id="76082409" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>R[p,f,p] <span class="op">=</span> rs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The risky action at the prosperous leads to the risky reward if the environment does not collapse, <span class="math inline">\(R(\mathsf{p,r,p}) = r_r\)</span>,</p>
<div id="a03713d3" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>R[p,r,p] <span class="op">=</span> rr</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Yet, whenever the environment enters, remains, or leaves the degraded state, it provides only the degraded reward <span class="math inline">\(R(\mathsf{d,:,:}) = R(\mathsf{:,:,d}) = r_d\)</span>, where <span class="math inline">\(:\)</span> denotes all possible states and actions.</p>
<div id="4bd41af2-3154-4335-8c0e-86f3bc35c2df" class="cell" data-tags="[]" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>R[d,:,:] <span class="op">=</span> R[:,:,d] <span class="op">=</span> rd</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Together, the reward tensor looks as follows:</p>
<div id="0666bc92" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>R</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>array([[[0.8, 0. ],
        [1. , 0. ]],

       [[0. , 0. ],
        [0. , 0. ]]])</code></pre>
</div>
</div>
</section>
<section id="policy" class="level3">
<h3 class="anchored" data-anchor-id="policy">Policy</h3>
<p>For now, we have not contemplated how the agent should behave. Therefore, to understand how transitions, rewards, and policies are related, let us simulate the MDP using a random policy.</p>
<p>We will implement a policy as a two-dimensional array or <strong>tensor</strong>, with dimensions <span class="math inline">\(Z \times M\)</span>, where <span class="math inline">\(Z\)</span> is the number of states and <span class="math inline">\(M\)</span> is the number of actions.</p>
<div id="ea321a79" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">2</span>,<span class="dv">2</span>)  <span class="co"># random values between 0 and 1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>A policy has to be a probability distribution over actions for each state, <span class="math inline">\(\sum_a X(s, a) = 1\)</span>. To ensure this, we normalize the policy tensor,</p>
<div id="5a0db777" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X <span class="op">/</span> X.<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>and test, if the policy is a valid probability distribution by asserting that the sum of all probabilities over actions is equal to one,</p>
<div id="bcec9343" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.allclose(X.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>), <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Together, the policy tensor looks as follows:</p>
<div id="910ba160" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>X</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>array([[0.32878922, 0.67121078],
       [0.9813571 , 0.0186429 ]])</code></pre>
</div>
</div>
<p>We convert this logic into a Python function, that returns a random policy for a given number of states and actions,</p>
<div id="ba4e546b-62af-41e9-a2e6-857704f72009" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_policy(Z<span class="op">=</span><span class="dv">2</span>, M<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.rand(Z,M)  <span class="co"># random values</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X<span class="op">/</span>X.<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="co"># normalize values, such that</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.allclose(X.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>), <span class="fl">1.0</span>) <span class="co"># X is a proper probability distribution</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>For example, a random policy for our example MDP with two states and two actions looks as follows:</p>
<div id="5e5145eb-db83-46dd-b12d-57afaaf78fa6" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>random_policy(M<span class="op">=</span><span class="dv">2</span>, Z<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>array([[0.28261752, 0.71738248],
       [0.55010153, 0.44989847]])</code></pre>
</div>
</div>
<p>This completes all definitions required for an MDP. We can now simulate the MDP by iterating over time steps and applying the policy to the current state.</p>
</section>
</section>
<section id="simulation" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="simulation"><span class="header-section-number">6.3</span> Simulation</h2>
<section id="stochastic-simulation" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-simulation">Stochastic simulation</h3>
<p>As in the case of <a href="./02.04-StateTransitions.html">Markov chains</a>, we can simulate the MDP by drawing random numbers. We draw the actions according to the policy and the next state according to the transition probabilities. The reward is then a result of the current state, the current action, and the next state. We implement this as a Python function that takes the transition tensor, the reward tensor, the policy tensor, the initial state, and the number of time steps as input arguments.</p>
<div id="2a7cf2af" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_markov_decision_process(TransitionTensor, RewardTensor, Policy, InitialState, NrTimeSteps):   </span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    state_trajectory <span class="op">=</span> []</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    reward_trajectory <span class="op">=</span> []</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> InitialState</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, NrTimeSteps):</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Choose random action according to policy:</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> np.random.choice([f, r], p<span class="op">=</span>Policy[state]) </span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transition to new state:</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        state_ <span class="op">=</span> np.random.choice([p, d], p<span class="op">=</span>TransitionTensor[state][action]) </span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Record reward:</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> RewardTensor[state, action, state_] </span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update state:</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> state_  </span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store in trajectories</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>        state_trajectory.append(state)<span class="op">;</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        reward_trajectory.append(reward) </span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(state_trajectory), np.array(reward_trajectory)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We execute the simulation and visualize the time-evolution of the MDP’s environmental state and agent’s rewards.</p>
<div id="c2845b61" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1818</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>state_trajectory, reward_trajectory <span class="op">=</span> simulate_markov_decision_process(T, R, X, <span class="dv">0</span>, <span class="dv">500</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(<span class="dv">1</span><span class="op">-</span>np.array(state_trajectory), ls<span class="op">=</span><span class="st">'-'</span>, marker<span class="op">=</span><span class="st">'.'</span>, color<span class="op">=</span><span class="st">'Darkblue'</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(reward_trajectory, color<span class="op">=</span><span class="st">'Red'</span>)<span class="op">;</span> axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Environment'</span>)<span class="op">;</span> axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Rewards'</span>)<span class="op">;</span> axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Time steps'</span>)<span class="op">;</span> plt.tight_layout()<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03.01-SequentialDecisions_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We observe the same stochastic nature of the simulation as with Markov chains. Furthermore, the agent’s rewards fluctuate over time, depending on the environmental state and the agent’s actions. The agent’s rewards are higher in the prosperous state and lower in the degraded state.</p>
<p>Can we make sense of the stochasticity by computing averages over many simulations?</p>
</section>
<section id="ensemble-simulation" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-simulation">Ensemble simulation</h3>
<p>Let’s repeat the previous simulation to create an <strong>ensemble of stochastic simulation runs</strong>. Let’s assume we want an ensemble of 250 runs.</p>
<div id="926365cc" class="cell" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>state_ensemble <span class="op">=</span> []</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>reward_ensemble <span class="op">=</span> []</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">250</span>):</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    state_trajectory, reward_trajectory <span class="op">=\</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>        simulate_markov_decision_process(T, R, X, <span class="dv">0</span>, <span class="dv">500</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    state_ensemble.append(state_trajectory)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    reward_ensemble.append(reward_trajectory)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>state_ensemble <span class="op">=</span> np.array(state_ensemble)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>reward_ensemble <span class="op">=</span> np.array(reward_ensemble)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>It is always a good idea to investigate the object one has just created for consistency, for instance, checking the shape of the ensemble.</p>
<div id="964b8765" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(state_ensemble.shape, reward_ensemble.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>(250, 500) (250, 500)</code></pre>
</div>
</div>
<p>For each ensembel, the first dimension of the ensemble is the number of runs, the second dimension is the number of time steps.</p>
<p>Visualizing the ensemble by takeing the mean over the first dimension (using <code>ensemble.mean(axis=0)</code>),</p>
<div id="cell-fig-mdp-ensemble" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(<span class="dv">1</span><span class="op">-</span>state_ensemble.mean(<span class="dv">0</span>), ls<span class="op">=</span><span class="st">'-'</span>, marker<span class="op">=</span><span class="st">'.'</span>, color<span class="op">=</span><span class="st">'Darkblue'</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(reward_ensemble.mean(<span class="dv">0</span>), color<span class="op">=</span><span class="st">'Red'</span>)<span class="op">;</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Environment'</span>)<span class="op">;</span> axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Rewards'</span>)<span class="op">;</span> </span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylim(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">1.1</span>)<span class="op">;</span> axes[<span class="dv">1</span>].set_ylim(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">1.1</span>)<span class="op">;</span> axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Time steps'</span>)<span class="op">;</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div id="fig-mdp-ensemble" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mdp-ensemble-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03.01-SequentialDecisions_files/figure-html/fig-mdp-ensemble-output-1.png" id="fig-mdp-ensemble" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-mdp-ensemble-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-mdp-ensemble" class="quarto-xref">Figure&nbsp;<span>6.2</span></a> shows the ensemble average of the environmental state and the agent’s rewards over time. The ensemble average is smoother than the individual runs, indicating that the stochasticity averages out over many runs. This observation suggests that we can work with the MDP in the same way as a Markov chain, simulating the time evolution of the state <strong>distribution</strong> directly.</p>
</section>
<section id="distribution-trajectory" class="level3">
<h3 class="anchored" data-anchor-id="distribution-trajectory">Distribution trajectory</h3>
<p>We realize that the MDP’s transition tensor can be reduced to a Markov Chain’s transition matrix when we fix the agent’s policy:</p>
<p><span class="math display">\[
T_\mathbf{x}(s, s') := \sum_{a \in \mathcal A} x(s, a) T(s, a, s')
\]</span></p>
<p>In Python, we use the <code>einsum</code> function for that, since it gives us full control over which indices we want to execute the summation:</p>
<div id="1e28065f-5621-49d4-a990-539bbd5248a2" class="cell" data-tags="[]" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>s, a, s_ <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>Tss <span class="op">=</span> np.einsum(X, [s,a],     <span class="co"># first object with indices</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>                T, [s,a,s_],  <span class="co"># second object with indices</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>                [s,s_])       <span class="co"># indices of the output</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>Tss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>array([[0.96643946, 0.03356054],
       [0.02453393, 0.97546607]])</code></pre>
</div>
</div>
<p>With the effective Markov chain transition matrix, we use the matrix update derived in <a href="./02.04-StateTransitions.html">02.04-StateTransitions</a> to simulate how the state distribution evolves.</p>
<div id="5bd91237-0be8-44ef-a25d-e17cecffb058" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="30">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>p_trajectory <span class="op">=</span> []</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    ps <span class="op">=</span> ps <span class="op">@</span> Tss</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    p_trajectory.append(ps)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>p_trajectory <span class="op">=</span> np.array(p_trajectory)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The trajectory of the state distribution has the number of time steps as the first dimension and the number of states as the second dimension.</p>
<div id="64ea3fa8" class="cell" data-execution_count="31">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>p_trajectory.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>(500, 2)</code></pre>
</div>
</div>
<p>Visualizing the state distribution evolution together with the ensemble average reveals a close resembles between the two.</p>
<div id="13cb38c6-2be6-470c-a640-e45d02ccf059" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="32">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>state_ensemble.mean(<span class="dv">0</span>), ls<span class="op">=</span><span class="st">'-'</span>, marker<span class="op">=</span><span class="st">'.'</span>, color<span class="op">=</span><span class="st">'Darkblue'</span>,  label<span class="op">=</span><span class="st">'Ensemble average'</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>plt.plot(p_trajectory[:, <span class="dv">0</span>], ls<span class="op">=</span><span class="st">'-'</span>, marker<span class="op">=</span><span class="st">'.'</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'State distribution'</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Time steps $t$'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'Average system state $s$'</span>)<span class="op">;</span> plt.legend()<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03.01-SequentialDecisions_files/figure-html/cell-33-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>To compute the average reward trajectory over time, we use the same logic as for the state distribution trajectory, <span class="math inline">\(p_t(s)\)</span>. We compute the reward distribution by summing over the state dimension, weighted by the state distribution,</p>
<p><span class="math display">\[
\langle R_t \rangle_\mathbf{x}= \mathbb E_\mathbf{x}[r_t] = \sum_{s \in \mathcal S} \sum_{a \in \mathcal A} \sum_{s' \in \mathcal{S}}  p_t(s) x(s,a)T(s,a,s')R(s,a,s'),
\]</span></p>
<p>where <span class="math inline">\(\mathbb E_\mathbf{x}[\cdot]\)</span> denotes the expected value of a random variable <span class="math inline">\(\cdot\)</span> given the agent follows policy <span class="math inline">\(\mathbf x\)</span>.</p>
<p>You see how, in this equation on the right hand side, the information flows from the left to the right. The state distribution <span class="math inline">\(p_t(s)\)</span> is multiplied with the policy <span class="math inline">\(x(s,a)\)</span> to get the probability of taking action <span class="math inline">\(a\)</span>. This probability is then multiplied with the transition probability <span class="math inline">\(T(s,a,s')\)</span> to get the probability of transitioning to state <span class="math inline">\(s'\)</span>. Finally, this probability is multiplied with the reward <span class="math inline">\(R(s,a,s')\)</span> to get the expected reward.</p>
<p>We use the <code>einsum</code> function to convert this logic into Python,</p>
<div id="248f9cc0-6628-4f4c-b89b-666bfe5fb3fb" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>s, a, s_, t <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.einsum(p_trajectory, [t, s],</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>              X, [s,a],     </span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>              R, [s,a,s_],</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>              T, [s,a,s_],  </span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>              [t])  <span class="co"># output only in time dimension</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We check that the average-reward trajectory is only a one-dimensional array, with the number of timesteps as the first dimension.</p>
<div id="3f9990ab" class="cell" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>r.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>(500,)</code></pre>
</div>
</div>
<p>Visualizing the average-reward distribution evolution together with the ensemble average reveals a close resemblance between the two.</p>
<div id="27341819-84d1-49c7-9ff7-bdc0af07214a" class="cell" data-tags="[]" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>plt.plot(reward_ensemble.mean(<span class="dv">0</span>), color<span class="op">=</span><span class="st">'Red'</span>, label<span class="op">=</span><span class="st">'Ensemble average'</span>)<span class="op">;</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>plt.plot(r, ls<span class="op">=</span><span class="st">'-'</span>, marker<span class="op">=</span><span class="st">'.'</span>, color<span class="op">=</span><span class="st">'pink'</span>, label<span class="op">=</span><span class="st">'Average rewards'</span>)<span class="op">;</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Timesteps $t$'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'Rewards $R$'</span>)<span class="op">;</span> plt.legend()<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03.01-SequentialDecisions_files/figure-html/cell-36-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Thus, we can also calculate the stationary distribution of an MDP given a policy <span class="math inline">\(\mathbf x\)</span> in the same way as for a <a href="./02.04-StateTransitions.html">Markov chain</a>.</p>
</section>
</section>
<section id="goals-and-values" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="goals-and-values"><span class="header-section-number">6.4</span> Goals and values</h2>
<p>In the Markov Decision Process framework, the agent’s <strong>purpose</strong> or goal is formalized within the <em>reward signal</em>, flowing from the environment to the agent <span class="citation" data-cites="SuttonBarto2018">(<a href="References.html#ref-SuttonBarto2018" role="doc-biblioref">Sutton &amp; Barto, 2018</a>)</span>. At each time step, the agent the reward is represented by a single number <span class="math inline">\(R_t \in \mathbb R\)</span>. Informally, the agent’s goal is to maximize the total amount of reward it receives over time. This may entail choosing actions that yield less immediate rewards to get more rewards in the future.</p>
<p>Representing the agent’s goal by a series of single numbers might seem limiting. However, in practice, it has proven itself flexible and widely applicable. It also aligns well with the unidimensional concepts of <strong>utility</strong> in economics <span class="citation" data-cites="SchultzEtAl2017">(<a href="References.html#ref-SchultzEtAl2017" role="doc-biblioref">Schultz et al., 2017</a>)</span> and <strong>fitness</strong> in biological or cultural evolution.</p>
<!-- primary vs. secondary rewards  [@HoffmanYoeli2022] 
use reward function to encode goal, behavior follows
-->
<!-- reward are not necessarly physical quantities which we can measure -->
<section id="goal-functions" class="level3">
<h3 class="anchored" data-anchor-id="goal-functions">Goal functions</h3>
<p>How do we translate our informal definition of the agent’s goal as maximizing the total amount of reward into a formal mathematical equation?</p>
<p><strong>Finite-horizon goal.</strong> The simplest case for a goal function <span class="math inline">\(G_t\)</span> is to sum up all rewards the agent receives from timestep <span class="math inline">\(t\)</span> onwards until the final time step <span class="math inline">\(T\)</span>, <span class="math display">\[
G_t := R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T = \sum_{\tau=t+1}^T R_\tau.
\]</span></p>
<p>This definition makes sense only if we have a clearly defined final state, such as the end of a board game, the completion of an individual project, or the end of an individual’s life. However, in human-environment interaction in the context of sustainability transitions, we are interested in the long-term future without a clear final state. In these cases, we cannot use the goal definition from above as with <span class="math inline">\(T=\infty\)</span>, the sum <span class="math inline">\(G_t\)</span> itself could easily be infinite for multiple reward sequences, which would leave the agent without guidance on which reward sequence yields a higher <span class="math inline">\(G_t\)</span> and, hence, what to do.</p>
<p><em>For example</em>, on average, our ensemble of stochastic simulations yields a total finite-horizon gain of</p>
<div id="23e6dfa0" class="cell" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>reward_ensemble.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).mean()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>197.87679999999997</code></pre>
</div>
</div>
<p><strong>Discounted goal.</strong> We solve this problem of diverging gains <span class="math inline">\(G_t\)</span> with the concept of <strong>temporal discounting</strong>. We revise our informal definition of the agent’s goal: The agent tries to select actions to maximize the sum of discounted future rewards <span class="citation" data-cites="SuttonBarto2018">(<a href="References.html#ref-SuttonBarto2018" role="doc-biblioref">Sutton &amp; Barto, 2018</a>)</span>. The goal function then becomes,</p>
<p><span class="math display">\[
G_t := R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{\tau=t}^\infty \gamma^\tau R_{t+\tau+1},
\]</span></p>
<p>where <span class="math inline">\(\gamma \in [0, 1)\)</span> is the <strong>discount factor</strong>. The discount factor determines how much the agent cares about future rewards. A discount factor of <span class="math inline">\(\gamma=0\)</span> means that the agent only cares about the immediate reward, while as the discount factor approaches <span class="math inline">\(\gamma \rightarrow 1\)</span>, the agent takes future rewards into account more strongly and becomes more farsighted.</p>
<p><em>For example,</em> the discounted gain with a discount factor of <span class="math inline">\(\gamma=0.9\)</span> of the last ensemble run is</p>
<div id="713edf14" class="cell" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">sum</span>([<span class="fl">0.9</span><span class="op">**</span>t <span class="op">*</span> reward_ensemble[<span class="op">-</span><span class="dv">1</span>, t] <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>9.254059133343878</code></pre>
</div>
</div>
<p>Temporal discounting is a widely used concept in (environmental) economics, psychology, and neuroscience to model human decision-making. It implies that welfare experienced in the future is worth less to the agent than the same amount of welfare experienced now. This concept is used both as a <strong>normative</strong> and <strong>descriptive</strong> model of decision-making.</p>
<p>One reason for temporal discounting is the <strong>uncertainty</strong> about the future. The future is uncertain, and the agent might not be around to experience future rewards. In fact, <span class="math inline">\(\gamma\)</span> can be interpreted as the probability that the agent will be around to experience future rewards.</p>
<p>The primary value of temporal discounting and the discount factor for us in our quest to develop integrated system models of human-environment interactions is its ability to <strong>model the trade-off between present and future welfare</strong>. This trade-off is at the heart of many sustainability transitions, such as the trade-off between short-term economic gains and long-term environmental degradation.</p>
<p><em>For example,</em> let’s assume the agent receives a constant reward stream of <span class="math inline">\(R_t=1\)</span> for all timesteps <span class="math inline">\(t\)</span>. We compare the so-called (net) present value at timestep <span class="math inline">\(t\)</span> for different discount factors <span class="math inline">\(\gamma\)</span>. We also compute the sum of the discounted rewards for the infinite future, <span class="math inline">\(G_t\)</span>.</p>
<div id="cell-fig-discountfactors" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> discountfactor <span class="kw">in</span> [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="fl">0.99</span>]:</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    summands <span class="op">=</span> [discountfactor<span class="op">**</span>t <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>)]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    plt.plot(summands, label<span class="op">=</span>discountfactor)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    total_value <span class="op">=</span> np.<span class="bu">sum</span>(summands)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Discount factor </span><span class="sc">{dcf:3.2f}</span><span class="st">: Total </span><span class="sc">{total:5.1f}</span><span class="st">"</span><span class="op">\</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>        .<span class="bu">format</span>(dcf<span class="op">=</span>discountfactor, total<span class="op">=</span>total_value))</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> plt.ylabel(<span class="st">'Present value'</span>)<span class="op">;</span> plt.xlabel(<span class="st">'Timestep'</span>)<span class="op">;</span> </span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>,<span class="dv">100</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Discount factor 0.10: Total   1.1
Discount factor 0.50: Total   2.0
Discount factor 0.90: Total  10.0
Discount factor 0.99: Total 100.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-discountfactors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-discountfactors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03.01-SequentialDecisions_files/figure-html/fig-discountfactors-output-2.png" id="fig-discountfactors" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-discountfactors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3
</figcaption>
</figure>
</div>
</div>
</div>
<p>Here, we used the Python string method <code>format</code> to print the results in a readable way. It can be used to insert variables into a string. The curly brackets <code>{}</code> are placeholders for the variables, and the variables are passed to the <code>format</code> method as arguments. The colon <code>:</code> inside the curly brackets is used to format the output. For example, <code>:3.2f</code> formats the number as a floating-point number with three digits before and two digits after the decimal point.</p>
<p><strong><em>Normalized</em> goal</strong>. To account for the fact that the total value depends on the level of discounting, even if the reward stream is constant, we can normalize the goal as follows,</p>
<p><span class="math display">\[ G_t = (1-\gamma) \sum_{\tau=t}^\infty \gamma^\tau R_{t+\tau+1},\]</span></p>
<p>where <span class="math inline">\(1-\gamma\)</span> is a normalizing factor and <span class="math inline">\(R_{t+\tau+1}\)</span> is the reward received at time step <span class="math inline">\(t+\tau+1\)</span>.</p>
<div id="254f83c3-39b5-4c2b-8fe3-3a34fe716e9e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dcf <span class="kw">in</span> [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="fl">0.99</span>]:</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    summands <span class="op">=</span> [dcf<span class="op">**</span>t <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>)]</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    normalizing <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>dcf</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    total_value <span class="op">=</span> normalizing <span class="op">*</span> np.<span class="bu">sum</span>(summands)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Discount factor </span><span class="sc">{dcf:3.2f}</span><span class="st">: Total </span><span class="sc">{total:5.1f}</span><span class="st">"</span>.<span class="bu">format</span>(dcf<span class="op">=</span>dcf, total<span class="op">=</span>total_value))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Discount factor 0.10: Total   1.0
Discount factor 0.50: Total   1.0
Discount factor 0.90: Total   1.0
Discount factor 0.99: Total   1.0</code></pre>
</div>
</div>
<p>With normalization, the discount factor parameter <span class="math inline">\(\gamma\)</span> expresses how much the agent <strong>cares for the future</strong> without influencing the scale of the total value. That way, the outcomes of different discount factors can be <strong>compared</strong> with each other.</p>
<p><em>For example,</em> the normalized discounted gain with a discount factor of <span class="math inline">\(\gamma=0.9\)</span> of the last ensemble run is</p>
<div id="953607a2" class="cell" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>dcf <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="op">-</span>dcf) <span class="op">*</span> np.<span class="bu">sum</span>([dcf<span class="op">**</span>t <span class="op">*</span> reward_ensemble[<span class="op">-</span><span class="dv">1</span>, t] <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>0.9254059133343876</code></pre>
</div>
</div>
<p><strong>Bellman equation</strong>. Regardless of the goal formulation, the agent’s gains <span class="math inline">\(G_t\)</span> at successive time steps relate to each other in an important way:</p>
<p><span class="math display">\[\begin{align}
G_t &amp;= (1-\gamma) \sum_{\tau=t}^\infty \gamma^\tau R_{t+\tau+1}\\
   &amp;= (1-\gamma) \left(R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} \cdots \right)\\
   &amp;= (1-\gamma) \left(R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} \cdots) \right)\\
   &amp;= (1-\gamma) R_{t+1} + \gamma (1-\gamma) (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} \cdots) \\
   &amp;= (1-\gamma) R_{t+1} + \gamma G_{t+1},
\end{align}\]</span></p>
<p>The gain <span class="math inline">\(G_t\)</span> is composed of the current short-term reward and the (discounted) value of the future gains. This recursive relationship is known as the <strong>Bellman equation</strong> and is the foundation of many solution methods for MDPs, such as dynamic programming and reinforcement learning.</p>
<p><em>For example,</em> we can test the Bellman equation by comparing the gain at time step <span class="math inline">\(t\)</span> with the short-term reward at time step <span class="math inline">\(t\)</span> and the gain at time step <span class="math inline">\(t+1\)</span>.</p>
<div id="dcada212" class="cell" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>dcf <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>G0 <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>dcf) <span class="op">*</span> np.<span class="bu">sum</span>([dcf<span class="op">**</span>t <span class="op">*</span> reward_ensemble[<span class="op">-</span><span class="dv">1</span>, t] <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">500</span>)])</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>G1 <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>dcf) <span class="op">*</span> np.<span class="bu">sum</span>([dcf<span class="op">**</span>t <span class="op">*</span> reward_ensemble[<span class="op">-</span><span class="dv">1</span>, t<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">499</span>)])</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>np.allclose((<span class="dv">1</span><span class="op">-</span>dcf) <span class="op">*</span> reward_ensemble[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>] <span class="op">+</span> dcf <span class="op">*</span> G1, G0)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>True</code></pre>
</div>
</div>
<p>Why does that work even though we have a finite time horizon of 500 simulation timesteps here? From <a href="#fig-discountfactors" class="quarto-xref">Figure&nbsp;<span>6.3</span></a> we observe that for a discount factor <span class="math inline">\(\gamma=0.9\)</span>, the contributions of rewards for timesteps above <span class="math inline">\(t&gt;100\)</span> are practically zero. So, with a simulation time of 500 timesteps, we are well above the time horizon the agent cares about. This example illustrates not only the power of the Bellman equation. It also shows how a discount factor induces a timescale the agent cares about.</p>
<p>Goals or gains are defined over individual reward streams or trajectories. These may be stochastic beyond the agent’s control. Therefore, the agent’s course of action should consider the <strong>expected</strong> gains, i.e., the average gains over all possible reward streams, given a policy <span class="math inline">\(\mathbf x\)</span>.</p>
</section>
<section id="value-functions" class="level3">
<h3 class="anchored" data-anchor-id="value-functions">Value functions</h3>
<p><strong>Value functions are defined to be the expected gain <span class="math inline">\(G_t\)</span> for a policy <span class="math inline">\(\mathbf x\)</span>, given a state or state-action pair.</strong> They are helpful in finding a good policy since the best policy will yield the highest value.</p>
<p>Given a policy <span class="math inline">\(\mathbf x\)</span>, we define the <strong>state value</strong>, <span class="math inline">\(v_{\mathbf x}(s)\)</span>, as the expected gain, <span class="math inline">\(\mathbb E_\mathbf{x}[ G_t | S_t = s]\)</span>, when starting in state <span class="math inline">\(s\)</span> and the following the policy <span class="math inline">\(\mathbf x\)</span>,</p>
<p><span class="math display">\[
v_\mathbf{x}(s) := \mathbb E_\mathbf{x}[ G_t | S_t = s] = (1-\gamma) \mathbb E_\mathbf{x}\left[\sum_{\tau=t}^\infty \gamma^\tau R_{t+\tau+1} | S_t = s\right], \quad \text{for all } s \in \mathcal S,
\]</span></p>
<p>Analogously, we define the <strong>state-action value</strong>, <span class="math inline">\(q_\mathbf{x}(s, a)\)</span>, as the expected gain when starting in state <span class="math inline">\(s\)</span> and executing action <span class="math inline">\(a\)</span>, and from then on following policy <span class="math inline">\(\mathbf x\)</span>,</p>
<p><span class="math display">\[
q_\mathbf{x}(s, a) := \mathbb E_X [G(t) | s(t) = s, a(t)=a].
\]</span></p>
<p><span class="math display">\[
q_\mathbf{x}(s, a) := \mathbb E_\mathbf{x}[ G_t | S_t = s, A_t = a] = (1-\gamma) \mathbb E_\mathbf{x}\left[\sum_{\tau=t}^\infty \gamma^\tau R_{t+\tau+1} | S_t = s, A_t = a\right], \quad \text{for all } s \in \mathcal S, a \in \mathcal A.
\]</span></p>
<p><strong>How is that useful?</strong></p>
<ol type="1">
<li><p><strong>State values let us compare strategies.</strong> A strategy <span class="math inline">\(\mathbf x\)</span> is better than a strategy <span class="math inline">\(\mathbf y\)</span> iff for all states <span class="math inline">\(s\)</span>: <span class="math inline">\(v_\mathbf{x}(s) &gt; v_\mathbf{y}(s)\)</span>.</p></li>
<li><p><strong>The best strategy yields the highest value</strong>. At least one strategy is always better than or equal to all other strategies. That is an <em>optimal strategy</em> <span class="math inline">\(\mathbf x_*\)</span> with the <em>optimal state value</em> <span class="math inline">\(v_*(s) := \max_\mathbf{x} v_\mathbf{x}(s), \forall s\)</span>.</p></li>
<li><p><strong>Highest state-action values indicate the best action</strong>. If we knew the <em>optimal state-action value</em>, <span class="math inline">\(q_*(s, a) := \max_\mathbf{x} q_\mathbf{x}(s,a), \forall s,a\)</span>, we can simply assign nonzero probability at each state <span class="math inline">\(s\)</span> only to actions which yield maximum value, <span class="math inline">\(\max_{\tilde a} q_*(s, \tilde a)\)</span>.</p></li>
</ol>
<p>The <strong>beauty</strong> of state(-action) <strong>values</strong>, in general, and optimal state(-action) values, in particular, is that they <strong>encapsulate all relevant information about future environmental dynamics</strong> with all inherent stochasticity <strong>into short-term actionable numbers</strong>. <em>Relevant</em> means relevant to the agent regarding its goal function. State-action values represent the short-term consequences of actions in each state regarding the long-term goal. Optimal state-action values allow for selecting the best actions, irrespective of knowing potential successor states and their values or any details about environmental dynamics. Having such values would save the agent enormous cognitive computational demands every time it must make a decision.</p>
<p>The only problem we are left with is, how to compute a policy’s state(-action) values?</p>
</section>
<section id="bellman-equation" class="level3">
<h3 class="anchored" data-anchor-id="bellman-equation">Bellman equation</h3>
<p>We convert the recursive relationship of the goal function (<span class="quarto-unresolved-ref">?eq-bellman1</span>) to state values,</p>
<p><span class="math display">\[\begin{align}
v_\mathbf{x}(s) &amp;= \mathbb E_\mathbf{x} [G_t | S_t = s] \\
&amp;= \mathbb E_\mathbf{x} \left[ (1-\gamma) R_{t+1} + \gamma G_{t+1} | S_t = s \right] \\
&amp;= (1-\gamma) \mathbb E_\mathbf{x}[ R_{t+1} | S_t = s] + \gamma \mathbb E_\mathbf{x}[G_{t+1} | S_{t+1} = s' ] \\
&amp;= (1-\gamma) R_\mathbf{x}(s) + \gamma \sum_{s'} T_\mathbf{x}(s,s') v_\mathbf{x}(s'),
\end{align}\]</span></p>
<p>where <span class="math inline">\(R_\mathbf{x}(s)\)</span> is the expected reward in state <span class="math inline">\(s\)</span> under policy <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(T_\mathbf{x}(s,s')\)</span> is the expected transition probability from state <span class="math inline">\(s\)</span> to state <span class="math inline">\(s'\)</span> under policy <span class="math inline">\(\mathbf x\)</span>.</p>
<p>The expected state reward <span class="math inline">\(R_\mathbf{x}(s)\)</span> is given by</p>
<p><span class="math display">\[R_\mathbf{x}(s) = \sum_{a \in \mathcal A} \sum_{s' \in \mathcal{S}}  x(s,a)T(s,a,s')R(s,a,s'),\]</span></p>
<p>which can be neatly translated into Python using the <code>numpy.einsum</code> method.</p>
<div id="ffaaaa28" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>s, a, s_ <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>  <span class="co"># defining indices for convenicence</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>Rs <span class="op">=</span> np.einsum(X, [s, a], T, [s, a, s_], R, [s, a, s_], [s])<span class="op">;</span> Rs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>array([0.90068162, 0.        ])</code></pre>
</div>
</div>
<p>The recursive equation is called the <em>Bellman equation</em> in honor of Richard Bellman and his pioneering work (Bellman 1957). The recursive relationship is exploited in several algorithmic ways to compute the values or even approximate the optimal state values. In recent years, it became possible to approximate optimal state values with deep neural networks, a technique known as <em>deep reinforcement learning</em> <span class="citation" data-cites="MnihEtAl2015">(<a href="References.html#ref-MnihEtAl2015" role="doc-biblioref">Mnih et al., 2015</a>)</span>, allowing for solving high-dimensional MDPs with many - even infinitely many - states and actions. This is a fascinating field of research, which we will not cover in this course. I recommend the interested reader to start from the excellent <span class="citation" data-cites="SuttonBarto2018">(introduction to reinforcement learning by <a href="References.html#ref-SuttonBarto2018" role="doc-biblioref">Sutton &amp; Barto, 2018</a>)</span>.</p>
<p>Our focus lies on a <strong>transparent way of modeling</strong> human-environment interactions. We use MDPs as a framework to improve our conceptual understanding of decision-making under uncertainty. Specifically, we exemplify that with the trade-off between short-term and long-term welfare.</p>
<p>Using an MDP framework, our models are formulated in a way that - in principle - can scale to high-dimensional systems. The trade-off is, however, the computational cost of solving high-dimensional MDPs. The more complex and “realistic” a model, the less we can understand how the outcome depends on the model’s specifications.</p>
<p>As these model specifications are often highly uncertain in the context of sustainability and global change <span class="citation" data-cites="PolaskyEtAl2011">(<a href="References.html#ref-PolaskyEtAl2011" role="doc-biblioref">Polasky et al., 2011</a>)</span>, it is very <strong>likely that we end up with an optimal policy for a wrong model</strong> that is not useful for decision-making. It might even be harmful, conveying a false sense of optimality. This problem gets worse with the complexity of the model. The more model parameters we have to specify as the input to the model, the more sources of possible but unconscious uncertainty there is.</p>
<p>Therefore, we will focus on minimalistic models but take a radical stance to account for parameter uncertainty to keep the analysis and interpretation transparent. Thus, in the following, we derive an analytical expression how to compute the state values for a given policy.</p>
<p>We write the Bellman equation in matrix form,</p>
<p><span class="math display">\[
\mathbf v_\mathbf{x} = (1-\gamma) \mathbf R_\mathbf{x} + \gamma \underline{\mathbf T}_\mathbf{x} \mathbf v_\mathbf{x}
\]</span></p>
<p>where <span class="math inline">\(\mathbf R_\mathbf{x}\)</span> is the vector of expected state rewards <span class="math inline">\(R_\mathbf{x}(s)\)</span>, <span class="math inline">\(\underline{\mathbf T}_\mathbf{x}\)</span> is the transition matrix, and <span class="math inline">\(\mathbf v_\mathbf{x}\)</span> is the vector of state values under policy <span class="math inline">\(\mathbf x\)</span>. Thus, <span class="math inline">\(\mathbf v_\mathbf{x}\)</span> and <span class="math inline">\(\mathbf R_\mathbf{x}\)</span> are vectors of dimension <span class="math inline">\(Z\)</span>, i.e., the number of states, and <span class="math inline">\(\underline{\mathbf T}_\mathbf{x}\)</span> is the transition matrix of dimension <span class="math inline">\(Z \times Z\)</span>.</p>
<p>We can solve this equation for <span class="math inline">\(\mathbf v_\mathbf{x}\)</span>,</p>
<p><span class="math display">\[\begin{align}
\mathbf v_\mathbf{x} &amp;= (1-\gamma) \mathbf R_\mathbf{x} + \gamma \underline{\mathbf T}_\mathbf{x} \mathbf v_\mathbf{x} \\
\mathbf v_\mathbf{x} - \gamma \underline{\mathbf T}_\mathbf{x} \mathbf v_\mathbf{x} &amp;= (1-\gamma) \mathbf R_\mathbf{x} \\
(\mathbb 1_Z - \gamma\underline{\mathbf T}_\mathbf{x}) \mathbf v_\mathbf{x} &amp;= (1-\gamma) \mathbf R_\mathbf{x} \\
(\mathbb 1_Z - \gamma\underline{\mathbf T}_\mathbf{x})^{-1} (\mathbb 1_Z - \gamma\underline{\mathbf T}_\mathbf{x}) \mathbf v_\mathbf{x} &amp;= (1-\gamma) (\mathbb 1_Z - \gamma\underline{\mathbf T}_\mathbf{x})^{-1} \mathbf R_\mathbf{x} \\
\mathbf v_\mathbf{x} &amp;= (1-\gamma) (\mathbb 1_Z - \gamma\underline{\mathbf T}_\mathbf{x})^{-1} \mathbf R_\mathbf{x}, \\
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbb 1_Z\)</span> is the identity matrix of dimension <span class="math inline">\(Z\)</span>.</p>
<p>Thus, to compute state value, we must invert a <span class="math inline">\(Z\times Z\)</span>-matrix, which is computationly infeasable for large MDPs. For low-dimensional models, however, it works perfectly fine and can even be executed analytically.</p>
<p>In Python, an identity matrix can be created with the <code>eye</code> function from the <code>numpy</code> package.</p>
<div id="28ecaba0" class="cell" data-execution_count="43">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>np.eye(<span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>array([[1., 0.],
       [0., 1.]])</code></pre>
</div>
</div>
<p>We define a function to compute the state values given a policy, a transition tensor, a reward tensor, and a discount factor. The function returns a vector of state values. We use the <code>inv</code> function from the <code>numpy.linalg</code> package to invert the matrix.</p>
<div id="3990d027" class="cell" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_statevalues(</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    policy_Xsa, transitions_Tsas, rewards_Rsas, discountfactor):</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    s, a, s_ <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>  <span class="co"># defining indices for convenicence</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    Tss <span class="op">=</span> np.einsum(policy_Xsa, [s, a], transitions_Tsas, [s, a, s_], [s,s_])</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    Rs <span class="op">=</span> np.einsum(policy_Xsa, [s, a], transitions_Tsas, [s, a, s_], </span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>                   rewards_Rsas, [s, a, s_], [s])</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    inv <span class="op">=</span> np.linalg.inv((np.eye(<span class="dv">2</span>) <span class="op">-</span> discountfactor<span class="op">*</span>Tss))</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    Vs <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>discountfactor) <span class="op">*</span> np.einsum(inv, [s,s_], Rs, [s_], [s])</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Vs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="5ea396a6" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-execution_count="45">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>Vs <span class="op">=</span> compute_statevalues(X, T, R, <span class="fl">0.9</span>)<span class="op">;</span> Vs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>array([0.7220388 , 0.13059414])</code></pre>
</div>
</div>
<p>Thus, in contrast to the expected state rewards, the long-term value of the degraded state is above the immediate reward of the degraded state, <span class="math inline">\(r_d=0\)</span>.</p>
<div id="40f06b3f" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>Rs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>array([0.90068162, 0.        ])</code></pre>
</div>
</div>
<p>In the state value <span class="math inline">\(v_{\mathbf x}(\mathsf{d})\)</span> of the degraded state, the agent anticipates the recovery of the environment and the return to the prosperous state. Likewise, the agent anticipates the collapse of the environment and the loss of the prosperous state in the state value of the prosperous state. Hence, <span class="math inline">\(v_{\mathbf x}(\mathsf{p})\)</span> is smaller than the expected reward of the prosperous state, <span class="math inline">\(R_{\mathbf x}(\mathsf{p})\)</span>. The expected state rewards only consider the immediate possible transitions, while the state values also account for the long-term consequences of these transitions.</p>
<p><strong>How do these values depend on the discount factor <span class="math inline">\(\gamma\)</span>?</strong></p>
<p>We define an array of linearly spaced values of different discount factors,</p>
<div id="97e817f0" class="cell" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>discountfactors <span class="op">=</span> np.linspace(<span class="fl">0.001</span>, <span class="fl">0.9999</span>, <span class="dv">301</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We then compute the state value for each discount-factor value using a list comprehension,</p>
<div id="f3d869a0" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> np.array([compute_statevalues(X, T, R, dcf) <span class="cf">for</span> dcf <span class="kw">in</span> discountfactors])</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>values.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>(301, 2)</code></pre>
</div>
</div>
<p>We plot the state values along the discount factors on the x-axis. We also include the expected state rewards, which are independent of the discount factor.</p>
<div id="bedbf2e2" class="cell" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>plt.plot(discountfactors, Rs[<span class="dv">0</span>]<span class="op">*</span>np.ones_like(discountfactors), label<span class="op">=</span><span class="st">'$R(p)$'</span>, c<span class="op">=</span><span class="st">'green'</span>, ls<span class="op">=</span><span class="st">'--'</span>)<span class="op">;</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>plt.plot(discountfactors, values[:, <span class="dv">0</span>], label<span class="op">=</span><span class="st">'$v(p)$'</span>, c<span class="op">=</span><span class="st">'green'</span>)<span class="op">;</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>plt.plot(discountfactors, values[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">'$v(d)$'</span>, c<span class="op">=</span><span class="st">'brown'</span>)<span class="op">;</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>plt.plot(discountfactors, Rs[<span class="dv">1</span>]<span class="op">*</span>np.ones_like(discountfactors), label<span class="op">=</span><span class="st">'$R(d)$'</span>, c<span class="op">=</span><span class="st">'brown'</span>, ls<span class="op">=</span><span class="st">'--'</span>)<span class="op">;</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> plt.xlabel(<span class="st">'Discount factor'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'Value'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>Text(0, 0.5, 'Value')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03.01-SequentialDecisions_files/figure-html/cell-50-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>When the discount factor is close to zero, <span class="math inline">\(\gamma=0\)</span>, the values equal the average immediate rewards.</p>
<p>When the discount factor <span class="math inline">\(\gamma\rightarrow 1\)</span>, the state values for the prosperous and the degraded state approach each other.</p>
<p>Last, the state values change more for large <span class="math inline">\(\gamma&gt;0.85\)</span> than for lower <span class="math inline">\(\gamma\)</span>.</p>
<p>So far, we investigated how to compute the state values for a given policy and use a random policy as an example. To eventually answer what the agent should do, we must compare multiple policies and find the best one.</p>
</section>
</section>
<section id="optimal-policies" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="optimal-policies"><span class="header-section-number">6.5</span> Optimal policies</h2>
<p><strong>The key question of our example model is, when is it better to play safe, and when is it better to be risky?</strong> From our model definition, we can easily see that, in the degraded state, it is always better to play safe as this is the only way to recover to the more rewarding, prosperous state. But what about the prosperous state?</p>
<section id="numerical-computation" class="level3">
<h3 class="anchored" data-anchor-id="numerical-computation">Numerical computation</h3>
<p>We define two policies, a <strong>safe</strong> policy, <span class="math inline">\(\mathbf x_{\text{safe}}\)</span>, where the agent always chooses the safe action and a <strong>risky</strong> policy, <span class="math inline">\(\mathbf x_{\text{risky}}\)</span>, where the agent always chooses the risky action in the prosperous state.</p>
<div id="3d92c760-f986-4168-b67f-a345d8ee5818" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>Xsafe <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">0</span>]])</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>Xrisk <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>]])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>For each of these policies, we compute the state values with our <code>compute_statevalues</code> function,</p>
<div id="0d78823f-021b-4927-a989-651cf1ea999a" class="cell" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>V_safe <span class="op">=</span> np.array([compute_statevalues(Xsafe, T, R, dcf) <span class="cf">for</span> dcf <span class="kw">in</span> discountfactors])</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>V_risk <span class="op">=</span> np.array([compute_statevalues(Xrisk, T, R, dcf) <span class="cf">for</span> dcf <span class="kw">in</span> discountfactors])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>and plot these values for each policy and each state as</p>
<div id="7b09dfe4-faf8-4eae-b56a-a71d01bcb4c4" class="cell" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>plt.plot(discountfactors, V_safe[:, <span class="dv">0</span>], label<span class="op">=</span><span class="st">'$v_</span><span class="sc">{safe}</span><span class="st">(p)$'</span>, color<span class="op">=</span><span class="st">'blue'</span>)<span class="op">;</span> </span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>plt.plot(discountfactors, V_safe[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">'$v_</span><span class="sc">{safe}</span><span class="st">(d)$'</span>, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="fl">0.4</span>)<span class="op">;</span> </span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>plt.plot(discountfactors, V_risk[:, <span class="dv">0</span>], label<span class="op">=</span><span class="st">'$v_</span><span class="sc">{risk}</span><span class="st">(p)$'</span>, color<span class="op">=</span><span class="st">'red'</span>)<span class="op">;</span> </span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>plt.plot(discountfactors, V_risk[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">'$v_</span><span class="sc">{risk}</span><span class="st">(d)$'</span>, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="fl">0.4</span>)<span class="op">;</span> </span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> plt.xlabel(<span class="st">'Discount factor'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'Value'</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03.01-SequentialDecisions_files/figure-html/cell-53-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We find a <strong>critical discount factor</strong> <span class="math inline">\(\hat \gamma\)</span>, where the optimal policy changes. Below <span class="math inline">\(\hat\gamma\)</span>, the agent acts optimally by choosing the risky policy. Above the critical discount factor, <span class="math inline">\(\hat\gamma\)</span>, the agent acts optimally by choosing the safe policy.</p>
<p>Hence, when the agent cares enough about the future, it is better to be safe than sorry, even if this means giving up immediate, short-term welfare (<span class="math inline">\(r_s &lt; r_r\)</span>).</p>
<p><strong>But how does this result depend on the other parameters, <span class="math inline">\(p_c, p_r, r_s, r_r, r_d\)</span>?</strong></p>
<p>This investigates how the optimal policy depends on all parameters of the model; we first <strong>define general transition and reward functions</strong> that return a transition and reward tensor, given our model parameters. We make these functions general by passing the most general datatype to the respective <code>numpy.array</code>s, i.e., <code>dtype=object.</code> This allows us to store arbitrary Python objects in the arrays, such as float numbers or symbolic expressions.</p>
<div id="186fac98-cb32-4f2e-946f-d7d935e5a117" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_transitions(pc, pr):</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span><span class="dv">0</span><span class="op">;</span> r<span class="op">=</span><span class="dv">1</span><span class="op">;</span> p<span class="op">=</span><span class="dv">0</span><span class="op">;</span> d<span class="op">=</span><span class="dv">1</span>  <span class="co"># for reference we define these as function-local variables</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> np.zeros((<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">object</span>)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    T[p,c,d] <span class="op">=</span> <span class="dv">0</span>            <span class="co"># Cautious action guarantees prosperous state</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    T[p,c,p] <span class="op">=</span> <span class="dv">1</span>   <span class="co"># </span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    T[p,r,d] <span class="op">=</span> pc<span class="op">;</span>          <span class="co"># Risky action risks collapse</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    T[p,r,p] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>T[p,r,d]   <span class="co"># ... but collapse may not happen</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>    T[d,c,p] <span class="op">=</span> pr           <span class="co"># Recovery only possible with cautious action </span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    T[d,c,d] <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>T[d,c,p]   <span class="co"># ... but recovery might not happen</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>    T[d,r,p] <span class="op">=</span> <span class="dv">0</span>            <span class="co"># Risky action remains at degraded state</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>    T[d,r,d] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> T</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c0fd2091-e1e3-4078-87c3-ecc17b2f7a6c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_rewards(rs, rr<span class="op">=</span><span class="dv">1</span>, rd<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span><span class="dv">0</span><span class="op">;</span> r<span class="op">=</span><span class="dv">1</span><span class="op">;</span> p<span class="op">=</span><span class="dv">0</span><span class="op">;</span> d<span class="op">=</span><span class="dv">1</span>  <span class="co"># for reference we define these as function-local variables</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> np.zeros((<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">object</span>)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    R[p,c,p] <span class="op">=</span> rs            <span class="co"># The cautious action at the prosperous state guarantees the safe reward </span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>    R[p,r,p] <span class="op">=</span> rr            <span class="co"># The risky action can yield the risky reward if the environment remains at p</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    R[d,:,:] <span class="op">=</span> R[:,:,d] <span class="op">=</span> rd <span class="co"># Otherwise, the agent receives rd</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> R</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now, we can create transition and reward tensors flexibly. As we want to perform numerical computations, we specify the data type of the arrays to be float numbers.</p>
<div id="527db0d9-a514-48b0-be74-4a7483f01c96" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="55">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> get_transitions(<span class="fl">0.04</span>, <span class="fl">0.1</span>).astype(<span class="bu">float</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>T</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>array([[[1.  , 0.  ],
        [0.96, 0.04]],

       [[0.1 , 0.9 ],
        [0.  , 1.  ]]])</code></pre>
</div>
</div>
<div id="cfacd92f-5ab0-462c-82ad-d6cd50af72ee" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-execution_count="56">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> get_rewards(<span class="fl">0.7</span>).astype(<span class="bu">float</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>R</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>array([[[0.7, 0. ],
        [1. , 0. ]],

       [[0. , 0. ],
        [0. , 0. ]]])</code></pre>
</div>
</div>
<p>Let’s assume we want to know how the critical discount factor <span class="math inline">\(\hat \gamma\)</span> depends on the collapse probability <span class="math inline">\(p_c\)</span> for a given recovery probability <span class="math inline">\(p_r=0.01\)</span> and safe reward <span class="math inline">\(r_s=0.8\)</span>, a risky reward <span class="math inline">\(r_r = 1.0\)</span> and a degraded reward <span class="math inline">\(r_d=0.0\)</span>. We define these quantities as</p>
<div id="12daeed8-daab-4ed2-b0f2-f54873a34b00" class="cell" data-execution_count="57">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>pr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>rs <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>rr <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>rd <span class="op">=</span> <span class="fl">0.0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>and let the discount factor and collapse probabilities run from almost zero to almost one with a resolution of <code>301</code> elements,</p>
<div id="af69d5a2-85f3-4133-84b5-574f1d2e757e" class="cell" data-execution_count="58">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>discountfactors <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, <span class="dv">301</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>collapseprobabilities <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, <span class="dv">301</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We will go through each combination of discount factors and collapse probabilities, compute the state values for both policies, compare them, and store the result in a <em>data container</em>. We prepare this <em>data container</em> by</p>
<div id="f0caeb5f-72e6-4a4b-bbac-124d27ab6d91" class="cell" data-execution_count="59">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>risky_optimal_data_container <span class="op">=</span> np.zeros((discountfactors.size, collapseprobabilities.size, <span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now, we are ready to execute our simulation. We loop through each discount factor and for each discount factor through each collapse probability, obtain our new transition matrix, compute the state values, and store them in our data container. The Jupyter cell magic <code>%%time</code> shows us how long it took to execute that cell.</p>
<div id="66fd9088-84a3-469e-96ed-8b7014f9c9d4" class="cell" data-execution_count="60">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, dcf <span class="kw">in</span> <span class="bu">enumerate</span>(discountfactors):</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, pc <span class="kw">in</span> <span class="bu">enumerate</span>(collapseprobabilities):</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>        T <span class="op">=</span> get_transitions(pc, pr).astype(<span class="bu">float</span>)</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>        R <span class="op">=</span> get_rewards(rs, rr, rd).astype(<span class="bu">float</span>)</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>        Vs_risk <span class="op">=</span> compute_statevalues(Xrisk, T, R, dcf)</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>        Vs_safe <span class="op">=</span> compute_statevalues(Xsafe, T, R, dcf)</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>        risky_optimal_data_container[i, j, :] <span class="op">=</span> Vs_risk <span class="op">&gt;</span> Vs_safe</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 10.9 s, sys: 1.94 s, total: 12.8 s
Wall time: 8.7 s</code></pre>
</div>
</div>
<p>We noticeably have to wait for the result!</p>
<div id="965ecf21-b385-4076-9520-4539193cf2e1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="61">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">131</span>)<span class="op">;</span> plt.xticks([])<span class="op">;</span> plt.yticks([])<span class="op">;</span> </span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">133</span>)<span class="op">;</span> plt.xticks([])<span class="op">;</span> plt.yticks([])<span class="op">;</span> </span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">132</span>) <span class="co"># just to center the plot in the middle</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>plt.pcolormesh(collapseprobabilities, discountfactors, </span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>               risky_optimal_data_container[:,:,<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'bwr'</span>)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Discount factor'</span>)<span class="op">;</span> plt.xlabel(<span class="st">'Collapse probabiliy'</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03.01-SequentialDecisions_files/figure-html/cell-62-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>The higher the collapse probability, the lower the critical discount factor.</strong> When the collapse is more likely, less future care is required to evaluate the safe policy as optimal. When the collapse probability is zero (<span class="math inline">\(p_c=0\)</span>), the critical discount factor is one (<span class="math inline">\(\hat\gamma=1\)</span>), and the agent should always choose the risky policy, as the environment cannot be destroyed.</p>
<p>When the discount factor is zero (<span class="math inline">\(\gamma=0\)</span>), the critical collapse probability is a half <span class="math inline">\(\hat p_c=0.5\)</span>. If an environmental collapse under the risky action is more likely <span class="math inline">\(p_c &gt; \hat p_c\)</span>, the agent should always choose the safe policy and vice versa. But where does the value <span class="math inline">\(0.5\)</span> come from? Intuitively, it is the ratio between the safe and the risky reward, <span class="math inline">\(r_s/r_r\)</span>.</p>
<p>But how can we be sure? And wouldn’t it be great, if we could speed up the computation time somehow?</p>
<p>The solution to both questions lies in a symbolic computation of the critical parameter values <span class="math inline">\(\hat \gamma, \hat p_c, \hat p_r, \hat r_s, \hat r_r, \hat r_d\)</span>.</p>
</section>
<section id="symbolic-computation" class="level3">
<h3 class="anchored" data-anchor-id="symbolic-computation">Symbolic computation</h3>
<p>We define symbolic expressions for our model parameters and obtain the corresponding transition and reward tensors,</p>
<div id="30ea46fd-7d4d-4102-9b7b-e18a749d3c47" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-execution_count="62">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>pc, pr <span class="op">=</span> sp.symbols(<span class="st">"p_c, p_r"</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> sp.Array(get_transitions(pc, pr))</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>T</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="62">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}\left[\begin{matrix}1 &amp; 0\\1 - p_{c} &amp; p_{c}\end{matrix}\right] &amp; \left[\begin{matrix}p_{r} &amp; 1 - p_{r}\\0 &amp; 1\end{matrix}\right]\end{matrix}\right]\)</span></p>
</div>
</div>
<div id="020bcc2f-a49d-41b8-833b-755874f8008a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-execution_count="63">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>rs, rr, rd <span class="op">=</span> sp.symbols(<span class="st">"r_s r_r r_d"</span>)</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> sp.Array(get_rewards(rs, rr, rd))</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>R</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="63">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}\left[\begin{matrix}r_{s} &amp; r_{d}\\r_{r} &amp; r_{d}\end{matrix}\right] &amp; \left[\begin{matrix}r_{d} &amp; r_{d}\\r_{d} &amp; r_{d}\end{matrix}\right]\end{matrix}\right]\)</span></p>
</div>
</div>
<p>As before, we also define a risky and a safe policy, now as symbolic variables,</p>
<div id="51c0f97e-2f85-46fc-9102-75c63b21497e" class="cell" data-execution_count="64">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>Xsafe <span class="op">=</span> sp.Array([[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">0</span>]])</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>Xrisk <span class="op">=</span> sp.Array([[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>]])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>and also the discount factor as a symbolic variable</p>
<div id="6e1f373a-e6a4-40fc-afdc-15b082bb9402" class="cell" data-execution_count="65">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>dcf <span class="op">=</span> sp.symbols(<span class="st">"gamma"</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>dcf</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="65">
<p><span class="math inline">\(\displaystyle \gamma\)</span></p>
</div>
</div>
<p>Luckily, we only have to change our <code>compute_statevalues</code> slightly, (since the <code>np.einsum</code> function also works with Sympy expressions)</p>
<div id="7645f393-fef5-4b82-aeae-f29787c3c2a0" class="cell" data-execution_count="66">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> symbolic_statevalues(policy_Xsa, transitions_Tsas, rewards_Rsas, discountfactor<span class="op">=</span>dcf):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    s, a, s_ <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>  <span class="co"># defining indices for convenicence</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    Tss <span class="op">=</span> sp.Matrix(np.einsum(policy_Xsa, [s, a], transitions_Tsas, [s, a, s_], [s,s_]))   </span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    Rs <span class="op">=</span> sp.Array(np.einsum(policy_Xsa, [s, a], transitions_Tsas, [s, a, s_], rewards_Rsas, [s, a, s_], [s]))</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>    inv <span class="op">=</span> (sp.eye(<span class="dv">2</span>) <span class="op">-</span> discountfactor<span class="op">*</span>Tss).inv()<span class="op">;</span> inv.simplify()  <span class="co"># sp.simplify() often helps </span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>    Vs <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>discountfactor) <span class="op">*</span> sp.Matrix(np.einsum(inv, [s,s_], Rs, [s_], [s]))<span class="op">;</span> Vs.simplify()</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Vs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The symbolic expressions of the state values for the risky policy are</p>
<div id="a76c807a-de60-4be6-8212-a4fb33a35ff2" class="cell" data-execution_count="67">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>symbolic_statevalues(Xrisk, T, R)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="67">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}\frac{\gamma p_{c} p_{r} r_{d} - \gamma p_{c} p_{r} r_{r} + \gamma p_{c} r_{r} + \gamma p_{r} r_{r} - \gamma r_{r} + p_{c} r_{d} - p_{c} r_{r} + r_{r}}{\gamma p_{c} + \gamma p_{r} - \gamma + 1}\\\frac{\gamma p_{c} p_{r} r_{d} - \gamma p_{c} p_{r} r_{r} + \gamma p_{c} r_{d} + \gamma p_{r} r_{r} - \gamma r_{d} + r_{d}}{\gamma p_{c} + \gamma p_{r} - \gamma + 1}\end{matrix}\right]\)</span></p>
</div>
</div>
<p>and for the safe policy, are</p>
<div id="e6892507-3bdf-4418-86ec-9bd05c62b172" class="cell" data-execution_count="68">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>symbolic_statevalues(Xsafe, T, R)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="68">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}r_{s}\\\frac{\gamma p_{r} r_{s} - \gamma r_{d} + r_{d}}{\gamma p_{r} - \gamma + 1}\end{matrix}\right]\)</span></p>
</div>
</div>
<p>To check whether the risky policy is optimal, we subtract the value of the safe policy from the risky policy’s value at the prosperous state <code>0</code>.</p>
<div id="bf52f7ae-688a-4143-8f70-103fa744797f" class="cell" data-execution_count="69">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>risky_optimal <span class="op">=</span> sp.simplify(symbolic_statevalues(Xrisk, T, R)[<span class="dv">0</span>])<span class="op">\</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">-</span> sp.simplify(symbolic_statevalues(Xsafe, T, R)[<span class="dv">0</span>]) </span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>sp.simplify(risky_optimal)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="69">
<p><span class="math inline">\(\displaystyle \frac{\gamma p_{c} p_{r} r_{d} - \gamma p_{c} p_{r} r_{r} + \gamma p_{c} r_{r} + \gamma p_{r} r_{r} - \gamma r_{r} + p_{c} r_{d} - p_{c} r_{r} + r_{r} - r_{s} \left(\gamma p_{c} + \gamma p_{r} - \gamma + 1\right)}{\gamma p_{c} + \gamma p_{r} - \gamma + 1}\)</span></p>
</div>
</div>
<p>We can solve this equation for any variable. For example, to check the critical collapse probability for an entirely myopic agent with zero care for the future, we solve the equation for the collapse probability <span class="math inline">\(p_c\)</span> and substitute the discount factor <span class="math inline">\(\gamma=0\)</span>.</p>
<div id="53b32430" class="cell" data-execution_count="70">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>sp.solve(risky_optimal, pc)[<span class="dv">0</span>].subs(dcf, <span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="70">
<p><span class="math inline">\(\displaystyle \frac{- r_{r} + r_{s}}{r_{d} - r_{r}}\)</span></p>
</div>
</div>
<p>Thus, <strong>our initution about the ratio between <span class="math inline">\(r_s\)</span> and <span class="math inline">\(r_r\)</span> was not entirely correct</strong>. In fact, we can simplify the three reward parameters <span class="math inline">\(r_r\)</span>, <span class="math inline">\(r_s\)</span>, and <span class="math inline">\(r_d\)</span>. As it is irrelevant to the agent’s decision whether all rewards are multiplied by a factor or all rewards are added by a constant, we can set <span class="math inline">\(r_d=0\)</span> and <span class="math inline">\(r_s=1\)</span> without loss of generality.</p>
<p>Setting the degraded reward to zero, <span class="math inline">\(r_d=0\)</span>, and the risky reward to one, <span class="math inline">\(r_r=1\)</span>, improves the transparency and interpretability of the model.</p>
<div id="861de457" class="cell" data-execution_count="71">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>sp.solve(risky_optimal.subs(rr, <span class="dv">1</span>).subs(rd, <span class="dv">0</span>), pc)[<span class="dv">0</span>].subs(dcf, <span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="71">
<p><span class="math inline">\(\displaystyle 1 - r_{s}\)</span></p>
</div>
</div>
<p>Thus, the critical collapse probability <span class="math inline">\(\hat p_c\)</span> for <span class="math inline">\(\gamma =0\)</span> is given by the <span class="math inline">\(\hat p_c = 1-r_s\)</span>.</p>
<p><strong>By using symbolic calculations, we improve the transparency and interpretability of our model.</strong></p>
<p>How can we speed up the computation time with <code>sympy</code>?</p>
</section>
<section id="efficient-computation" class="level3">
<h3 class="anchored" data-anchor-id="efficient-computation">Efficient computation</h3>
<p>To create a plot as above, it is an excellent strategy to <strong>convert this symbolic expression into a numeric function</strong>. In <code>sympy,</code> this is done with the <code>sympy.lambdify</code> function, (called as <code>sp.lambdify((&lt;symbolic parameters&gt;), &lt;symbolic expression to be turned into a numeric function&gt;</code>)</p>
<div id="254a2f8c-ca2c-4db2-a4ef-be7ea915a3f8" class="cell" data-execution_count="72">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>risky_optimal_func <span class="op">=</span> sp.lambdify((pc,pr,dcf,rs,rr,rd), risky_optimal)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>For example, we can now execute <code>risky_optimal_func</code> for <span class="math inline">\(p_c=0.2\)</span>, <span class="math inline">\(p_r=0.01\)</span>, <span class="math inline">\(\gamma=0.9\)</span>, <span class="math inline">\(r_s=0.5\)</span>, <span class="math inline">\(r_r=1.0\)</span>, and <span class="math inline">\(r_d=0.0\)</span> as</p>
<div id="3b45efd9-279f-485d-9475-53073eefc25e" class="cell" data-execution_count="73">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>risky_optimal_func(<span class="fl">0.2</span>, <span class="fl">0.01</span>, <span class="fl">0.9</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>-0.19826989619377183</code></pre>
</div>
</div>
<p>and learn that the risky policy is <em>not</em> optimal in this case.</p>
<p>However, the big advantage of a <em>lambdified</em> function is that we can apply it in <strong>vectorized form</strong>. This means the parameters don’t have to be single numbers. They can be vectors or even larger tensors. See, for example,</p>
<div id="1f84d2c5-362c-4e5c-be58-64990bfbe86c" class="cell" data-execution_count="74">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>gams <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, <span class="dv">9</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>risky_optimal_func(<span class="fl">0.2</span>, <span class="fl">0.01</span>, gams, <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="74">
<pre><code>array([ 0.299984  ,  0.27779382,  0.25014334,  0.21473437,  0.1677686 ,
        0.10248474,  0.00556964, -0.15331544, -0.46154209])</code></pre>
</div>
</div>
<p>Thus, to recreate our example from above, where we wanted to know how the critical discount factor <span class="math inline">\(\hat\gamma\)</span> depends on the collapse probability <span class="math inline">\(p_c\)</span> for given other parameters, we can now use the <code>risky_optimal_func</code> directly in vectorized form.</p>
<p>However, if we simply put two vectors (of the same dimension) inside the function, we only get</p>
<div id="f2d8c248-a734-4505-9583-91d91e7657e0" class="cell" data-execution_count="75">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>discountfactors <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, <span class="dv">9</span>)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>collapseprobabilities <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, <span class="dv">9</span>)</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>risky_optimal_func(collapseprobabilities, <span class="fl">0.01</span>, discountfactors, <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="75">
<pre><code>array([ 0.49989999,  0.3595776 ,  0.19241376,  0.01072705, -0.16556291,
       -0.31475139, -0.42146066, -0.48138804, -0.499999  ])</code></pre>
</div>
</div>
<p>we only get one vector (of the same dimension) out. This is, beacuse vectorization groups changes along a dimension together.</p>
<p>This means we need to separate the variation in the discount factors (in <code>discountfactors</code>) and the variation in the collapse probabilities (in <code>collapseprobabilities</code>) into different dimensions. Luckily, we don’t have to do that manually. The <code>numpy</code> method <code>numpy.meshrid</code> exactly fits this purpose. For example,</p>
<div id="e0302b30-0ef0-46dc-98e3-52efa7e02739" class="cell" data-execution_count="76">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>discountfactors <span class="op">=</span> [<span class="fl">0.8</span>, <span class="fl">0.9</span>]</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>collapseprobabilities <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>]</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>np.meshgrid(discountfactors, collapseprobabilities)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>[array([[0.8, 0.9],
        [0.8, 0.9],
        [0.8, 0.9]]),
 array([[0.1, 0.1],
        [0.2, 0.2],
        [0.3, 0.3]])]</code></pre>
</div>
</div>
<p>In practise, we can use a meshgrid as follows,</p>
<div id="9cbeaeb1-8240-48ec-88d4-519574820a5f" class="cell" data-execution_count="77">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>pr_ <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>rs_ <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>rr_ <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>rd_ <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>discountfactors <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, <span class="dv">301</span>)</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>collapseprobabilities <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, <span class="dv">301</span>)</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>DCFs, PCs <span class="op">=</span> np.meshgrid(discountfactors, collapseprobabilities)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="e88a9d2a-a9d3-42af-bccc-9a582bffe744" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-execution_count="78">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>risky_optimal_func(PCs, pr_, DCFs, rs_, rr_, rd_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>array([[ 0.49989999,  0.49989966,  0.49989932, ...,  0.49398743,
         0.49251766,  0.49009707],
       [ 0.49656699,  0.49655555,  0.49654403, ...,  0.32758543,
         0.29387422,  0.24378043],
       [ 0.49323399,  0.49321152,  0.4931889 , ...,  0.20822659,
         0.1607447 ,  0.09481031],
       ...,
       [-0.49323534, -0.49325773, -0.49328013, ..., -0.4998874 ,
        -0.49990965, -0.4999319 ],
       [-0.49656768, -0.49657908, -0.49659048, ..., -0.49994305,
        -0.49995431, -0.49996556],
       [-0.49990001, -0.49990034, -0.49990068, ..., -0.49999835,
        -0.49999867, -0.499999  ]])</code></pre>
</div>
</div>
<p>Notice how quickly that was compared to our previous calculations!</p>
<p>To time how long the cell execution takes more precisely, we can use the <code>%%timeit</code> cell magic command:</p>
<div id="dcec789a-c869-4e07-a0ea-a4341fb06e08" class="cell" data-execution_count="79">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>timeit</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>risky_optimal_func(PCs, pr_, DCFs, rs_, rr_, rd_)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.92 ms ± 396 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)</code></pre>
</div>
</div>
<p>It executes the cell multiple times and presents us with a short summary statistic. Compare the average runtime of the cell with the numerical computation. It is around 5000 times faster!</p>
<p>Thus, we can summarize the lambdified <code>sympy</code> expression into a <code>plot_parameter_space</code> function:</p>
<div id="fdd2696d-894b-4e9b-8b56-c9f5dfefa4a9" class="cell" data-execution_count="80">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_parameter_space(safe_reward<span class="op">=</span><span class="fl">0.5</span>, risky_reward<span class="op">=</span><span class="fl">1.0</span>, degraded_reward<span class="op">=</span><span class="fl">0.0</span>, recov_prop<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">131</span>)<span class="op">;</span> plt.xticks([])<span class="op">;</span> plt.yticks([])<span class="op">;</span> </span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">133</span>)<span class="op">;</span> plt.xticks([])<span class="op">;</span> plt.yticks([])<span class="op">;</span> </span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">132</span>) <span class="co"># just to center the plot in the middle</span></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>    resolution<span class="op">=</span><span class="dv">251</span></span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, resolution)</span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> np.linspace(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>, resolution)</span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a>    XX, YY <span class="op">=</span> np.meshgrid(X, Y)</span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-12"><a href="#cb109-12" aria-hidden="true" tabindex="-1"></a>    ro <span class="op">=</span> risky_optimal_func(XX, recov_prop, YY, safe_reward, risky_reward, degraded_reward)</span>
<span id="cb109-13"><a href="#cb109-13" aria-hidden="true" tabindex="-1"></a>    plt.pcolormesh(XX, YY, ro, cmap<span class="op">=</span><span class="st">'bwr'</span>, vmin<span class="op">=-</span><span class="fl">0.1</span>, vmax<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb109-14"><a href="#cb109-14" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Discount factor'</span>)<span class="op">;</span> plt.xlabel(<span class="st">'Collapse leverage'</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="634e91d1" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;slide&quot;}}" data-tags="[]" data-execution_count="81">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>plot_parameter_space()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03.01-SequentialDecisions_files/figure-html/cell-82-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>When working with this Jupyter Notebook directly, we can interactively explore the parameter space of the model.</p>
<p>For an agent that does not discount the future at all, i.e., with <span class="math inline">\(\gamma \rightarrow 1\)</span>, the critical collapse leverage yields,</p>
<div id="4ac1ec76" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="83">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>sp.simplify(sp.solve(risky_optimal.subs(rr,<span class="dv">1</span>).subs(rd, <span class="dv">0</span>), pc)[<span class="dv">0</span>].subs(dcf, <span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="83">
<p><span class="math inline">\(\displaystyle \frac{p_{r} \left(1 - r_{s}\right)}{p_{r} + r_{s}}\)</span></p>
</div>
</div>
<p>Thus, if there is zero recovery probability <span class="math inline">\(p_r=0\)</span>, the safe policy is optimal regardless of the relative reward <span class="math inline">\(0&lt;r_s&lt;1\)</span>.</p>
<div id="cd7912fe" class="cell" data-editable="true" data-quarto-private-1="{&quot;key&quot;:&quot;slideshow&quot;,&quot;value&quot;:{&quot;slide_type&quot;:&quot;&quot;}}" data-tags="[]" data-execution_count="84">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>plot_parameter_space(recov_prop<span class="op">=</span><span class="fl">0.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03.01-SequentialDecisions_files/figure-html/cell-85-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If <span class="math inline">\(p_r &gt; 0\)</span>, then it depends on the relative reward <span class="math inline">\(r_s\)</span> whether the safe or the risky policy is optimal for a fully farsighted agent.</p>
<p><strong>Taken together, by using symbolic computation from the <code>sympy</code> package, we improve interpretability, transparancey and computational efficiency of our model.</strong></p>
</section>
</section>
<section id="learning-goals-revisited" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="learning-goals-revisited"><span class="header-section-number">6.6</span> Learning goals <strong>revisited</strong></h2>
<ul>
<li>We <strong>introduced</strong> the elements of a Markov Decision Process (MDP) and discussed how they relate to applications in human-environment interactions</li>
<li>We <strong>simulateed</strong> and <strong>visualized</strong> the time-evolution of an MDP.</li>
<li>We <strong>covered</strong> what value functions are, why they are usful and how to realte to the agent’s goal and Bellman equation.</li>
<li>We <strong>computed</strong> value functions <em>in serveral ways</em> and <strong>visualized</strong> how the best policy depends on other model parameters.</li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-BarfussEtAl2018" class="csl-entry" role="listitem">
Barfuss, W., Donges, J. F., Lade, S. J., &amp; Kurths, J. (2018). When optimization for governing human-environment tipping elements is neither sustainable nor safe. <em>Nature Communications</em>, <em>9</em>(1), 2354. <a href="https://doi.org/10.1038/s41467-018-04738-z">https://doi.org/10.1038/s41467-018-04738-z</a>
</div>
<div id="ref-MarescotEtAl2013" class="csl-entry" role="listitem">
Marescot, L., Chapron, G., Chadès, I., Fackler, P. L., Duchamp, C., Marboutin, E., &amp; Gimenez, O. (2013). Complex decisions made simple: A primer on stochastic dynamic programming. <em>Methods in Ecology and Evolution</em>, <em>4</em>(9), 872–884. <a href="https://doi.org/10.1111/2041-210X.12082">https://doi.org/10.1111/2041-210X.12082</a>
</div>
<div id="ref-MnihEtAl2015" class="csl-entry" role="listitem">
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., &amp; Hassabis, D. (2015). Human-level control through deep reinforcement learning. <em>Nature</em>, <em>518</em>(7540), 529–533. <a href="https://doi.org/10.1038/nature14236">https://doi.org/10.1038/nature14236</a>
</div>
<div id="ref-PolaskyEtAl2011" class="csl-entry" role="listitem">
Polasky, S., Carpenter, S. R., Folke, C., &amp; Keeler, B. (2011). Decision-making under great uncertainty: Environmental management in an era of global change. <em>Trends in Ecology &amp; Evolution</em>, <em>26</em>(8), 398–404. <a href="https://doi.org/10.1016/j.tree.2011.04.007">https://doi.org/10.1016/j.tree.2011.04.007</a>
</div>
<div id="ref-SchultzEtAl2017" class="csl-entry" role="listitem">
Schultz, W., Stauffer, W. R., &amp; Lak, A. (2017). The phasic dopamine signal maturing: From reward via behavioural activation to formal economic utility. <em>Current Opinion in Neurobiology</em>, <em>43</em>, 139–148. <a href="https://doi.org/10.1016/j.conb.2017.03.013">https://doi.org/10.1016/j.conb.2017.03.013</a>
</div>
<div id="ref-SuttonBarto2018" class="csl-entry" role="listitem">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (Second edition). The MIT Press.
</div>
<div id="ref-Williams2009" class="csl-entry" role="listitem">
Williams, B. K. (2009). Markov decision processes in natural resources management: Observability and uncertainty. <em>Ecological Modelling</em>, <em>220</em>(6), 830–840. <a href="https://doi.org/10.1016/j.ecolmodel.2008.12.023">https://doi.org/10.1016/j.ecolmodel.2008.12.023</a>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-TargetEquilibria.html" class="pagination-link" aria-label="Target Equilibria">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Target Equilibria</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03.02-StrategicInteractions.html" class="pagination-link" aria-label="Strategic Interactions">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Strategic Interactions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>